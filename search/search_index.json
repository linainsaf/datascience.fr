{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"C'est quoi la Data Science La data science est un domaine qui s'int\u00e9resse \u00e0 l'extraction de connaissances \u00e0 partir de donn\u00e9es. Il utilise des techniques statistiques, math\u00e9matiques et informatiques pour analyser, interpr\u00e9ter et mod\u00e9liser des donn\u00e9es. La data science est utilis\u00e9e dans plusieurs domaines tels que la finance, la sant\u00e9, la vente au d\u00e9tail, le marketing, les transports et bien d'autres. C'est devenu de plus en plus important \u00e0 mesure que la quantit\u00e9 de donn\u00e9es g\u00e9n\u00e9r\u00e9es par la technologie moderne continue de cro\u00eetre. Les diff\u00e9rentes \u00e9tapes d'un projet de Data Science Collecte de donn\u00e9es Le premier pas dans le processus de data science est de collecter des donn\u00e9es. Les donn\u00e9es peuvent provenir de diff\u00e9rentes sources telles que des fichiers, des bases de donn\u00e9es, des capteurs, des sondages, des enregistrements de transactions, des images, etc. Il est important de choisir des donn\u00e9es de qualit\u00e9 qui sont pertinentes pour le probl\u00e8me que vous souhaitez r\u00e9soudre. Par exemple, pour pr\u00e9dire si un client d'une entreprise va acheter un produit, vous pouvez collecter des donn\u00e9es sur les achats pass\u00e9s, les pr\u00e9f\u00e9rences de produits, les habitudes de navigation en ligne, les interactions avec la marque sur les r\u00e9seaux sociaux, etc. Exploration des donn\u00e9es Une fois les donn\u00e9es collect\u00e9es, il est important de les explorer pour comprendre leur nature, leur qualit\u00e9 et leur contenu. Cette \u00e9tape permet de d\u00e9tecter les valeurs manquantes, les donn\u00e9es aberrantes, les corr\u00e9lations, les distributions, etc. Cela peut se faire en utilisant des outils statistiques et graphiques. Par exemple, pour comprendre les habitudes d'achat des clients, vous pouvez visualiser la distribution des prix d'achat, la corr\u00e9lation entre l'\u00e2ge et le montant d'achat, ou encore la fr\u00e9quence d'achat selon le canal de vente. Types de donn\u00e9es Les donn\u00e9es sont le fondement de la data science, et elles peuvent \u00eatre de diff\u00e9rents types. Comprendre la nature des donn\u00e9es est important pour choisir les m\u00e9thodes d'analyse appropri\u00e9es. Donn\u00e9es num\u00e9riques Les donn\u00e9es num\u00e9riques sont des nombres, et peuvent \u00eatre continues ou discr\u00e8tes. Les donn\u00e9es continues peuvent prendre n'importe quelle valeur dans un intervalle donn\u00e9, tandis que les donn\u00e9es discr\u00e8tes sont limit\u00e9es \u00e0 des valeurs enti\u00e8res. Les donn\u00e9es num\u00e9riques peuvent \u00eatre utilis\u00e9es pour mesurer des grandeurs physiques, financi\u00e8res, ou pour quantifier des caract\u00e9ristiques. Exemples : La temp\u00e9rature en degr\u00e9s Celsius est une donn\u00e9e num\u00e9rique continue. Le nombre de produits vendus est une donn\u00e9e num\u00e9rique discr\u00e8te. Le montant d'une transaction financi\u00e8re est une donn\u00e9e num\u00e9rique continue. Donn\u00e9es cat\u00e9gorielles Les donn\u00e9es cat\u00e9gorielles sont des valeurs qui peuvent \u00eatre regroup\u00e9es en cat\u00e9gories. Elles peuvent \u00eatre nominales ou ordinales. Les donn\u00e9es cat\u00e9gorielles sont souvent utilis\u00e9es pour d\u00e9crire des caract\u00e9ristiques qualitatives. Exemples : La couleur des yeux est une donn\u00e9e cat\u00e9gorielle nominale. La taille des v\u00eatements est une donn\u00e9e cat\u00e9gorielle ordinale. La marque d'un produit est une donn\u00e9e cat\u00e9gorielle nominale. Donn\u00e9es temporelles Les donn\u00e9es temporelles sont des valeurs qui d\u00e9crivent des moments dans le temps. Elles peuvent \u00eatre des instants pr\u00e9cis, des p\u00e9riodes, ou des fr\u00e9quences. Les donn\u00e9es temporelles sont souvent utilis\u00e9es pour analyser des ph\u00e9nom\u00e8nes qui \u00e9voluent dans le temps. Exemples : La date de naissance est une donn\u00e9e temporelle de type instant. La dur\u00e9e d'une p\u00e9riode de vente est une donn\u00e9e temporelle de type p\u00e9riode. La fr\u00e9quence d'achat est une donn\u00e9e temporelle de type fr\u00e9quence. Donn\u00e9es textuelles Les donn\u00e9es textuelles sont des valeurs qui d\u00e9crivent du texte. Elles peuvent \u00eatre des phrases, des paragraphes, ou des documents entiers. Les donn\u00e9es textuelles sont souvent utilis\u00e9es pour analyser le langage naturel, ou pour extraire des informations de textes non structur\u00e9s. Exemples : Les avis des clients sur un produit sont des donn\u00e9es textuelles. Les descriptions des produits dans un catalogue sont des donn\u00e9es textuelles. Les articles de journaux sont des donn\u00e9es textuelles. Donn\u00e9es g\u00e9ospatiales Les donn\u00e9es g\u00e9ospatiales sont des valeurs qui d\u00e9crivent des positions dans l'espace. Elles peuvent \u00eatre des coordonn\u00e9es g\u00e9ographiques, des adresses, ou des noms de lieux. Les donn\u00e9es g\u00e9ospatiales sont souvent utilis\u00e9es pour cartographier des donn\u00e9es, ou pour analyser des ph\u00e9nom\u00e8nes qui varient dans l'espace. Exemples : Les adresses des clients sont des donn\u00e9es g\u00e9ospatiales. Les coordonn\u00e9es GPS d'un point de vente sont des donn\u00e9es g\u00e9ospatiales. Les noms de villes dans un tableau de donn\u00e9es sont des donn\u00e9es g\u00e9ospatiales. Donn\u00e9es en images Les donn\u00e9es en images sont des valeurs qui d\u00e9crivent des images. Elles sont souvent stock\u00e9es sous forme de pixels qui repr\u00e9sentent les couleurs ou les niveaux de gris des diff\u00e9rents points de l'image. Les donn\u00e9es en images sont souvent utilis\u00e9es dans des applications telles que la reconnaissance d'objets, la d\u00e9tection de fraudes et la surveillance. Exemples : Les images m\u00e9dicales sont des donn\u00e9es en images, comme les radiographies et les IRM. Les images satellites sont des donn\u00e9es en images qui peuvent \u00eatre utilis\u00e9es pour cartographier les zones g\u00e9ographiques et surveiller les changements dans les environnements naturels. Les images de surveillance de la circulation routi\u00e8re sont des donn\u00e9es en images qui peuvent \u00eatre utilis\u00e9es pour d\u00e9tecter les violations du code de la route. Donn\u00e9es en audio Les donn\u00e9es en audio sont des valeurs qui d\u00e9crivent des enregistrements sonores. Elles peuvent \u00eatre stock\u00e9es sous forme de signaux num\u00e9riques qui repr\u00e9sentent la fr\u00e9quence et l'amplitude du son \u00e0 diff\u00e9rents moments. Les donn\u00e9es en audio sont souvent utilis\u00e9es dans des applications telles que la reconnaissance vocale, la d\u00e9tection de fraudes et la surveillance. Exemples : Les enregistrements de voix sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour la reconnaissance vocale, telle que la conversion de la parole en texte. Les enregistrements de musique sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour la recommandation de musique et la classification de genres musicaux. Les enregistrements de surveillance audio sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour d\u00e9tecter les menaces et les comportements suspects. En tant que data scientist, il est important de comprendre les diff\u00e9rents types de donn\u00e9es, car cela peut avoir une incidence sur les m\u00e9thodes de collecte, de stockage, de traitement et d'analyse. En fonction des donn\u00e9es, les algorithmes de machine learning et les outils d'analyse peuvent varier consid\u00e9rablement. S\u00e9lection de caract\u00e9ristiques Si le nombre de caract\u00e9ristiques est tr\u00e8s \u00e9lev\u00e9, il peut \u00eatre judicieux de s\u00e9lectionner les caract\u00e9ristiques les plus importantes pour am\u00e9liorer l'efficacit\u00e9 du mod\u00e8le. S\u00e9paration des donn\u00e9es Les donn\u00e9es doivent \u00eatre divis\u00e9es en deux parties, l'ensemble d'entra\u00eenement et l'ensemble de test. L'ensemble d'entra\u00eenement est utilis\u00e9 pour entra\u00eener le mod\u00e8le et l'ensemble de test est utilis\u00e9 pour \u00e9valuer les performances du mod\u00e8le. Construction de notre mod\u00e8le (IA) Une fois les donn\u00e9es nettoy\u00e9es et pr\u00e9trait\u00e9es, il est temps de choisir un mod\u00e8le d'IA et de l'entra\u00eener sur l'ensemble d'entra\u00eenement. Le choix du mod\u00e8le d\u00e9pend du type de probl\u00e8me \u00e0 r\u00e9soudre et du type de donn\u00e9es. L'intelligence artificielle (IA) est un domaine de l'informatique qui vise \u00e0 cr\u00e9er des machines capables de r\u00e9aliser des t\u00e2ches qui n\u00e9cessitent normalement l'intelligence humaine. Cette discipline est bas\u00e9e sur le d\u00e9veloppement de programmes informatiques qui peuvent apprendre et s'adapter \u00e0 de nouvelles situations, et ainsi ex\u00e9cuter des t\u00e2ches sans intervention humaine. Il existe plusieurs modes d'apprentissage : Le Machine Learning (apprentissage automatique en fran\u00e7ais) est une branche de l'intelligence artificielle qui se concentre sur l'apprentissage de mod\u00e8les \u00e0 partir de donn\u00e9es, afin de r\u00e9aliser des t\u00e2ches de pr\u00e9diction ou de classification. Les mod\u00e8les peuvent \u00eatre entra\u00een\u00e9s \u00e0 l'aide de diff\u00e9rentes techniques, notamment l'apprentissage supervis\u00e9 (o\u00f9 les donn\u00e9es sont \u00e9tiquet\u00e9es pour indiquer la r\u00e9ponse souhait\u00e9e) et l'apprentissage non supervis\u00e9 (o\u00f9 les donn\u00e9es ne sont pas \u00e9tiquet\u00e9es). Le Machine Learning est utilis\u00e9 dans de nombreux domaines, notamment la reconnaissance vocale, la reconnaissance d'images, la recommandation de produits et la d\u00e9tection de fraudes. Le Reinforcement Learning (apprentissage par renforcement en fran\u00e7ais) est une branche sp\u00e9cifique du Machine Learning qui se concentre sur l'apprentissage par essais et erreurs. Dans ce type d'apprentissage, un agent est plac\u00e9 dans un environnement et doit apprendre \u00e0 prendre des d\u00e9cisions pour maximiser une r\u00e9compense. L'agent prend une action, re\u00e7oit une r\u00e9compense ou une p\u00e9nalit\u00e9, et utilise cette information pour ajuster sa strat\u00e9gie. Le Reinforcement Learning est utilis\u00e9 pour des applications comme les jeux vid\u00e9o, les robots, et les syst\u00e8mes de recommandation personnalis\u00e9e. Le Deep Learning (apprentissage profond en fran\u00e7ais) est une sous-branche du Machine Learning qui utilise des r\u00e9seaux de neurones artificiels pour apprendre \u00e0 partir de donn\u00e9es. Les r\u00e9seaux de neurones artificiels sont des mod\u00e8les computationnels qui sont con\u00e7us pour imiter le fonctionnement du cerveau humain. Les r\u00e9seaux de neurones sont compos\u00e9s de plusieurs couches, chacune traitant une partie de l'information en entr\u00e9e pour produire une sortie. Le Deep Learning est utilis\u00e9 dans de nombreux domaines, notamment la reconnaissance vocale, la reconnaissance d'images, la traduction automatique et la reconnaissance de caract\u00e8res manuscrits. \u00c9valuation du mod\u00e8le Apr\u00e8s l'entra\u00eenement du mod\u00e8le, il est important d'\u00e9valuer sa performance sur l'ensemble de test. Il existe de nombreuses mesures pour \u00e9valuer les performances d'un mod\u00e8le, telles que l'exactitude, la pr\u00e9cision, le rappel et le F1-score. Optimisation du mod\u00e8le Si les performances du mod\u00e8le ne sont pas satisfaisantes, il peut \u00eatre n\u00e9cessaire de le r\u00e9ajuster ou de r\u00e9gler ses param\u00e8tres pour am\u00e9liorer ses performances. D\u00e9ploiement du mod\u00e8le Une fois que le mod\u00e8le a \u00e9t\u00e9 entra\u00een\u00e9 et \u00e9valu\u00e9 avec succ\u00e8s, il peut \u00eatre d\u00e9ploy\u00e9 dans un environnement de production. Cela peut inclure l'int\u00e9gration du mod\u00e8le dans une application ou un syst\u00e8me existant.","title":"Introduction \u00e0 la Data Science"},{"location":"#cest-quoi-la-data-science","text":"La data science est un domaine qui s'int\u00e9resse \u00e0 l'extraction de connaissances \u00e0 partir de donn\u00e9es. Il utilise des techniques statistiques, math\u00e9matiques et informatiques pour analyser, interpr\u00e9ter et mod\u00e9liser des donn\u00e9es. La data science est utilis\u00e9e dans plusieurs domaines tels que la finance, la sant\u00e9, la vente au d\u00e9tail, le marketing, les transports et bien d'autres. C'est devenu de plus en plus important \u00e0 mesure que la quantit\u00e9 de donn\u00e9es g\u00e9n\u00e9r\u00e9es par la technologie moderne continue de cro\u00eetre.","title":"C'est quoi la Data Science"},{"location":"#les-differentes-etapes-dun-projet-de-data-science","text":"","title":"Les diff\u00e9rentes \u00e9tapes d'un projet de Data Science"},{"location":"#collecte-de-donnees","text":"Le premier pas dans le processus de data science est de collecter des donn\u00e9es. Les donn\u00e9es peuvent provenir de diff\u00e9rentes sources telles que des fichiers, des bases de donn\u00e9es, des capteurs, des sondages, des enregistrements de transactions, des images, etc. Il est important de choisir des donn\u00e9es de qualit\u00e9 qui sont pertinentes pour le probl\u00e8me que vous souhaitez r\u00e9soudre. Par exemple, pour pr\u00e9dire si un client d'une entreprise va acheter un produit, vous pouvez collecter des donn\u00e9es sur les achats pass\u00e9s, les pr\u00e9f\u00e9rences de produits, les habitudes de navigation en ligne, les interactions avec la marque sur les r\u00e9seaux sociaux, etc.","title":"Collecte de donn\u00e9es"},{"location":"#exploration-des-donnees","text":"Une fois les donn\u00e9es collect\u00e9es, il est important de les explorer pour comprendre leur nature, leur qualit\u00e9 et leur contenu. Cette \u00e9tape permet de d\u00e9tecter les valeurs manquantes, les donn\u00e9es aberrantes, les corr\u00e9lations, les distributions, etc. Cela peut se faire en utilisant des outils statistiques et graphiques. Par exemple, pour comprendre les habitudes d'achat des clients, vous pouvez visualiser la distribution des prix d'achat, la corr\u00e9lation entre l'\u00e2ge et le montant d'achat, ou encore la fr\u00e9quence d'achat selon le canal de vente.","title":"Exploration des donn\u00e9es"},{"location":"#types-de-donnees","text":"Les donn\u00e9es sont le fondement de la data science, et elles peuvent \u00eatre de diff\u00e9rents types. Comprendre la nature des donn\u00e9es est important pour choisir les m\u00e9thodes d'analyse appropri\u00e9es.","title":"Types de donn\u00e9es"},{"location":"#donnees-numeriques","text":"Les donn\u00e9es num\u00e9riques sont des nombres, et peuvent \u00eatre continues ou discr\u00e8tes. Les donn\u00e9es continues peuvent prendre n'importe quelle valeur dans un intervalle donn\u00e9, tandis que les donn\u00e9es discr\u00e8tes sont limit\u00e9es \u00e0 des valeurs enti\u00e8res. Les donn\u00e9es num\u00e9riques peuvent \u00eatre utilis\u00e9es pour mesurer des grandeurs physiques, financi\u00e8res, ou pour quantifier des caract\u00e9ristiques. Exemples : La temp\u00e9rature en degr\u00e9s Celsius est une donn\u00e9e num\u00e9rique continue. Le nombre de produits vendus est une donn\u00e9e num\u00e9rique discr\u00e8te. Le montant d'une transaction financi\u00e8re est une donn\u00e9e num\u00e9rique continue.","title":"Donn\u00e9es num\u00e9riques"},{"location":"#donnees-categorielles","text":"Les donn\u00e9es cat\u00e9gorielles sont des valeurs qui peuvent \u00eatre regroup\u00e9es en cat\u00e9gories. Elles peuvent \u00eatre nominales ou ordinales. Les donn\u00e9es cat\u00e9gorielles sont souvent utilis\u00e9es pour d\u00e9crire des caract\u00e9ristiques qualitatives. Exemples : La couleur des yeux est une donn\u00e9e cat\u00e9gorielle nominale. La taille des v\u00eatements est une donn\u00e9e cat\u00e9gorielle ordinale. La marque d'un produit est une donn\u00e9e cat\u00e9gorielle nominale.","title":"Donn\u00e9es cat\u00e9gorielles"},{"location":"#donnees-temporelles","text":"Les donn\u00e9es temporelles sont des valeurs qui d\u00e9crivent des moments dans le temps. Elles peuvent \u00eatre des instants pr\u00e9cis, des p\u00e9riodes, ou des fr\u00e9quences. Les donn\u00e9es temporelles sont souvent utilis\u00e9es pour analyser des ph\u00e9nom\u00e8nes qui \u00e9voluent dans le temps. Exemples : La date de naissance est une donn\u00e9e temporelle de type instant. La dur\u00e9e d'une p\u00e9riode de vente est une donn\u00e9e temporelle de type p\u00e9riode. La fr\u00e9quence d'achat est une donn\u00e9e temporelle de type fr\u00e9quence.","title":"Donn\u00e9es temporelles"},{"location":"#donnees-textuelles","text":"Les donn\u00e9es textuelles sont des valeurs qui d\u00e9crivent du texte. Elles peuvent \u00eatre des phrases, des paragraphes, ou des documents entiers. Les donn\u00e9es textuelles sont souvent utilis\u00e9es pour analyser le langage naturel, ou pour extraire des informations de textes non structur\u00e9s. Exemples : Les avis des clients sur un produit sont des donn\u00e9es textuelles. Les descriptions des produits dans un catalogue sont des donn\u00e9es textuelles. Les articles de journaux sont des donn\u00e9es textuelles.","title":"Donn\u00e9es textuelles"},{"location":"#donnees-geospatiales","text":"Les donn\u00e9es g\u00e9ospatiales sont des valeurs qui d\u00e9crivent des positions dans l'espace. Elles peuvent \u00eatre des coordonn\u00e9es g\u00e9ographiques, des adresses, ou des noms de lieux. Les donn\u00e9es g\u00e9ospatiales sont souvent utilis\u00e9es pour cartographier des donn\u00e9es, ou pour analyser des ph\u00e9nom\u00e8nes qui varient dans l'espace. Exemples : Les adresses des clients sont des donn\u00e9es g\u00e9ospatiales. Les coordonn\u00e9es GPS d'un point de vente sont des donn\u00e9es g\u00e9ospatiales. Les noms de villes dans un tableau de donn\u00e9es sont des donn\u00e9es g\u00e9ospatiales.","title":"Donn\u00e9es g\u00e9ospatiales"},{"location":"#donnees-en-images","text":"Les donn\u00e9es en images sont des valeurs qui d\u00e9crivent des images. Elles sont souvent stock\u00e9es sous forme de pixels qui repr\u00e9sentent les couleurs ou les niveaux de gris des diff\u00e9rents points de l'image. Les donn\u00e9es en images sont souvent utilis\u00e9es dans des applications telles que la reconnaissance d'objets, la d\u00e9tection de fraudes et la surveillance. Exemples : Les images m\u00e9dicales sont des donn\u00e9es en images, comme les radiographies et les IRM. Les images satellites sont des donn\u00e9es en images qui peuvent \u00eatre utilis\u00e9es pour cartographier les zones g\u00e9ographiques et surveiller les changements dans les environnements naturels. Les images de surveillance de la circulation routi\u00e8re sont des donn\u00e9es en images qui peuvent \u00eatre utilis\u00e9es pour d\u00e9tecter les violations du code de la route.","title":"Donn\u00e9es en images"},{"location":"#donnees-en-audio","text":"Les donn\u00e9es en audio sont des valeurs qui d\u00e9crivent des enregistrements sonores. Elles peuvent \u00eatre stock\u00e9es sous forme de signaux num\u00e9riques qui repr\u00e9sentent la fr\u00e9quence et l'amplitude du son \u00e0 diff\u00e9rents moments. Les donn\u00e9es en audio sont souvent utilis\u00e9es dans des applications telles que la reconnaissance vocale, la d\u00e9tection de fraudes et la surveillance. Exemples : Les enregistrements de voix sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour la reconnaissance vocale, telle que la conversion de la parole en texte. Les enregistrements de musique sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour la recommandation de musique et la classification de genres musicaux. Les enregistrements de surveillance audio sont des donn\u00e9es en audio qui peuvent \u00eatre utilis\u00e9es pour d\u00e9tecter les menaces et les comportements suspects. En tant que data scientist, il est important de comprendre les diff\u00e9rents types de donn\u00e9es, car cela peut avoir une incidence sur les m\u00e9thodes de collecte, de stockage, de traitement et d'analyse. En fonction des donn\u00e9es, les algorithmes de machine learning et les outils d'analyse peuvent varier consid\u00e9rablement.","title":"Donn\u00e9es en audio"},{"location":"#selection-de-caracteristiques","text":"Si le nombre de caract\u00e9ristiques est tr\u00e8s \u00e9lev\u00e9, il peut \u00eatre judicieux de s\u00e9lectionner les caract\u00e9ristiques les plus importantes pour am\u00e9liorer l'efficacit\u00e9 du mod\u00e8le.","title":"S\u00e9lection de caract\u00e9ristiques"},{"location":"#separation-des-donnees","text":"Les donn\u00e9es doivent \u00eatre divis\u00e9es en deux parties, l'ensemble d'entra\u00eenement et l'ensemble de test. L'ensemble d'entra\u00eenement est utilis\u00e9 pour entra\u00eener le mod\u00e8le et l'ensemble de test est utilis\u00e9 pour \u00e9valuer les performances du mod\u00e8le.","title":"S\u00e9paration des donn\u00e9es"},{"location":"#construction-de-notre-modele-ia","text":"Une fois les donn\u00e9es nettoy\u00e9es et pr\u00e9trait\u00e9es, il est temps de choisir un mod\u00e8le d'IA et de l'entra\u00eener sur l'ensemble d'entra\u00eenement. Le choix du mod\u00e8le d\u00e9pend du type de probl\u00e8me \u00e0 r\u00e9soudre et du type de donn\u00e9es. L'intelligence artificielle (IA) est un domaine de l'informatique qui vise \u00e0 cr\u00e9er des machines capables de r\u00e9aliser des t\u00e2ches qui n\u00e9cessitent normalement l'intelligence humaine. Cette discipline est bas\u00e9e sur le d\u00e9veloppement de programmes informatiques qui peuvent apprendre et s'adapter \u00e0 de nouvelles situations, et ainsi ex\u00e9cuter des t\u00e2ches sans intervention humaine. Il existe plusieurs modes d'apprentissage : Le Machine Learning (apprentissage automatique en fran\u00e7ais) est une branche de l'intelligence artificielle qui se concentre sur l'apprentissage de mod\u00e8les \u00e0 partir de donn\u00e9es, afin de r\u00e9aliser des t\u00e2ches de pr\u00e9diction ou de classification. Les mod\u00e8les peuvent \u00eatre entra\u00een\u00e9s \u00e0 l'aide de diff\u00e9rentes techniques, notamment l'apprentissage supervis\u00e9 (o\u00f9 les donn\u00e9es sont \u00e9tiquet\u00e9es pour indiquer la r\u00e9ponse souhait\u00e9e) et l'apprentissage non supervis\u00e9 (o\u00f9 les donn\u00e9es ne sont pas \u00e9tiquet\u00e9es). Le Machine Learning est utilis\u00e9 dans de nombreux domaines, notamment la reconnaissance vocale, la reconnaissance d'images, la recommandation de produits et la d\u00e9tection de fraudes. Le Reinforcement Learning (apprentissage par renforcement en fran\u00e7ais) est une branche sp\u00e9cifique du Machine Learning qui se concentre sur l'apprentissage par essais et erreurs. Dans ce type d'apprentissage, un agent est plac\u00e9 dans un environnement et doit apprendre \u00e0 prendre des d\u00e9cisions pour maximiser une r\u00e9compense. L'agent prend une action, re\u00e7oit une r\u00e9compense ou une p\u00e9nalit\u00e9, et utilise cette information pour ajuster sa strat\u00e9gie. Le Reinforcement Learning est utilis\u00e9 pour des applications comme les jeux vid\u00e9o, les robots, et les syst\u00e8mes de recommandation personnalis\u00e9e. Le Deep Learning (apprentissage profond en fran\u00e7ais) est une sous-branche du Machine Learning qui utilise des r\u00e9seaux de neurones artificiels pour apprendre \u00e0 partir de donn\u00e9es. Les r\u00e9seaux de neurones artificiels sont des mod\u00e8les computationnels qui sont con\u00e7us pour imiter le fonctionnement du cerveau humain. Les r\u00e9seaux de neurones sont compos\u00e9s de plusieurs couches, chacune traitant une partie de l'information en entr\u00e9e pour produire une sortie. Le Deep Learning est utilis\u00e9 dans de nombreux domaines, notamment la reconnaissance vocale, la reconnaissance d'images, la traduction automatique et la reconnaissance de caract\u00e8res manuscrits.","title":"Construction de notre mod\u00e8le (IA)"},{"location":"#evaluation-du-modele","text":"Apr\u00e8s l'entra\u00eenement du mod\u00e8le, il est important d'\u00e9valuer sa performance sur l'ensemble de test. Il existe de nombreuses mesures pour \u00e9valuer les performances d'un mod\u00e8le, telles que l'exactitude, la pr\u00e9cision, le rappel et le F1-score.","title":"\u00c9valuation du mod\u00e8le"},{"location":"#optimisation-du-modele","text":"Si les performances du mod\u00e8le ne sont pas satisfaisantes, il peut \u00eatre n\u00e9cessaire de le r\u00e9ajuster ou de r\u00e9gler ses param\u00e8tres pour am\u00e9liorer ses performances.","title":"Optimisation du mod\u00e8le"},{"location":"#deploiement-du-modele","text":"Une fois que le mod\u00e8le a \u00e9t\u00e9 entra\u00een\u00e9 et \u00e9valu\u00e9 avec succ\u00e8s, il peut \u00eatre d\u00e9ploy\u00e9 dans un environnement de production. Cela peut inclure l'int\u00e9gration du mod\u00e8le dans une application ou un syst\u00e8me existant.","title":"D\u00e9ploiement du mod\u00e8le"},{"location":"api/","text":"API (Application Programming Interface) C'est quoi ? API (Application Programmable Interface) est un ensemble de fonctions permettant d'utiliser les services d\u2019une application. Une API est peut \u00eatre distribu\u00e9 localement dans un programme informatique (accessible uniquement par un algorithme), ou au contraire peut avoir vocation a \u00eatre utilis\u00e9e par un plus grand nombre d'acteurs. Il y a diff\u00e9rents type d'API. Dans ce cours, nous nous int\u00e9ressons surtout au API Web, c'est-\u00e0-dire celles qui permettent de fournir une interface accessible en ligne. Cela est le cas lorsque l'on effectue une requ\u00eate \u00e0 un serveur afin que l'on re\u00e7oive le r\u00e9sultat d'un traitement. Les API RESTful Une API RESTful (Representational State Transfer) est une architecture d'API Web qui repose sur le protocole HTTP et qui utilise ses m\u00e9thodes (GET, POST, PUT, DELETE) pour permettre aux clients de communiquer avec les serveurs et de r\u00e9cup\u00e9rer des donn\u00e9es. Les API RESTful sont largement utilis\u00e9es dans les applications Web et mobiles pour fournir une interface standardis\u00e9e et facile \u00e0 utiliser pour acc\u00e9der aux donn\u00e9es. Voici les principaux concepts de l'architecture RESTful : Ressources : les ressources sont les entit\u00e9s expos\u00e9es par l'API, telles que les utilisateurs, les produits, les commandes, etc. Verbes HTTP : les verbes HTTP (GET, POST, PUT, DELETE) sont utilis\u00e9s pour d\u00e9crire les op\u00e9rations sur les ressources. Le verbe GET est utilis\u00e9 pour r\u00e9cup\u00e9rer une ressource, POST pour cr\u00e9er une nouvelle ressource, PUT pour mettre \u00e0 jour une ressource existante et DELETE pour supprimer une ressource. Les routes : Les routes dans une RESTful API sont des points d'entr\u00e9e sp\u00e9cifiques dans l'API qui d\u00e9finissent les op\u00e9rations disponibles et les ressources accessibles. Une route est g\u00e9n\u00e9ralement un URI (Uniform Resource Identifier) qui identifie de mani\u00e8re unique une ressource et une m\u00e9thode HTTP qui sp\u00e9cifie l'action \u00e0 effectuer sur cette ressource. Repr\u00e9sentations : les repr\u00e9sentations sont les formats de donn\u00e9es utilis\u00e9s pour repr\u00e9senter les ressources, tels que JSON, XML, ou YAML. Le format le plus couramment utilis\u00e9 est le JSON (JavaScript Object Notation), car il est l\u00e9ger, facile \u00e0 lire et \u00e0 \u00e9crire. Les codes de r\u00e9ponse HTTP : les codes de r\u00e9ponse HTTP sont utilis\u00e9s pour indiquer le r\u00e9sultat d'une op\u00e9ration sur une ressource. Par exemple, le code 200 indique que la demande a \u00e9t\u00e9 trait\u00e9e avec succ\u00e8s, le code 201 indique que la ressource a \u00e9t\u00e9 cr\u00e9\u00e9e avec succ\u00e8s, le code 404 indique que la ressource n'a pas \u00e9t\u00e9 trouv\u00e9e, etc. En utilisant ces concepts, une API RESTful peut fournir une interface simple, coh\u00e9rente et facile \u00e0 utiliser pour acc\u00e9der aux donn\u00e9es et aux services d'un serveur. Les clients peuvent utiliser des requ\u00eates HTTP standard pour acc\u00e9der aux ressources et interagir avec le serveur, ce qui rend l'API RESTful tr\u00e8s flexible et facile \u00e0 int\u00e9grer dans des applications Web et mobiles. Flask Flask est une biblioth\u00e8que Python qui permet de cr\u00e9er des applications Web et des API RESTful. Voici un exemple de code pour cr\u00e9er une API RESTful simple avec Flask : from flask import Flask , jsonify , request app = Flask ( __name__ ) # D\u00e9finition de quelques donn\u00e9es pour notre API students = [ { 'id' : 1 , 'name' : 'Alice' , 'age' : 20 }, { 'id' : 2 , 'name' : 'Bob' , 'age' : 22 }, { 'id' : 3 , 'name' : 'Charlie' , 'age' : 21 } ] # Route pour r\u00e9cup\u00e9rer tous les \u00e9tudiants @app . route ( '/students' , methods = [ 'GET' ]) def get_students (): return jsonify ( students ) # Route pour r\u00e9cup\u00e9rer un \u00e9tudiant en particulier @app . route ( '/students/<int:id>' , methods = [ 'GET' ]) def get_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] return jsonify ( student ) # Route pour cr\u00e9er un nouvel \u00e9tudiant @app . route ( '/students' , methods = [ 'POST' ]) def create_student (): student = { 'id' : request . json [ 'id' ], 'name' : request . json [ 'name' ], 'age' : request . json [ 'age' ] } students . append ( student ) return jsonify ( student ) # Route pour mettre \u00e0 jour un \u00e9tudiant existant @app . route ( '/students/<int:id>' , methods = [ 'PUT' ]) def update_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] student [ 0 ][ 'name' ] = request . json . get ( 'name' , student [ 0 ][ 'name' ]) student [ 0 ][ 'age' ] = request . json . get ( 'age' , student [ 0 ][ 'age' ]) return jsonify ( student [ 0 ]) # Route pour supprimer un \u00e9tudiant existant @app . route ( '/students/<int:id>' , methods = [ 'DELETE' ]) def delete_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] students . remove ( student [ 0 ]) return jsonify ({ 'result' : True }) # Lancement de l'application Flask if __name__ == '__main__' : app . run ( debug = True ) Dans cet exemple, nous avons cr\u00e9\u00e9 une API RESTful simple qui permet de r\u00e9cup\u00e9rer, cr\u00e9er, mettre \u00e0 jour et supprimer des \u00e9tudiants. Nous avons utilis\u00e9 les d\u00e9corateurs @app.route pour d\u00e9finir les diff\u00e9rentes routes de notre API, ainsi que les m\u00e9thodes HTTP correspondantes (GET, POST, PUT, DELETE). Nous avons \u00e9galement utilis\u00e9 la fonction jsonify de Flask pour retourner des donn\u00e9es JSON \u00e0 partir de notre API. Pour tester notre API, nous pouvons utiliser un outil comme Postman ou simplement effectuer des requ\u00eates HTTP en utilisant l'URL de notre API (par exemple http://localhost:5000/students pour r\u00e9cup\u00e9rer tous les \u00e9tudiants).","title":"API (Application Programmable Interface)"},{"location":"api/#api-application-programming-interface","text":"","title":"API (Application Programming Interface)"},{"location":"api/#cest-quoi","text":"API (Application Programmable Interface) est un ensemble de fonctions permettant d'utiliser les services d\u2019une application. Une API est peut \u00eatre distribu\u00e9 localement dans un programme informatique (accessible uniquement par un algorithme), ou au contraire peut avoir vocation a \u00eatre utilis\u00e9e par un plus grand nombre d'acteurs. Il y a diff\u00e9rents type d'API. Dans ce cours, nous nous int\u00e9ressons surtout au API Web, c'est-\u00e0-dire celles qui permettent de fournir une interface accessible en ligne. Cela est le cas lorsque l'on effectue une requ\u00eate \u00e0 un serveur afin que l'on re\u00e7oive le r\u00e9sultat d'un traitement.","title":"C'est quoi ?"},{"location":"api/#les-api-restful","text":"Une API RESTful (Representational State Transfer) est une architecture d'API Web qui repose sur le protocole HTTP et qui utilise ses m\u00e9thodes (GET, POST, PUT, DELETE) pour permettre aux clients de communiquer avec les serveurs et de r\u00e9cup\u00e9rer des donn\u00e9es. Les API RESTful sont largement utilis\u00e9es dans les applications Web et mobiles pour fournir une interface standardis\u00e9e et facile \u00e0 utiliser pour acc\u00e9der aux donn\u00e9es. Voici les principaux concepts de l'architecture RESTful : Ressources : les ressources sont les entit\u00e9s expos\u00e9es par l'API, telles que les utilisateurs, les produits, les commandes, etc. Verbes HTTP : les verbes HTTP (GET, POST, PUT, DELETE) sont utilis\u00e9s pour d\u00e9crire les op\u00e9rations sur les ressources. Le verbe GET est utilis\u00e9 pour r\u00e9cup\u00e9rer une ressource, POST pour cr\u00e9er une nouvelle ressource, PUT pour mettre \u00e0 jour une ressource existante et DELETE pour supprimer une ressource. Les routes : Les routes dans une RESTful API sont des points d'entr\u00e9e sp\u00e9cifiques dans l'API qui d\u00e9finissent les op\u00e9rations disponibles et les ressources accessibles. Une route est g\u00e9n\u00e9ralement un URI (Uniform Resource Identifier) qui identifie de mani\u00e8re unique une ressource et une m\u00e9thode HTTP qui sp\u00e9cifie l'action \u00e0 effectuer sur cette ressource. Repr\u00e9sentations : les repr\u00e9sentations sont les formats de donn\u00e9es utilis\u00e9s pour repr\u00e9senter les ressources, tels que JSON, XML, ou YAML. Le format le plus couramment utilis\u00e9 est le JSON (JavaScript Object Notation), car il est l\u00e9ger, facile \u00e0 lire et \u00e0 \u00e9crire. Les codes de r\u00e9ponse HTTP : les codes de r\u00e9ponse HTTP sont utilis\u00e9s pour indiquer le r\u00e9sultat d'une op\u00e9ration sur une ressource. Par exemple, le code 200 indique que la demande a \u00e9t\u00e9 trait\u00e9e avec succ\u00e8s, le code 201 indique que la ressource a \u00e9t\u00e9 cr\u00e9\u00e9e avec succ\u00e8s, le code 404 indique que la ressource n'a pas \u00e9t\u00e9 trouv\u00e9e, etc. En utilisant ces concepts, une API RESTful peut fournir une interface simple, coh\u00e9rente et facile \u00e0 utiliser pour acc\u00e9der aux donn\u00e9es et aux services d'un serveur. Les clients peuvent utiliser des requ\u00eates HTTP standard pour acc\u00e9der aux ressources et interagir avec le serveur, ce qui rend l'API RESTful tr\u00e8s flexible et facile \u00e0 int\u00e9grer dans des applications Web et mobiles.","title":"Les API RESTful"},{"location":"api/#flask","text":"Flask est une biblioth\u00e8que Python qui permet de cr\u00e9er des applications Web et des API RESTful. Voici un exemple de code pour cr\u00e9er une API RESTful simple avec Flask : from flask import Flask , jsonify , request app = Flask ( __name__ ) # D\u00e9finition de quelques donn\u00e9es pour notre API students = [ { 'id' : 1 , 'name' : 'Alice' , 'age' : 20 }, { 'id' : 2 , 'name' : 'Bob' , 'age' : 22 }, { 'id' : 3 , 'name' : 'Charlie' , 'age' : 21 } ] # Route pour r\u00e9cup\u00e9rer tous les \u00e9tudiants @app . route ( '/students' , methods = [ 'GET' ]) def get_students (): return jsonify ( students ) # Route pour r\u00e9cup\u00e9rer un \u00e9tudiant en particulier @app . route ( '/students/<int:id>' , methods = [ 'GET' ]) def get_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] return jsonify ( student ) # Route pour cr\u00e9er un nouvel \u00e9tudiant @app . route ( '/students' , methods = [ 'POST' ]) def create_student (): student = { 'id' : request . json [ 'id' ], 'name' : request . json [ 'name' ], 'age' : request . json [ 'age' ] } students . append ( student ) return jsonify ( student ) # Route pour mettre \u00e0 jour un \u00e9tudiant existant @app . route ( '/students/<int:id>' , methods = [ 'PUT' ]) def update_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] student [ 0 ][ 'name' ] = request . json . get ( 'name' , student [ 0 ][ 'name' ]) student [ 0 ][ 'age' ] = request . json . get ( 'age' , student [ 0 ][ 'age' ]) return jsonify ( student [ 0 ]) # Route pour supprimer un \u00e9tudiant existant @app . route ( '/students/<int:id>' , methods = [ 'DELETE' ]) def delete_student ( id ): student = [ student for student in students if student [ 'id' ] == id ] students . remove ( student [ 0 ]) return jsonify ({ 'result' : True }) # Lancement de l'application Flask if __name__ == '__main__' : app . run ( debug = True ) Dans cet exemple, nous avons cr\u00e9\u00e9 une API RESTful simple qui permet de r\u00e9cup\u00e9rer, cr\u00e9er, mettre \u00e0 jour et supprimer des \u00e9tudiants. Nous avons utilis\u00e9 les d\u00e9corateurs @app.route pour d\u00e9finir les diff\u00e9rentes routes de notre API, ainsi que les m\u00e9thodes HTTP correspondantes (GET, POST, PUT, DELETE). Nous avons \u00e9galement utilis\u00e9 la fonction jsonify de Flask pour retourner des donn\u00e9es JSON \u00e0 partir de notre API. Pour tester notre API, nous pouvons utiliser un outil comme Postman ou simplement effectuer des requ\u00eates HTTP en utilisant l'URL de notre API (par exemple http://localhost:5000/students pour r\u00e9cup\u00e9rer tous les \u00e9tudiants).","title":"Flask"},{"location":"dl/","text":"C'est quoi le Deep Learning Le Deep Learning est une sous-branche du Machine Learning qui utilise des r\u00e9seaux de neurones profonds pour apprendre des repr\u00e9sentations hi\u00e9rarchiques de donn\u00e9es complexes. Contrairement au Machine Learning classique, qui se concentre sur l'extraction de caract\u00e9ristiques manuellement con\u00e7ues \u00e0 partir de donn\u00e9es, le Deep Learning permet aux mod\u00e8les d'apprendre automatiquement \u00e0 partir de donn\u00e9es brutes sans la n\u00e9cessit\u00e9 d'ing\u00e9nierie de caract\u00e9ristiques. Les avantages du Deep Learning incluent : Performance am\u00e9lior\u00e9e : Les mod\u00e8les de Deep Learning peuvent apprendre \u00e0 partir de donn\u00e9es brutes et trouver des caract\u00e9ristiques importantes pour une t\u00e2che donn\u00e9e, ce qui peut conduire \u00e0 des performances am\u00e9lior\u00e9es sur une grande vari\u00e9t\u00e9 de t\u00e2ches, y compris la vision par ordinateur, la reconnaissance vocale et le traitement du langage naturel. Scalabilit\u00e9 : Les r\u00e9seaux de neurones profonds peuvent \u00eatre con\u00e7us pour traiter des donn\u00e9es massives et peuvent \u00eatre utilis\u00e9s dans des environnements distribu\u00e9s pour am\u00e9liorer encore les performances. Adaptabilit\u00e9 : Les mod\u00e8les de Deep Learning peuvent \u00eatre adapt\u00e9s \u00e0 de nouveaux jeux de donn\u00e9es sans avoir besoin de modifications majeures de l'architecture du mod\u00e8le, ce qui les rend plus flexibles et adaptables que les mod\u00e8les de Machine Learning classiques. Cependant, le Deep Learning pr\u00e9sente \u00e9galement quelques inconv\u00e9nients, notamment : Co\u00fbt en ressources : L'entra\u00eenement de mod\u00e8les de Deep Learning peut \u00eatre tr\u00e8s co\u00fbteux en temps et en ressources informatiques, en particulier pour des jeux de donn\u00e9es massifs. L'inf\u00e9rence peut \u00e9galement \u00eatre co\u00fbteuse, en particulier sur des dispositifs mobiles ou des syst\u00e8mes embarqu\u00e9s. Manque de transparence : Les mod\u00e8les de Deep Learning peuvent \u00eatre difficiles \u00e0 interpr\u00e9ter, car ils sont souvent con\u00e7us avec des couches cach\u00e9es qui rendent difficile la compr\u00e9hension de la mani\u00e8re dont le mod\u00e8le prend des d\u00e9cisions. Cela peut rendre le d\u00e9beugage et le d\u00e9pannage plus difficiles. Besoin de donn\u00e9es de haute qualit\u00e9 : Les mod\u00e8les de Deep Learning n\u00e9cessitent souvent des ensembles de donn\u00e9es massifs et de haute qualit\u00e9 pour obtenir des performances optimales. Cela peut rendre l'entra\u00eenement de mod\u00e8les de Deep Learning difficile pour les domaines o\u00f9 les donn\u00e9es sont rares ou co\u00fbteuses \u00e0 collecter. C'est quoi un Neurone Un neurone est la plus petite unit\u00e9 de traitement dans un r\u00e9seau de neurones. Il re\u00e7oit des entr\u00e9es pond\u00e9r\u00e9es, les somme et les passe \u00e0 travers une fonction d'activation non-lin\u00e9aire pour produire une sortie. Les poids sont des param\u00e8tres qui sont ajust\u00e9s par le mod\u00e8le pendant l'apprentissage pour minimiser une fonction de co\u00fbt. C'est quoi les couches de Neurones Pour un probl\u00e8me non lin\u00e9aire ou utilise des couches. Une couche de neurones est un groupe de neurones qui traitent simultan\u00e9ment les entr\u00e9es qu'ils re\u00e7oivent. Les couches de neurones sont g\u00e9n\u00e9ralement organis\u00e9es en s\u00e9quences, avec chaque couche prenant en entr\u00e9e les sorties de la couche pr\u00e9c\u00e9dente. On peut distingue distinguer deux blocs de couches : Une ou plusieurs couches cach\u00e9es compos\u00e9es de plusieurs neurones Une couche de sortie compos\u00e9e de K neurones \\(y = [y_1, ...,y_k]\\) En r\u00e9gression : Sortie continue/un seule neurone => la derni\u00e9re couche a une fonction d'activation lin\u00e9aire En classification : La derniere couche a K neurones (K = Nombre de classes) => La derni\u00e8re couche a une fonction d'activation softmax qui transforme la sortie du r\u00e9seau en probabilit\u00e9s d'appartenance aux classes. C'est quoi les fonctions d'activation Une fonction d\u2019activation est une fonction math\u00e9matique utilis\u00e9 sur un signal. Elle va reproduire le potentiel d\u2019activation que l\u2019on retrouve dans le domaine de la biologie du cerveau humain. Elle va permettre le passage d\u2019information ou non de l\u2019information si le seuil de stimulation est atteint. Concr\u00e8tement, elle va avoir pour r\u00f4le de d\u00e9cider si on active ou non une r\u00e9ponse du neurone. Voici les principales fonctions d\u2019activations que l\u2019on peut trouver dans des r\u00e9seaux de neurones : Linear : Utilis\u00e9 en couche de sortie pour une utilisation pour une r\u00e9gression. On peut la caract\u00e9riser de nulle, puisque les unit\u00e9s de sortie seront identiques \u00e0 leur niveau d\u2019entr\u00e9. Intervalle de sortie (-\u221e;+\u221e). Sigmoid (logistic) : Fonction la plus populaire depuis des d\u00e9cennies. Mais aujourd\u2019hui, elle devient beaucoup moins efficace par rapport \u00e0 d\u2019autre pour une utilisation pour les couches cach\u00e9es. Utilis\u00e9 en couche de sortie pour de la classification binaire. Intervalle de sortie : {0,1} TanH : Utilis\u00e9 pour des RNN pour des donn\u00e9es en continue. Intervalle de sortie : (-1,1) Softmax : Utilis\u00e9 pour de la multi classification en couche de sortie. Intervalle de sortie (-\u221e;+\u221e). ReLU ( Rectified Linear Unit ) : Ce sont les fonctions les plus populaires de nos jours. Elles permettent un entrainement plus rapide compar\u00e9 aux fonctions sigmoid et tanh, \u00e9tant plus l\u00e9g\u00e8res. Attention au ph\u00e9nom\u00e8ne de \u2018Dying ReLU\u2019, auquel on pr\u00e9f\u00e9rera les variations de ReLU. Tr\u00e8s utilis\u00e9 pour les CNN, RBM, et les r\u00e9seaux de multi perceptron. Intervalle de sortie (0;+\u221e). Leaky ReLU : La Leakey Relu permet d\u2019ajouter une variante pour les nombres n\u00e9gatifs, ainsi les neurones ne meurent jamais. Ils entrent dans un long coma mais on toujours la chance de se r\u00e9veiller \u00e0 un moment donn\u00e9. Intervalle de sortie (-\u221e;+\u221e). C'est quoi la backpropagation (Comment un r\u00e9seau s'entraine) La r\u00e9tropropagation (backpropagation en anglais) est l'algorithme d'optimisation utilis\u00e9 pour entra\u00eener les r\u00e9seaux de neurones. Il s'agit d'une m\u00e9thode de calcul de gradient qui permet de calculer les gradients de l'erreur par rapport aux poids de chaque neurone dans le r\u00e9seau, en utilisant la r\u00e8gle de la cha\u00eene. Le processus de r\u00e9tropropagation commence par une \u00e9tape de propagation avant (forward pass) dans laquelle les entr\u00e9es sont pr\u00e9sent\u00e9es au r\u00e9seau et les pr\u00e9dictions sont calcul\u00e9es en passant successivement \u00e0 travers chaque couche du r\u00e9seau. Ensuite, l'erreur de pr\u00e9diction est calcul\u00e9e en comparant les pr\u00e9dictions du r\u00e9seau aux valeurs r\u00e9elles de sortie. L'objectif de l'apprentissage est de minimiser cette erreur. \u00c0 partir de l'erreur de pr\u00e9diction, la r\u00e9tropropagation calcule le gradient de l'erreur par rapport \u00e0 chaque poids dans le r\u00e9seau, en utilisant la r\u00e8gle de la cha\u00eene pour calculer les gradients \u00e0 travers les couches du r\u00e9seau en partant de la derni\u00e8re couche vers la premi\u00e8re. Une fois que les gradients ont \u00e9t\u00e9 calcul\u00e9s, ils peuvent \u00eatre utilis\u00e9s pour ajuster les poids dans le r\u00e9seau en utilisant une m\u00e9thode d'optimisation telle que la descente de gradient stochastique (SGD en anglais). L'ensemble du processus de r\u00e9tropropagation est r\u00e9p\u00e9t\u00e9 de nombreuses fois, en pr\u00e9sentant diff\u00e9rentes donn\u00e9es d'entra\u00eenement au r\u00e9seau \u00e0 chaque it\u00e9ration, jusqu'\u00e0 ce que le r\u00e9seau converge vers une solution qui minimise l'erreur de pr\u00e9diction moyenne sur l'ensemble des donn\u00e9es d'entra\u00eenement. Terminologie : Batch : sous ensemble de la base train Epoch : nombre de fois que tous les exemples sont vus en apprentissage It\u00e9ration : nombre de batchs vus en apprentissage Comprendre l'overfitting et underfitting On souhaite avoir un r\u00e9seau qui puisse effectuer des pr\u00e9dictions sur de nouvelles donn\u00e9es. Selon la fa\u00e7on dont est entrain\u00e9 le model, on peut se heurter \u00e0 2 probl\u00e8mes : Sur apprentissage : Cela repr\u00e9sente un mod\u00e8le qui a appris par c\u0153ur ses donn\u00e9es d\u2019entrainement, qui fonctionne donc bien sur le jeu d\u2019entrainement mais pas de validation. Il effectue alors de mauvaise pr\u00e9diction sur de nouvelles, car elles ne sont pas exactement les m\u00eames que celle du jeu d\u2019entrainement. Pour y rem\u00e9dier, il faut am\u00e9liorer la flexibilit\u00e9 du mod\u00e8le, et donc jouer sur des concept de r\u00e9gularisation par exemple, ou encore d\u2019early stopping. Sous apprentissage : Ce cas-ci repr\u00e9sente un mod\u00e8le qui n\u2019arrive pas \u00e0 d\u00e9duire des informations du jeu de donn\u00e9es. Il n\u2019apprend donc pas assez et r\u00e9alise de mauvaise pr\u00e9diction sur le jeu d\u2019entrainement. Il faut donc complexifier le r\u00e9seau, car il ne taille pas bien par rapport aux types de donn\u00e9es d\u2019entr\u00e9es. En effet, il n\u2019arrive pas \u00e0 capter la relation entre les donn\u00e9es d\u2019entr\u00e9es et leur label. Dans le cas o\u00f9 la pr\u00e9cision du r\u00e9seau n\u2019est ni bonne sur le jeu d\u2019entrainement, ni sur celui de validation, c\u2019est que le r\u00e9seau n\u2019a pas eu assez de temps pour apprendre des donn\u00e9es. Il faut donc augmenter le nombre d\u2019it\u00e9ration, ou augmenter la taille du jeu de donn\u00e9e. R\u00e9gularisation du r\u00e9seau avec les Dropout Le Dropout est une technique de r\u00e9gularisation utilis\u00e9e pour r\u00e9duire le surapprentissage dans les r\u00e9seaux de neurones profonds. On va souhaiter favoriser l\u2019extraction de caract\u00e9ristique de fa\u00e7on ind\u00e9pendante, afin d\u2019apprendre des caract\u00e9ristique plus g\u00e9n\u00e9ral et plus diverse. Cela va consister \u00e0 \u2018\u00e9teindre\u2019, \u00e0 d\u00e9sactiver certains neurones du mod\u00e8le, et ce de fa\u00e7on al\u00e9atoire d\u2019une m\u00eame couche, qui ne contribuera donc ni \u00e0 la phase de feedforward, ni \u00e0 la phase de backpropagation. D\u2019un point de vue du r\u00e9seau, cela revient \u00e0 instancier la valeur en sortie d\u2019une fonction d\u2019activation \u00e0 0","title":"Introduction au Deep Learning"},{"location":"dl/#cest-quoi-le-deep-learning","text":"Le Deep Learning est une sous-branche du Machine Learning qui utilise des r\u00e9seaux de neurones profonds pour apprendre des repr\u00e9sentations hi\u00e9rarchiques de donn\u00e9es complexes. Contrairement au Machine Learning classique, qui se concentre sur l'extraction de caract\u00e9ristiques manuellement con\u00e7ues \u00e0 partir de donn\u00e9es, le Deep Learning permet aux mod\u00e8les d'apprendre automatiquement \u00e0 partir de donn\u00e9es brutes sans la n\u00e9cessit\u00e9 d'ing\u00e9nierie de caract\u00e9ristiques. Les avantages du Deep Learning incluent : Performance am\u00e9lior\u00e9e : Les mod\u00e8les de Deep Learning peuvent apprendre \u00e0 partir de donn\u00e9es brutes et trouver des caract\u00e9ristiques importantes pour une t\u00e2che donn\u00e9e, ce qui peut conduire \u00e0 des performances am\u00e9lior\u00e9es sur une grande vari\u00e9t\u00e9 de t\u00e2ches, y compris la vision par ordinateur, la reconnaissance vocale et le traitement du langage naturel. Scalabilit\u00e9 : Les r\u00e9seaux de neurones profonds peuvent \u00eatre con\u00e7us pour traiter des donn\u00e9es massives et peuvent \u00eatre utilis\u00e9s dans des environnements distribu\u00e9s pour am\u00e9liorer encore les performances. Adaptabilit\u00e9 : Les mod\u00e8les de Deep Learning peuvent \u00eatre adapt\u00e9s \u00e0 de nouveaux jeux de donn\u00e9es sans avoir besoin de modifications majeures de l'architecture du mod\u00e8le, ce qui les rend plus flexibles et adaptables que les mod\u00e8les de Machine Learning classiques. Cependant, le Deep Learning pr\u00e9sente \u00e9galement quelques inconv\u00e9nients, notamment : Co\u00fbt en ressources : L'entra\u00eenement de mod\u00e8les de Deep Learning peut \u00eatre tr\u00e8s co\u00fbteux en temps et en ressources informatiques, en particulier pour des jeux de donn\u00e9es massifs. L'inf\u00e9rence peut \u00e9galement \u00eatre co\u00fbteuse, en particulier sur des dispositifs mobiles ou des syst\u00e8mes embarqu\u00e9s. Manque de transparence : Les mod\u00e8les de Deep Learning peuvent \u00eatre difficiles \u00e0 interpr\u00e9ter, car ils sont souvent con\u00e7us avec des couches cach\u00e9es qui rendent difficile la compr\u00e9hension de la mani\u00e8re dont le mod\u00e8le prend des d\u00e9cisions. Cela peut rendre le d\u00e9beugage et le d\u00e9pannage plus difficiles. Besoin de donn\u00e9es de haute qualit\u00e9 : Les mod\u00e8les de Deep Learning n\u00e9cessitent souvent des ensembles de donn\u00e9es massifs et de haute qualit\u00e9 pour obtenir des performances optimales. Cela peut rendre l'entra\u00eenement de mod\u00e8les de Deep Learning difficile pour les domaines o\u00f9 les donn\u00e9es sont rares ou co\u00fbteuses \u00e0 collecter.","title":"C'est quoi le Deep Learning"},{"location":"dl/#cest-quoi-un-neurone","text":"Un neurone est la plus petite unit\u00e9 de traitement dans un r\u00e9seau de neurones. Il re\u00e7oit des entr\u00e9es pond\u00e9r\u00e9es, les somme et les passe \u00e0 travers une fonction d'activation non-lin\u00e9aire pour produire une sortie. Les poids sont des param\u00e8tres qui sont ajust\u00e9s par le mod\u00e8le pendant l'apprentissage pour minimiser une fonction de co\u00fbt.","title":"C'est quoi un Neurone"},{"location":"dl/#cest-quoi-les-couches-de-neurones","text":"Pour un probl\u00e8me non lin\u00e9aire ou utilise des couches. Une couche de neurones est un groupe de neurones qui traitent simultan\u00e9ment les entr\u00e9es qu'ils re\u00e7oivent. Les couches de neurones sont g\u00e9n\u00e9ralement organis\u00e9es en s\u00e9quences, avec chaque couche prenant en entr\u00e9e les sorties de la couche pr\u00e9c\u00e9dente. On peut distingue distinguer deux blocs de couches : Une ou plusieurs couches cach\u00e9es compos\u00e9es de plusieurs neurones Une couche de sortie compos\u00e9e de K neurones \\(y = [y_1, ...,y_k]\\) En r\u00e9gression : Sortie continue/un seule neurone => la derni\u00e9re couche a une fonction d'activation lin\u00e9aire En classification : La derniere couche a K neurones (K = Nombre de classes) => La derni\u00e8re couche a une fonction d'activation softmax qui transforme la sortie du r\u00e9seau en probabilit\u00e9s d'appartenance aux classes.","title":"C'est quoi les couches de Neurones"},{"location":"dl/#cest-quoi-les-fonctions-dactivation","text":"Une fonction d\u2019activation est une fonction math\u00e9matique utilis\u00e9 sur un signal. Elle va reproduire le potentiel d\u2019activation que l\u2019on retrouve dans le domaine de la biologie du cerveau humain. Elle va permettre le passage d\u2019information ou non de l\u2019information si le seuil de stimulation est atteint. Concr\u00e8tement, elle va avoir pour r\u00f4le de d\u00e9cider si on active ou non une r\u00e9ponse du neurone. Voici les principales fonctions d\u2019activations que l\u2019on peut trouver dans des r\u00e9seaux de neurones : Linear : Utilis\u00e9 en couche de sortie pour une utilisation pour une r\u00e9gression. On peut la caract\u00e9riser de nulle, puisque les unit\u00e9s de sortie seront identiques \u00e0 leur niveau d\u2019entr\u00e9. Intervalle de sortie (-\u221e;+\u221e). Sigmoid (logistic) : Fonction la plus populaire depuis des d\u00e9cennies. Mais aujourd\u2019hui, elle devient beaucoup moins efficace par rapport \u00e0 d\u2019autre pour une utilisation pour les couches cach\u00e9es. Utilis\u00e9 en couche de sortie pour de la classification binaire. Intervalle de sortie : {0,1} TanH : Utilis\u00e9 pour des RNN pour des donn\u00e9es en continue. Intervalle de sortie : (-1,1) Softmax : Utilis\u00e9 pour de la multi classification en couche de sortie. Intervalle de sortie (-\u221e;+\u221e). ReLU ( Rectified Linear Unit ) : Ce sont les fonctions les plus populaires de nos jours. Elles permettent un entrainement plus rapide compar\u00e9 aux fonctions sigmoid et tanh, \u00e9tant plus l\u00e9g\u00e8res. Attention au ph\u00e9nom\u00e8ne de \u2018Dying ReLU\u2019, auquel on pr\u00e9f\u00e9rera les variations de ReLU. Tr\u00e8s utilis\u00e9 pour les CNN, RBM, et les r\u00e9seaux de multi perceptron. Intervalle de sortie (0;+\u221e). Leaky ReLU : La Leakey Relu permet d\u2019ajouter une variante pour les nombres n\u00e9gatifs, ainsi les neurones ne meurent jamais. Ils entrent dans un long coma mais on toujours la chance de se r\u00e9veiller \u00e0 un moment donn\u00e9. Intervalle de sortie (-\u221e;+\u221e).","title":"C'est quoi les fonctions d'activation"},{"location":"dl/#cest-quoi-la-backpropagation-comment-un-reseau-sentraine","text":"La r\u00e9tropropagation (backpropagation en anglais) est l'algorithme d'optimisation utilis\u00e9 pour entra\u00eener les r\u00e9seaux de neurones. Il s'agit d'une m\u00e9thode de calcul de gradient qui permet de calculer les gradients de l'erreur par rapport aux poids de chaque neurone dans le r\u00e9seau, en utilisant la r\u00e8gle de la cha\u00eene. Le processus de r\u00e9tropropagation commence par une \u00e9tape de propagation avant (forward pass) dans laquelle les entr\u00e9es sont pr\u00e9sent\u00e9es au r\u00e9seau et les pr\u00e9dictions sont calcul\u00e9es en passant successivement \u00e0 travers chaque couche du r\u00e9seau. Ensuite, l'erreur de pr\u00e9diction est calcul\u00e9e en comparant les pr\u00e9dictions du r\u00e9seau aux valeurs r\u00e9elles de sortie. L'objectif de l'apprentissage est de minimiser cette erreur. \u00c0 partir de l'erreur de pr\u00e9diction, la r\u00e9tropropagation calcule le gradient de l'erreur par rapport \u00e0 chaque poids dans le r\u00e9seau, en utilisant la r\u00e8gle de la cha\u00eene pour calculer les gradients \u00e0 travers les couches du r\u00e9seau en partant de la derni\u00e8re couche vers la premi\u00e8re. Une fois que les gradients ont \u00e9t\u00e9 calcul\u00e9s, ils peuvent \u00eatre utilis\u00e9s pour ajuster les poids dans le r\u00e9seau en utilisant une m\u00e9thode d'optimisation telle que la descente de gradient stochastique (SGD en anglais). L'ensemble du processus de r\u00e9tropropagation est r\u00e9p\u00e9t\u00e9 de nombreuses fois, en pr\u00e9sentant diff\u00e9rentes donn\u00e9es d'entra\u00eenement au r\u00e9seau \u00e0 chaque it\u00e9ration, jusqu'\u00e0 ce que le r\u00e9seau converge vers une solution qui minimise l'erreur de pr\u00e9diction moyenne sur l'ensemble des donn\u00e9es d'entra\u00eenement. Terminologie : Batch : sous ensemble de la base train Epoch : nombre de fois que tous les exemples sont vus en apprentissage It\u00e9ration : nombre de batchs vus en apprentissage","title":"C'est quoi la backpropagation (Comment un r\u00e9seau s'entraine)"},{"location":"dl/#comprendre-loverfitting-et-underfitting","text":"On souhaite avoir un r\u00e9seau qui puisse effectuer des pr\u00e9dictions sur de nouvelles donn\u00e9es. Selon la fa\u00e7on dont est entrain\u00e9 le model, on peut se heurter \u00e0 2 probl\u00e8mes : Sur apprentissage : Cela repr\u00e9sente un mod\u00e8le qui a appris par c\u0153ur ses donn\u00e9es d\u2019entrainement, qui fonctionne donc bien sur le jeu d\u2019entrainement mais pas de validation. Il effectue alors de mauvaise pr\u00e9diction sur de nouvelles, car elles ne sont pas exactement les m\u00eames que celle du jeu d\u2019entrainement. Pour y rem\u00e9dier, il faut am\u00e9liorer la flexibilit\u00e9 du mod\u00e8le, et donc jouer sur des concept de r\u00e9gularisation par exemple, ou encore d\u2019early stopping. Sous apprentissage : Ce cas-ci repr\u00e9sente un mod\u00e8le qui n\u2019arrive pas \u00e0 d\u00e9duire des informations du jeu de donn\u00e9es. Il n\u2019apprend donc pas assez et r\u00e9alise de mauvaise pr\u00e9diction sur le jeu d\u2019entrainement. Il faut donc complexifier le r\u00e9seau, car il ne taille pas bien par rapport aux types de donn\u00e9es d\u2019entr\u00e9es. En effet, il n\u2019arrive pas \u00e0 capter la relation entre les donn\u00e9es d\u2019entr\u00e9es et leur label. Dans le cas o\u00f9 la pr\u00e9cision du r\u00e9seau n\u2019est ni bonne sur le jeu d\u2019entrainement, ni sur celui de validation, c\u2019est que le r\u00e9seau n\u2019a pas eu assez de temps pour apprendre des donn\u00e9es. Il faut donc augmenter le nombre d\u2019it\u00e9ration, ou augmenter la taille du jeu de donn\u00e9e.","title":"Comprendre l'overfitting et underfitting"},{"location":"dl/#regularisation-du-reseau-avec-les-dropout","text":"Le Dropout est une technique de r\u00e9gularisation utilis\u00e9e pour r\u00e9duire le surapprentissage dans les r\u00e9seaux de neurones profonds. On va souhaiter favoriser l\u2019extraction de caract\u00e9ristique de fa\u00e7on ind\u00e9pendante, afin d\u2019apprendre des caract\u00e9ristique plus g\u00e9n\u00e9ral et plus diverse. Cela va consister \u00e0 \u2018\u00e9teindre\u2019, \u00e0 d\u00e9sactiver certains neurones du mod\u00e8le, et ce de fa\u00e7on al\u00e9atoire d\u2019une m\u00eame couche, qui ne contribuera donc ni \u00e0 la phase de feedforward, ni \u00e0 la phase de backpropagation. D\u2019un point de vue du r\u00e9seau, cela revient \u00e0 instancier la valeur en sortie d\u2019une fonction d\u2019activation \u00e0 0","title":"R\u00e9gularisation du r\u00e9seau avec les Dropout"},{"location":"dltypes/","text":"Il existe diff\u00e9rents types de r\u00e9seaux de neurones, chacune ayant sa propre fonction et sa propre m\u00e9thode de traitement des donn\u00e9es d'entr\u00e9e. Voici un aper\u00e7u de quelques types courants de couches de neurones : Les r\u00e9seaux de neurones denses (Fully Connected Layers) Un r\u00e9seau de neurones dense, \u00e9galement appel\u00e9 r\u00e9seau de neurones \u00e0 couches enti\u00e8rement connect\u00e9es, est un type de r\u00e9seau de neurones o\u00f9 chaque neurone dans une couche est connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente et de la couche suivante. Cela signifie que chaque neurone de la couche actuelle prend en compte toutes les activations de la couche pr\u00e9c\u00e9dente et renvoie une activation qui est propag\u00e9e \u00e0 tous les neurones de la couche suivante. La sortie d'une couche dense est calcul\u00e9e comme suit : \\[y_j = f\\left(\\sum_{i=1}^n w_{ji}x_i + b_j\\right)\\] o\u00f9 : \\(y_j\\) est la sortie du \\(j\\) -\u00e8me neurone de la couche dense. \\(x_i\\) est l'activation du \\(i\\) -\u00e8me neurone de la couche pr\u00e9c\u00e9dente. \\(w_{ji}\\) est le poids de la connexion entre le \\(i\\) -\u00e8me neurone de la couche pr\u00e9c\u00e9dente et le \\(j\\) -\u00e8me neurone de la couche dense. \\(b_j\\) est le biais (un param\u00e8tre suppl\u00e9mentaire) du \\(j\\) -\u00e8me neurone de la couche dense. \\(f\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. La fonction d'activation \\(f\\) peut \u00eatre choisie en fonction du probl\u00e8me \u00e0 r\u00e9soudre. Les r\u00e9seaux de neurones convolutifs (Convolutional Neural Networks) Les r\u00e9seaux de neurones convolutifs (Convolutional Neural Networks ou CNNs) sont un type de r\u00e9seau de neurones sp\u00e9cialement con\u00e7us pour le traitement des images. Ils ont \u00e9t\u00e9 introduits pour la premi\u00e8re fois par Yann LeCun et ses coll\u00e8gues en 1998 pour la reconnaissance de caract\u00e8res manuscrits. Les CNNs sont bas\u00e9s sur des filtres de convolution, qui sont des matrices de poids qui sont appliqu\u00e9es \u00e0 des r\u00e9gions de l'image. Les filtres de convolution permettent d'extraire des caract\u00e9ristiques visuelles \u00e0 diff\u00e9rentes \u00e9chelles. Par exemple, un filtre de convolution peut \u00eatre con\u00e7u pour d\u00e9tecter les bords dans une image, tandis qu'un autre filtre peut \u00eatre con\u00e7u pour d\u00e9tecter les coins. Un r\u00e9seau de neurones convolutif se compose de plusieurs couches. La premi\u00e8re couche est la couche d'entr\u00e9e, qui contient l'image brute. Les couches suivantes sont des couches de convolution, qui appliquent des filtres de convolution \u00e0 l'image. Chaque filtre de convolution produit une carte de caract\u00e9ristiques, qui est une image qui met en \u00e9vidence les caract\u00e9ristiques sp\u00e9cifiques d\u00e9tect\u00e9es par le filtre. Les cartes de caract\u00e9ristiques sont ensuite pass\u00e9es \u00e0 une fonction d'activation, qui introduit une non-lin\u00e9arit\u00e9 dans le mod\u00e8le. Apr\u00e8s les couches de convolution, il y a g\u00e9n\u00e9ralement des couches de regroupement (pooling layers), qui r\u00e9duisent la dimensionnalit\u00e9 de l'image en prenant le maximum, la moyenne ou la somme de petites r\u00e9gions de l'image. Les couches de regroupement permettent de r\u00e9duire le temps de calcul et de rendre le mod\u00e8le plus robuste aux variations mineures dans l'image. Apr\u00e8s les couches de regroupement, il y a g\u00e9n\u00e9ralement des couches enti\u00e8rement connect\u00e9es (Fully Connected Layers), qui sont similaires aux couches de neurones denses des r\u00e9seaux de neurones classiques. Les couches enti\u00e8rement connect\u00e9es prennent les cartes de caract\u00e9ristiques en entr\u00e9e et les transforment en une repr\u00e9sentation de sortie qui est utilis\u00e9e pour la classification ou la r\u00e9gression. Les CNNs sont entra\u00een\u00e9s \u00e0 l'aide d'un algorithme d'optimisation, tel que la descente de gradient stochastique (Stochastic Gradient Descent), pour minimiser une fonction de perte (loss function) qui mesure la diff\u00e9rence entre la sortie pr\u00e9dite et la sortie r\u00e9elle. Pendant l'entra\u00eenement, les poids des filtres de convolution et des couches enti\u00e8rement connect\u00e9es sont ajust\u00e9s pour minimiser la fonction de perte. L'\u00e9quation pour la sortie d'une couche de convolution: \\[y(i,j,k) = \\text{activation}\\left(\\sum_{l=1}^{p}\\sum_{m=1}^{q}\\sum_{n=1}^{r} w(m,n,l,k) x(i+m-1,j+n-1,l)\\right)\\] o\u00f9 : \\(y(i,j,k)\\) est la sortie du \\(k\\) -\u00e8me filtre \u00e0 la position \\((i,j)\\) dans la carte de caract\u00e9ristiques de sortie. \\(\\text{activation}\\) est la fonction d'activation appliqu\u00e9e \u00e9l\u00e9ment par \u00e9l\u00e9ment \u00e0 la somme des entr\u00e9es pond\u00e9r\u00e9es. \\(w(m,n,l,k)\\) est le poids du \\(k\\) -\u00e8me filtre \u00e0 la position \\((m,n)\\) dans le \\(l\\) -\u00e8me canal d'entr\u00e9e. \\(x(i+m-1,j+n-1,l)\\) est l'activation d'entr\u00e9e \u00e0 la position \\((i+m-1,j+n-1)\\) dans le \\(l\\) -\u00e8me canal d'entr\u00e9e. \\(p,q,r\\) sont les dimensions du filtre. Les r\u00e9seaux de neurones r\u00e9currents (Recurrent Neural Networks) Un r\u00e9seau de neurones r\u00e9currents (RNN) est un type de r\u00e9seau de neurones qui est capable de traiter des s\u00e9quences de donn\u00e9es en utilisant la r\u00e9troaction (feedback) des sorties pr\u00e9c\u00e9dentes comme entr\u00e9e pour les calculs futurs. Contrairement aux r\u00e9seaux de neurones classiques, les RNN ont des connexions de neurones cycliques, ce qui leur permet de stocker une m\u00e9moire interne de l'information trait\u00e9e jusqu'\u00e0 pr\u00e9sent. Un RNN peut \u00eatre repr\u00e9sent\u00e9 comme une s\u00e9quence de cellules, o\u00f9 chaque cellule est une copie du m\u00eame r\u00e9seau de neurones. Chaque cellule prend en entr\u00e9e les donn\u00e9es de la s\u00e9quence et l'\u00e9tat cach\u00e9 de la cellule pr\u00e9c\u00e9dente. L'\u00e9tat cach\u00e9 est une repr\u00e9sentation vectorielle de la m\u00e9moire interne de la cellule, qui est mise \u00e0 jour \u00e0 chaque \u00e9tape de la s\u00e9quence. La sortie de chaque cellule est renvoy\u00e9e \u00e0 la cellule suivante et est \u00e9galement utilis\u00e9e pour la pr\u00e9diction finale. La pr\u00e9diction finale est donc bas\u00e9e sur l'ensemble des sorties de toutes les cellules de la s\u00e9quence. La sortie d'une cellule RNN est calcul\u00e9e comme suit : \\[ h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h) \\] o\u00f9 : \\(h_t\\) est l'\u00e9tat cach\u00e9 de la cellule \u00e0 l'\u00e9tape \\(t\\) . \\(x_t\\) est l'entr\u00e9e \u00e0 l'\u00e9tape \\(t\\) . \\(W_{xh}\\) est la matrice de poids pour les connexions entre les entr\u00e9es et l'\u00e9tat cach\u00e9. \\(W_{hh}\\) est la matrice de poids pour les connexions entre l'\u00e9tat cach\u00e9 de l'\u00e9tape pr\u00e9c\u00e9dente et l'\u00e9tat cach\u00e9 actuel. \\(b_h\\) est le biais ajout\u00e9 \u00e0 l'\u00e9tat cach\u00e9. \\(f\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. La sortie de la cellule peut \u00e9galement \u00eatre utilis\u00e9e pour pr\u00e9dire la prochaine valeur de la s\u00e9quence : \\[ y_t = g(W_{hy}h_t + b_y) \\] o\u00f9 : \\(y_t\\) est la pr\u00e9diction de la valeur de la s\u00e9quence \u00e0 l'\u00e9tape \\(t\\) . \\(W_{hy}\\) est la matrice de poids pour les connexions entre l'\u00e9tat cach\u00e9 et la pr\u00e9diction. \\(b_y\\) est le biais ajout\u00e9 \u00e0 la pr\u00e9diction. \\(g\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. Les RNN sont largement utilis\u00e9s pour la mod\u00e9lisation de s\u00e9quences, notamment pour la reconnaissance de la parole, la traduction automatique et la g\u00e9n\u00e9ration de texte. Les r\u00e9seaux de neurones de m\u00e9moire \u00e0 court terme (Long Short-Term Memory Networks) Ce sont des r\u00e9seaux de neurones r\u00e9currents am\u00e9lior\u00e9s qui permettent de traiter des s\u00e9quences de donn\u00e9es plus longues. Les r\u00e9seaux de neurones de m\u00e9moire \u00e0 court terme utilisent des unit\u00e9s de m\u00e9moire qui leur permettent de retenir l'information importante sur une longue p\u00e9riode de temps, et sont utilis\u00e9s pour la g\u00e9n\u00e9ration de texte, la pr\u00e9diction de la prochaine valeur dans une s\u00e9rie temporelle, etc. Les r\u00e9seaux de neurones auto-encodeurs (Autoencoder Neural Networks) Ce sont des r\u00e9seaux de neurones qui apprennent \u00e0 reconstruire une entr\u00e9e \u00e0 partir d'une repr\u00e9sentation latente, en comprimant l'information dans une couche cach\u00e9e de neurones. Les r\u00e9seaux de neurones auto-encodeurs sont utilis\u00e9s pour la compression de donn\u00e9es, la d\u00e9tection d'anomalies, etc. Les r\u00e9seaux de neurones g\u00e9n\u00e9ratifs (Generative Neural Networks) Ce sont des r\u00e9seaux de neurones qui apprennent \u00e0 g\u00e9n\u00e9rer de nouvelles donn\u00e9es \u00e0 partir d'un ensemble de donn\u00e9es d'entra\u00eenement. Les r\u00e9seaux de neurones g\u00e9n\u00e9ratifs sont utilis\u00e9s pour la g\u00e9n\u00e9ration de texte, d'images, de musique, etc.","title":"Les diff\u00e9rents types de r\u00e9seaux de neurones"},{"location":"dltypes/#les-reseaux-de-neurones-denses-fully-connected-layers","text":"Un r\u00e9seau de neurones dense, \u00e9galement appel\u00e9 r\u00e9seau de neurones \u00e0 couches enti\u00e8rement connect\u00e9es, est un type de r\u00e9seau de neurones o\u00f9 chaque neurone dans une couche est connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente et de la couche suivante. Cela signifie que chaque neurone de la couche actuelle prend en compte toutes les activations de la couche pr\u00e9c\u00e9dente et renvoie une activation qui est propag\u00e9e \u00e0 tous les neurones de la couche suivante. La sortie d'une couche dense est calcul\u00e9e comme suit : \\[y_j = f\\left(\\sum_{i=1}^n w_{ji}x_i + b_j\\right)\\] o\u00f9 : \\(y_j\\) est la sortie du \\(j\\) -\u00e8me neurone de la couche dense. \\(x_i\\) est l'activation du \\(i\\) -\u00e8me neurone de la couche pr\u00e9c\u00e9dente. \\(w_{ji}\\) est le poids de la connexion entre le \\(i\\) -\u00e8me neurone de la couche pr\u00e9c\u00e9dente et le \\(j\\) -\u00e8me neurone de la couche dense. \\(b_j\\) est le biais (un param\u00e8tre suppl\u00e9mentaire) du \\(j\\) -\u00e8me neurone de la couche dense. \\(f\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. La fonction d'activation \\(f\\) peut \u00eatre choisie en fonction du probl\u00e8me \u00e0 r\u00e9soudre.","title":"Les r\u00e9seaux de neurones denses (Fully Connected Layers)"},{"location":"dltypes/#les-reseaux-de-neurones-convolutifs-convolutional-neural-networks","text":"Les r\u00e9seaux de neurones convolutifs (Convolutional Neural Networks ou CNNs) sont un type de r\u00e9seau de neurones sp\u00e9cialement con\u00e7us pour le traitement des images. Ils ont \u00e9t\u00e9 introduits pour la premi\u00e8re fois par Yann LeCun et ses coll\u00e8gues en 1998 pour la reconnaissance de caract\u00e8res manuscrits. Les CNNs sont bas\u00e9s sur des filtres de convolution, qui sont des matrices de poids qui sont appliqu\u00e9es \u00e0 des r\u00e9gions de l'image. Les filtres de convolution permettent d'extraire des caract\u00e9ristiques visuelles \u00e0 diff\u00e9rentes \u00e9chelles. Par exemple, un filtre de convolution peut \u00eatre con\u00e7u pour d\u00e9tecter les bords dans une image, tandis qu'un autre filtre peut \u00eatre con\u00e7u pour d\u00e9tecter les coins. Un r\u00e9seau de neurones convolutif se compose de plusieurs couches. La premi\u00e8re couche est la couche d'entr\u00e9e, qui contient l'image brute. Les couches suivantes sont des couches de convolution, qui appliquent des filtres de convolution \u00e0 l'image. Chaque filtre de convolution produit une carte de caract\u00e9ristiques, qui est une image qui met en \u00e9vidence les caract\u00e9ristiques sp\u00e9cifiques d\u00e9tect\u00e9es par le filtre. Les cartes de caract\u00e9ristiques sont ensuite pass\u00e9es \u00e0 une fonction d'activation, qui introduit une non-lin\u00e9arit\u00e9 dans le mod\u00e8le. Apr\u00e8s les couches de convolution, il y a g\u00e9n\u00e9ralement des couches de regroupement (pooling layers), qui r\u00e9duisent la dimensionnalit\u00e9 de l'image en prenant le maximum, la moyenne ou la somme de petites r\u00e9gions de l'image. Les couches de regroupement permettent de r\u00e9duire le temps de calcul et de rendre le mod\u00e8le plus robuste aux variations mineures dans l'image. Apr\u00e8s les couches de regroupement, il y a g\u00e9n\u00e9ralement des couches enti\u00e8rement connect\u00e9es (Fully Connected Layers), qui sont similaires aux couches de neurones denses des r\u00e9seaux de neurones classiques. Les couches enti\u00e8rement connect\u00e9es prennent les cartes de caract\u00e9ristiques en entr\u00e9e et les transforment en une repr\u00e9sentation de sortie qui est utilis\u00e9e pour la classification ou la r\u00e9gression. Les CNNs sont entra\u00een\u00e9s \u00e0 l'aide d'un algorithme d'optimisation, tel que la descente de gradient stochastique (Stochastic Gradient Descent), pour minimiser une fonction de perte (loss function) qui mesure la diff\u00e9rence entre la sortie pr\u00e9dite et la sortie r\u00e9elle. Pendant l'entra\u00eenement, les poids des filtres de convolution et des couches enti\u00e8rement connect\u00e9es sont ajust\u00e9s pour minimiser la fonction de perte. L'\u00e9quation pour la sortie d'une couche de convolution: \\[y(i,j,k) = \\text{activation}\\left(\\sum_{l=1}^{p}\\sum_{m=1}^{q}\\sum_{n=1}^{r} w(m,n,l,k) x(i+m-1,j+n-1,l)\\right)\\] o\u00f9 : \\(y(i,j,k)\\) est la sortie du \\(k\\) -\u00e8me filtre \u00e0 la position \\((i,j)\\) dans la carte de caract\u00e9ristiques de sortie. \\(\\text{activation}\\) est la fonction d'activation appliqu\u00e9e \u00e9l\u00e9ment par \u00e9l\u00e9ment \u00e0 la somme des entr\u00e9es pond\u00e9r\u00e9es. \\(w(m,n,l,k)\\) est le poids du \\(k\\) -\u00e8me filtre \u00e0 la position \\((m,n)\\) dans le \\(l\\) -\u00e8me canal d'entr\u00e9e. \\(x(i+m-1,j+n-1,l)\\) est l'activation d'entr\u00e9e \u00e0 la position \\((i+m-1,j+n-1)\\) dans le \\(l\\) -\u00e8me canal d'entr\u00e9e. \\(p,q,r\\) sont les dimensions du filtre.","title":"Les r\u00e9seaux de neurones convolutifs (Convolutional Neural Networks)"},{"location":"dltypes/#les-reseaux-de-neurones-recurrents-recurrent-neural-networks","text":"Un r\u00e9seau de neurones r\u00e9currents (RNN) est un type de r\u00e9seau de neurones qui est capable de traiter des s\u00e9quences de donn\u00e9es en utilisant la r\u00e9troaction (feedback) des sorties pr\u00e9c\u00e9dentes comme entr\u00e9e pour les calculs futurs. Contrairement aux r\u00e9seaux de neurones classiques, les RNN ont des connexions de neurones cycliques, ce qui leur permet de stocker une m\u00e9moire interne de l'information trait\u00e9e jusqu'\u00e0 pr\u00e9sent. Un RNN peut \u00eatre repr\u00e9sent\u00e9 comme une s\u00e9quence de cellules, o\u00f9 chaque cellule est une copie du m\u00eame r\u00e9seau de neurones. Chaque cellule prend en entr\u00e9e les donn\u00e9es de la s\u00e9quence et l'\u00e9tat cach\u00e9 de la cellule pr\u00e9c\u00e9dente. L'\u00e9tat cach\u00e9 est une repr\u00e9sentation vectorielle de la m\u00e9moire interne de la cellule, qui est mise \u00e0 jour \u00e0 chaque \u00e9tape de la s\u00e9quence. La sortie de chaque cellule est renvoy\u00e9e \u00e0 la cellule suivante et est \u00e9galement utilis\u00e9e pour la pr\u00e9diction finale. La pr\u00e9diction finale est donc bas\u00e9e sur l'ensemble des sorties de toutes les cellules de la s\u00e9quence. La sortie d'une cellule RNN est calcul\u00e9e comme suit : \\[ h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h) \\] o\u00f9 : \\(h_t\\) est l'\u00e9tat cach\u00e9 de la cellule \u00e0 l'\u00e9tape \\(t\\) . \\(x_t\\) est l'entr\u00e9e \u00e0 l'\u00e9tape \\(t\\) . \\(W_{xh}\\) est la matrice de poids pour les connexions entre les entr\u00e9es et l'\u00e9tat cach\u00e9. \\(W_{hh}\\) est la matrice de poids pour les connexions entre l'\u00e9tat cach\u00e9 de l'\u00e9tape pr\u00e9c\u00e9dente et l'\u00e9tat cach\u00e9 actuel. \\(b_h\\) est le biais ajout\u00e9 \u00e0 l'\u00e9tat cach\u00e9. \\(f\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. La sortie de la cellule peut \u00e9galement \u00eatre utilis\u00e9e pour pr\u00e9dire la prochaine valeur de la s\u00e9quence : \\[ y_t = g(W_{hy}h_t + b_y) \\] o\u00f9 : \\(y_t\\) est la pr\u00e9diction de la valeur de la s\u00e9quence \u00e0 l'\u00e9tape \\(t\\) . \\(W_{hy}\\) est la matrice de poids pour les connexions entre l'\u00e9tat cach\u00e9 et la pr\u00e9diction. \\(b_y\\) est le biais ajout\u00e9 \u00e0 la pr\u00e9diction. \\(g\\) est la fonction d'activation appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es. Les RNN sont largement utilis\u00e9s pour la mod\u00e9lisation de s\u00e9quences, notamment pour la reconnaissance de la parole, la traduction automatique et la g\u00e9n\u00e9ration de texte.","title":"Les r\u00e9seaux de neurones r\u00e9currents (Recurrent Neural Networks)"},{"location":"dltypes/#les-reseaux-de-neurones-de-memoire-a-court-terme-long-short-term-memory-networks","text":"Ce sont des r\u00e9seaux de neurones r\u00e9currents am\u00e9lior\u00e9s qui permettent de traiter des s\u00e9quences de donn\u00e9es plus longues. Les r\u00e9seaux de neurones de m\u00e9moire \u00e0 court terme utilisent des unit\u00e9s de m\u00e9moire qui leur permettent de retenir l'information importante sur une longue p\u00e9riode de temps, et sont utilis\u00e9s pour la g\u00e9n\u00e9ration de texte, la pr\u00e9diction de la prochaine valeur dans une s\u00e9rie temporelle, etc.","title":"Les r\u00e9seaux de neurones de m\u00e9moire \u00e0 court terme (Long Short-Term Memory Networks)"},{"location":"dltypes/#les-reseaux-de-neurones-auto-encodeurs-autoencoder-neural-networks","text":"Ce sont des r\u00e9seaux de neurones qui apprennent \u00e0 reconstruire une entr\u00e9e \u00e0 partir d'une repr\u00e9sentation latente, en comprimant l'information dans une couche cach\u00e9e de neurones. Les r\u00e9seaux de neurones auto-encodeurs sont utilis\u00e9s pour la compression de donn\u00e9es, la d\u00e9tection d'anomalies, etc.","title":"Les r\u00e9seaux de neurones auto-encodeurs (Autoencoder Neural Networks)"},{"location":"dltypes/#les-reseaux-de-neurones-generatifs-generative-neural-networks","text":"Ce sont des r\u00e9seaux de neurones qui apprennent \u00e0 g\u00e9n\u00e9rer de nouvelles donn\u00e9es \u00e0 partir d'un ensemble de donn\u00e9es d'entra\u00eenement. Les r\u00e9seaux de neurones g\u00e9n\u00e9ratifs sont utilis\u00e9s pour la g\u00e9n\u00e9ration de texte, d'images, de musique, etc.","title":"Les r\u00e9seaux de neurones g\u00e9n\u00e9ratifs (Generative Neural Networks)"},{"location":"eval/","text":"Comment evaluer mon mod\u00e9le \u00c9valuation d'un mod\u00e8le de r\u00e9gression Les coefficients de r\u00e9gression L'\u00e9quation d'une r\u00e9gression lin\u00e9aire simple peut \u00eatre \u00e9crite comme : y = \u03b20 + \u03b21x + \u03b5 o\u00f9 : y est la variable d\u00e9pendante, x est la variable ind\u00e9pendante, \u03b20 est l'interception ou constante, \u03b21 est la pente ou coefficient de r\u00e9gression, \u03b5 est l'erreur r\u00e9siduelle. L'\u00e9quation indique que la valeur de y d\u00e9pend de la valeur de x multipli\u00e9e par le coefficient de r\u00e9gression \u03b21 et ajout\u00e9e \u00e0 l'interception \u03b20, avec une certaine erreur r\u00e9siduelle \u03b5. La r\u00e9gression lin\u00e9aire multiple peut \u00eatre \u00e9tendue pour inclure plusieurs variables ind\u00e9pendantes, ce qui donnerait une \u00e9quation de la forme : y = \u03b20 + \u03b21x1 + \u03b22x2 + ... + \u03b2pxp + \u03b5 o\u00f9 p est le nombre de variables ind\u00e9pendantes. Les coefficients de r\u00e9gression sont utilis\u00e9s pour mesurer la relation entre les variables ind\u00e9pendantes et la variable d\u00e9pendante dans un mod\u00e8le de r\u00e9gression. Un coefficient positif indique une relation positive entre les variables, tandis qu'un coefficient n\u00e9gatif indique une relation inverse. En r\u00e9gression lin\u00e9aire simple, le coefficient de pente mesure la variation de la variable d\u00e9pendante pour une unit\u00e9 de variation de la variable ind\u00e9pendante, tandis que le coefficient d'interception indique la valeur de la variable d\u00e9pendante lorsque la variable ind\u00e9pendante est \u00e9gale \u00e0 z\u00e9ro. Cependant, l'interpr\u00e9tation des coefficients d\u00e9pend du type de mod\u00e8le et de la nature des donn\u00e9es. Mean Absolute Error (MAE) La MAE mesure l'erreur absolue moyenne entre les pr\u00e9visions et les observations r\u00e9elles. Elle est calcul\u00e9e en faisant la somme des valeurs absolues des diff\u00e9rences entre chaque pr\u00e9diction et la valeur r\u00e9elle, puis en divisant cette somme par le nombre total de pr\u00e9dictions. Elle est souvent utilis\u00e9e pour des mod\u00e8les de r\u00e9gression simples, Plus le MAE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation de la MAE est la suivante : MAE = (1/n) * \u03a3|i=1 \u00e0 n|(|y_i - \u0177_i|) O\u00f9 : n : le nombre d'observations dans l'ensemble de donn\u00e9es de test y_i : la valeur r\u00e9elle de la variable cible pour la i-\u00e8me observation \u0177_i : la valeur pr\u00e9dite de la variable cible pour la i-\u00e8me observation Exemple de code Python pour calculer la MAE : from sklearn.metrics import mean_absolute_error y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mae = mean_absolute_error(y_true, y_pred) print(\"MAE: \", mae) #R\u00e9sultat : MAE: 0.5 Mean Squared Error (MSE) La MSE mesure la moyenne des carr\u00e9s des erreurs entre les pr\u00e9visions et les observations r\u00e9elles. Elle est calcul\u00e9e en faisant la somme des carr\u00e9s des diff\u00e9rences entre chaque pr\u00e9diction et la valeur r\u00e9elle, puis en divisant cette somme par le nombre total de pr\u00e9dictions. Elle est souvent utilis\u00e9e pour des mod\u00e8les de r\u00e9gression plus complexes. Il est plus sensible aux grandes erreurs que le MAE. Plus le MSE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation de la MSE est la suivante : MSE = (1/n) * \u03a3|i=1 \u00e0 n|(y_i - \u0177_i)^2 O\u00f9 : n : le nombre d'observations dans l'ensemble de donn\u00e9es de test y_i : la valeur r\u00e9elle de la variable cible pour la i-\u00e8me observation \u0177_i : la valeur pr\u00e9dite de la variable cible pour la i-\u00e8me observation Exemple de code Python pour calculer la MSE : from sklearn.metrics import mean_squared_error y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mse = mean_squared_error(y_true, y_pred) print(\"MSE: \", mse) #R\u00e9sultat : MSE: 0.375 Root Mean Squared Error (RMSE) Le RMSE est la racine carr\u00e9e du MSE et repr\u00e9sente l'erreur moyenne entre les pr\u00e9dictions du mod\u00e8le et les valeurs r\u00e9elles de la variable cible. Il est souvent utilis\u00e9 pour des mod\u00e8les de r\u00e9gression plus complexes. Plus le RMSE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation du RMSE est la suivante : RMSE = sqrt(MSE) Exemple de code Python pour calculer le RMSE : from sklearn.metrics import mean_squared_error import numpy as np y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) print(\"RMSE: \", rmse) # R\u00e9sultat : RMSE: 0.6123724356957945 Coefficient de d\u00e9termination R\u00b2 Le coefficient de d\u00e9termination R\u00b2 mesure la proportion de variance expliqu\u00e9e par le mod\u00e8le. Il varie de 0 \u00e0 1, o\u00f9 0 signifie que le mod\u00e8le ne parvient pas \u00e0 expliquer la variance de la variable cible et 1 signifie qu'il explique toute la variance. Plus le R\u00b2 est \u00e9lev\u00e9, meilleure est la performance du mod\u00e8le. Equation : R\u00b2 = 1 - SSE / SST o\u00f9 SSE est la somme des carr\u00e9s des r\u00e9sidus (c'est-\u00e0-dire la somme des carr\u00e9s des diff\u00e9rences entre les valeurs pr\u00e9dites et les valeurs r\u00e9elles), et SST est la somme des carr\u00e9s des diff\u00e9rences entre les valeurs r\u00e9elles et la moyenne des valeurs r\u00e9elles. Somme des carr\u00e9s des erreurs (SSE) : SSE = \u03a3(yi - \u0177i)\u00b2 o\u00f9 yi est la valeur r\u00e9elle de la variable d\u00e9pendante, et \u0177i est la valeur pr\u00e9dite par le mod\u00e8le. Somme totale des carr\u00e9s (SST) : SST = \u03a3(yi - \u0233)\u00b2 o\u00f9 yi est la valeur r\u00e9elle de la variable d\u00e9pendante, et \u0233 est la moyenne des valeurs r\u00e9elles. from sklearn.metrics import r2_score y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] # Calcul du coefficient de d\u00e9termination R\u00b2 r2 = r2_score(y_train, y_pred) print(\"Coefficient de d\u00e9termination R\u00b2 :\", r2) Interpr\u00e9tation des r\u00e9sultats MAE : Un MAE de 0 signifie que le mod\u00e8le pr\u00e9dit parfaitement les valeurs de la variable cible. En g\u00e9n\u00e9ral, un MAE faible indique une bonne performance, mais cela d\u00e9pend du contexte et de la plage de valeurs de la variable cible. MSE et RMSE : Comme mentionn\u00e9 pr\u00e9c\u00e9demment, ces deux m\u00e9triques sont plus sensibles aux grandes erreurs que le MAE. Par cons\u00e9quent, un MSE ou un RMSE \u00e9lev\u00e9 peut indiquer qu'il y a des valeurs aberrantes dans les donn\u00e9es ou que le mod\u00e8le ne parvient pas \u00e0 pr\u00e9dire correctement les valeurs extr\u00eames de la variable cible. R\u00b2 : indique la proportion de la variance totale des donn\u00e9es qui est expliqu\u00e9e par le mod\u00e8le. Un R\u00b2 de 1 signifie que le mod\u00e8le explique parfaitement la variance des donn\u00e9es, tandis qu'un R\u00b2 de 0 signifie que le mod\u00e8le n'explique aucune variance des donn\u00e9es. Dans la pratique, un R\u00b2 \u00e9lev\u00e9 (g\u00e9n\u00e9ralement sup\u00e9rieur \u00e0 0,7 ou 0,8) est consid\u00e9r\u00e9 comme un bon ajustement du mod\u00e8le aux donn\u00e9es. SSE indique la quantit\u00e9 d'erreur r\u00e9siduelle dans le mod\u00e8le. En g\u00e9n\u00e9ral, plus SSE est faible, mieux c'est, car cela signifie que les pr\u00e9dictions du mod\u00e8le sont plus proches des valeurs r\u00e9elles. SST indique la quantit\u00e9 de variation totale dans les donn\u00e9es. Il est souvent utilis\u00e9 comme mesure de la variabilit\u00e9 des donn\u00e9es de r\u00e9f\u00e9rence. Un mod\u00e8le qui explique une grande partie de la variation totale de SST est consid\u00e9r\u00e9 comme un bon ajustement aux donn\u00e9es. \u00c9valuation d'un mod\u00e8le de classification Accuracy L'accuracy est la m\u00e9trique la plus simple pour \u00e9valuer les mod\u00e8les de classification. Elle mesure la proportion de pr\u00e9dictions correctes sur l'ensemble de donn\u00e9es de test. L'\u00e9quation de l'accuracy est la suivante : Accuracy = (TP + TN) / (TP + TN + FP + FN) O\u00f9 : TP : True Positives, le nombre de pr\u00e9dictions positives correctes TN : True Negatives, le nombre de pr\u00e9dictions n\u00e9gatives correctes FP : False Positives, le nombre de pr\u00e9dictions positives incorrectes FN : False Negatives, le nombre de pr\u00e9dictions n\u00e9gatives incorrectes Exemple de code Python pour calculer l'accuracy : from sklearn.metrics import accuracy_score y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] accuracy = accuracy_score(y_true, y_pred) print(\"Accuracy: \", accuracy) #R\u00e9sultat : Accuracy: 0.3333333333333333 Precision, Recall et F1-score La pr\u00e9cision (precision), le rappel (recall) et le score F1 (F1-score) sont des m\u00e9triques plus avanc\u00e9es pour \u00e9valuer les mod\u00e8les de classification. Elles sont particuli\u00e8rement utiles pour les probl\u00e8mes de classification binaire, o\u00f9 l'on souhaite pr\u00e9dire la pr\u00e9sence ou l'absence d'une classe. La pr\u00e9cision mesure la proportion de pr\u00e9dictions positives qui sont correctes. Elle est calcul\u00e9e en divisant le nombre de vrais positifs par le nombre de vrais positifs plus le nombre de faux positifs.L'\u00e9quation de la pr\u00e9cision est la suivante : Precision = TP / (TP + FP) Le rappel mesure la proportion de vrais positifs qui sont correctement identifi\u00e9s. Il est calcul\u00e9 en divisant le nombre de vrais positifs par le nombre de vrais positifs plus le nombre de faux n\u00e9gatifs. L'\u00e9quation du rappel est la suivante : Recall = TP / (TP + FN) - Le score F1 est une moyenne harmonique de la pr\u00e9cision et du rappel. Il est calcul\u00e9 en prenant la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et du rappel, o\u00f9 la pond\u00e9ration est donn\u00e9e par l'harmonique de la pr\u00e9cision et du rappel. L'\u00e9quation du score F1 est la suivante : F1 = 2 * (Precision * Recall) / (Precision + Recall) Exemple de code Python pour calculer la pr\u00e9cision, le rappel et le score F1 : from sklearn.metrics import precision_score, recall_score, f1_score y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] precision = precision_score(y_true, y_pred, average='macro') recall = recall_score(y_true, y_pred, average='macro') f1 = f1_score(y_true, y_pred, average='macro') print(\"Precision: \", precision) print(\"Recall: \", recall) print(\"F1-score: \", f1) Matrice de confusion La matrice de confusion est un outil visuel pour \u00e9valuer les performances d'un mod\u00e8le de classification. Elle montre le nombre de pr\u00e9dictions correctes et incorrectes pour chaque classe. Exemple de code Python pour afficher la matrice de confusion : from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt import seaborn as sns y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] cm = confusion_matrix(y_true, y_pred) sns.heatmap(cm, annot=True, cmap=\"Blues\") plt.xlabel('Pr\u00e9dictions') plt.ylabel('Valeurs r\u00e9elles') plt.show() Interpr\u00e9ter les m\u00e9triques d'\u00e9valuation de classification: Tout d\u00e9pend du contexte sp\u00e9cifique de chaque projet de machine learning. Cependant, voici quelques points g\u00e9n\u00e9raux \u00e0 prendre en compte lors de l'interpr\u00e9tation de ces m\u00e9triques : Pr\u00e9cision : La pr\u00e9cision mesure le taux de pr\u00e9dictions positives correctes parmi toutes les pr\u00e9dictions positives. Par exemple, une pr\u00e9cision de 0,8 signifie que le mod\u00e8le a correctement pr\u00e9dit 80% des r\u00e9sultats positifs. Cependant, si la pr\u00e9cision est \u00e9lev\u00e9e mais que le rappel est faible, cela peut indiquer que le mod\u00e8le pr\u00e9dit trop peu de r\u00e9sultats positifs, manquant ainsi des r\u00e9sultats positifs r\u00e9els. Rappel : Le rappel mesure le taux de pr\u00e9dictions positives correctes parmi tous les r\u00e9sultats positifs r\u00e9els. Par exemple, un rappel de 0,7 signifie que le mod\u00e8le a correctement pr\u00e9dit 70% des r\u00e9sultats positifs. Cependant, si le rappel est \u00e9lev\u00e9 mais que la pr\u00e9cision est faible, cela peut indiquer que le mod\u00e8le pr\u00e9dit trop de r\u00e9sultats positifs, y compris des r\u00e9sultats faux positifs. Score F1 : Le score F1 est une moyenne harmonique de la pr\u00e9cision et du rappel. Il est g\u00e9n\u00e9ralement utilis\u00e9 lorsque l'on souhaite trouver un \u00e9quilibre entre la pr\u00e9cision et le rappel. Matrice de confusion : La matrice de confusion montre le nombre de pr\u00e9dictions correctes et incorrectes pour chaque classe. Elle permet de voir o\u00f9 le mod\u00e8le est en train de se tromper. Par exemple, si le mod\u00e8le pr\u00e9dit souvent la classe 0 alors qu'elle est r\u00e9ellement la classe 1, il peut y avoir une confusion entre ces deux classes. En g\u00e9n\u00e9ral, il est important d'\u00e9valuer le mod\u00e8le avec plusieurs m\u00e9triques pour avoir une vue d'ensemble de ses performances. Selon le contexte, certaines m\u00e9triques peuvent \u00eatre plus importantes que d'autres. Par exemple, dans les probl\u00e8mes de d\u00e9tection de fraude, un rappel \u00e9lev\u00e9 peut \u00eatre plus important que la pr\u00e9cision, car il est crucial de ne pas manquer de cas de fraude, m\u00eame si cela signifie avoir plus de faux positifs.","title":"Evaluation des mod\u00e9les de Classification et de Regression"},{"location":"eval/#comment-evaluer-mon-modele","text":"","title":"Comment evaluer mon mod\u00e9le"},{"location":"eval/#evaluation-dun-modele-de-regression","text":"","title":"\u00c9valuation d'un mod\u00e8le de r\u00e9gression"},{"location":"eval/#les-coefficients-de-regression","text":"L'\u00e9quation d'une r\u00e9gression lin\u00e9aire simple peut \u00eatre \u00e9crite comme : y = \u03b20 + \u03b21x + \u03b5 o\u00f9 : y est la variable d\u00e9pendante, x est la variable ind\u00e9pendante, \u03b20 est l'interception ou constante, \u03b21 est la pente ou coefficient de r\u00e9gression, \u03b5 est l'erreur r\u00e9siduelle. L'\u00e9quation indique que la valeur de y d\u00e9pend de la valeur de x multipli\u00e9e par le coefficient de r\u00e9gression \u03b21 et ajout\u00e9e \u00e0 l'interception \u03b20, avec une certaine erreur r\u00e9siduelle \u03b5. La r\u00e9gression lin\u00e9aire multiple peut \u00eatre \u00e9tendue pour inclure plusieurs variables ind\u00e9pendantes, ce qui donnerait une \u00e9quation de la forme : y = \u03b20 + \u03b21x1 + \u03b22x2 + ... + \u03b2pxp + \u03b5 o\u00f9 p est le nombre de variables ind\u00e9pendantes. Les coefficients de r\u00e9gression sont utilis\u00e9s pour mesurer la relation entre les variables ind\u00e9pendantes et la variable d\u00e9pendante dans un mod\u00e8le de r\u00e9gression. Un coefficient positif indique une relation positive entre les variables, tandis qu'un coefficient n\u00e9gatif indique une relation inverse. En r\u00e9gression lin\u00e9aire simple, le coefficient de pente mesure la variation de la variable d\u00e9pendante pour une unit\u00e9 de variation de la variable ind\u00e9pendante, tandis que le coefficient d'interception indique la valeur de la variable d\u00e9pendante lorsque la variable ind\u00e9pendante est \u00e9gale \u00e0 z\u00e9ro. Cependant, l'interpr\u00e9tation des coefficients d\u00e9pend du type de mod\u00e8le et de la nature des donn\u00e9es.","title":"Les coefficients de r\u00e9gression"},{"location":"eval/#mean-absolute-error-mae","text":"La MAE mesure l'erreur absolue moyenne entre les pr\u00e9visions et les observations r\u00e9elles. Elle est calcul\u00e9e en faisant la somme des valeurs absolues des diff\u00e9rences entre chaque pr\u00e9diction et la valeur r\u00e9elle, puis en divisant cette somme par le nombre total de pr\u00e9dictions. Elle est souvent utilis\u00e9e pour des mod\u00e8les de r\u00e9gression simples, Plus le MAE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation de la MAE est la suivante : MAE = (1/n) * \u03a3|i=1 \u00e0 n|(|y_i - \u0177_i|) O\u00f9 : n : le nombre d'observations dans l'ensemble de donn\u00e9es de test y_i : la valeur r\u00e9elle de la variable cible pour la i-\u00e8me observation \u0177_i : la valeur pr\u00e9dite de la variable cible pour la i-\u00e8me observation Exemple de code Python pour calculer la MAE : from sklearn.metrics import mean_absolute_error y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mae = mean_absolute_error(y_true, y_pred) print(\"MAE: \", mae) #R\u00e9sultat : MAE: 0.5","title":"Mean Absolute Error (MAE)"},{"location":"eval/#mean-squared-error-mse","text":"La MSE mesure la moyenne des carr\u00e9s des erreurs entre les pr\u00e9visions et les observations r\u00e9elles. Elle est calcul\u00e9e en faisant la somme des carr\u00e9s des diff\u00e9rences entre chaque pr\u00e9diction et la valeur r\u00e9elle, puis en divisant cette somme par le nombre total de pr\u00e9dictions. Elle est souvent utilis\u00e9e pour des mod\u00e8les de r\u00e9gression plus complexes. Il est plus sensible aux grandes erreurs que le MAE. Plus le MSE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation de la MSE est la suivante : MSE = (1/n) * \u03a3|i=1 \u00e0 n|(y_i - \u0177_i)^2 O\u00f9 : n : le nombre d'observations dans l'ensemble de donn\u00e9es de test y_i : la valeur r\u00e9elle de la variable cible pour la i-\u00e8me observation \u0177_i : la valeur pr\u00e9dite de la variable cible pour la i-\u00e8me observation Exemple de code Python pour calculer la MSE : from sklearn.metrics import mean_squared_error y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mse = mean_squared_error(y_true, y_pred) print(\"MSE: \", mse) #R\u00e9sultat : MSE: 0.375","title":"Mean Squared Error (MSE)"},{"location":"eval/#root-mean-squared-error-rmse","text":"Le RMSE est la racine carr\u00e9e du MSE et repr\u00e9sente l'erreur moyenne entre les pr\u00e9dictions du mod\u00e8le et les valeurs r\u00e9elles de la variable cible. Il est souvent utilis\u00e9 pour des mod\u00e8les de r\u00e9gression plus complexes. Plus le RMSE est faible, meilleure est la performance du mod\u00e8le. L'\u00e9quation du RMSE est la suivante : RMSE = sqrt(MSE) Exemple de code Python pour calculer le RMSE : from sklearn.metrics import mean_squared_error import numpy as np y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) print(\"RMSE: \", rmse) # R\u00e9sultat : RMSE: 0.6123724356957945","title":"Root Mean Squared Error (RMSE)"},{"location":"eval/#coefficient-de-determination-r2","text":"Le coefficient de d\u00e9termination R\u00b2 mesure la proportion de variance expliqu\u00e9e par le mod\u00e8le. Il varie de 0 \u00e0 1, o\u00f9 0 signifie que le mod\u00e8le ne parvient pas \u00e0 expliquer la variance de la variable cible et 1 signifie qu'il explique toute la variance. Plus le R\u00b2 est \u00e9lev\u00e9, meilleure est la performance du mod\u00e8le. Equation : R\u00b2 = 1 - SSE / SST o\u00f9 SSE est la somme des carr\u00e9s des r\u00e9sidus (c'est-\u00e0-dire la somme des carr\u00e9s des diff\u00e9rences entre les valeurs pr\u00e9dites et les valeurs r\u00e9elles), et SST est la somme des carr\u00e9s des diff\u00e9rences entre les valeurs r\u00e9elles et la moyenne des valeurs r\u00e9elles. Somme des carr\u00e9s des erreurs (SSE) : SSE = \u03a3(yi - \u0177i)\u00b2 o\u00f9 yi est la valeur r\u00e9elle de la variable d\u00e9pendante, et \u0177i est la valeur pr\u00e9dite par le mod\u00e8le. Somme totale des carr\u00e9s (SST) : SST = \u03a3(yi - \u0233)\u00b2 o\u00f9 yi est la valeur r\u00e9elle de la variable d\u00e9pendante, et \u0233 est la moyenne des valeurs r\u00e9elles. from sklearn.metrics import r2_score y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] # Calcul du coefficient de d\u00e9termination R\u00b2 r2 = r2_score(y_train, y_pred) print(\"Coefficient de d\u00e9termination R\u00b2 :\", r2)","title":"Coefficient de d\u00e9termination R\u00b2"},{"location":"eval/#interpretation-des-resultats","text":"MAE : Un MAE de 0 signifie que le mod\u00e8le pr\u00e9dit parfaitement les valeurs de la variable cible. En g\u00e9n\u00e9ral, un MAE faible indique une bonne performance, mais cela d\u00e9pend du contexte et de la plage de valeurs de la variable cible. MSE et RMSE : Comme mentionn\u00e9 pr\u00e9c\u00e9demment, ces deux m\u00e9triques sont plus sensibles aux grandes erreurs que le MAE. Par cons\u00e9quent, un MSE ou un RMSE \u00e9lev\u00e9 peut indiquer qu'il y a des valeurs aberrantes dans les donn\u00e9es ou que le mod\u00e8le ne parvient pas \u00e0 pr\u00e9dire correctement les valeurs extr\u00eames de la variable cible. R\u00b2 : indique la proportion de la variance totale des donn\u00e9es qui est expliqu\u00e9e par le mod\u00e8le. Un R\u00b2 de 1 signifie que le mod\u00e8le explique parfaitement la variance des donn\u00e9es, tandis qu'un R\u00b2 de 0 signifie que le mod\u00e8le n'explique aucune variance des donn\u00e9es. Dans la pratique, un R\u00b2 \u00e9lev\u00e9 (g\u00e9n\u00e9ralement sup\u00e9rieur \u00e0 0,7 ou 0,8) est consid\u00e9r\u00e9 comme un bon ajustement du mod\u00e8le aux donn\u00e9es. SSE indique la quantit\u00e9 d'erreur r\u00e9siduelle dans le mod\u00e8le. En g\u00e9n\u00e9ral, plus SSE est faible, mieux c'est, car cela signifie que les pr\u00e9dictions du mod\u00e8le sont plus proches des valeurs r\u00e9elles. SST indique la quantit\u00e9 de variation totale dans les donn\u00e9es. Il est souvent utilis\u00e9 comme mesure de la variabilit\u00e9 des donn\u00e9es de r\u00e9f\u00e9rence. Un mod\u00e8le qui explique une grande partie de la variation totale de SST est consid\u00e9r\u00e9 comme un bon ajustement aux donn\u00e9es.","title":"Interpr\u00e9tation des r\u00e9sultats"},{"location":"eval/#evaluation-dun-modele-de-classification","text":"","title":"\u00c9valuation d'un mod\u00e8le de classification"},{"location":"eval/#accuracy","text":"L'accuracy est la m\u00e9trique la plus simple pour \u00e9valuer les mod\u00e8les de classification. Elle mesure la proportion de pr\u00e9dictions correctes sur l'ensemble de donn\u00e9es de test. L'\u00e9quation de l'accuracy est la suivante : Accuracy = (TP + TN) / (TP + TN + FP + FN) O\u00f9 : TP : True Positives, le nombre de pr\u00e9dictions positives correctes TN : True Negatives, le nombre de pr\u00e9dictions n\u00e9gatives correctes FP : False Positives, le nombre de pr\u00e9dictions positives incorrectes FN : False Negatives, le nombre de pr\u00e9dictions n\u00e9gatives incorrectes Exemple de code Python pour calculer l'accuracy : from sklearn.metrics import accuracy_score y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] accuracy = accuracy_score(y_true, y_pred) print(\"Accuracy: \", accuracy) #R\u00e9sultat : Accuracy: 0.3333333333333333","title":"Accuracy"},{"location":"eval/#precision-recall-et-f1-score","text":"La pr\u00e9cision (precision), le rappel (recall) et le score F1 (F1-score) sont des m\u00e9triques plus avanc\u00e9es pour \u00e9valuer les mod\u00e8les de classification. Elles sont particuli\u00e8rement utiles pour les probl\u00e8mes de classification binaire, o\u00f9 l'on souhaite pr\u00e9dire la pr\u00e9sence ou l'absence d'une classe. La pr\u00e9cision mesure la proportion de pr\u00e9dictions positives qui sont correctes. Elle est calcul\u00e9e en divisant le nombre de vrais positifs par le nombre de vrais positifs plus le nombre de faux positifs.L'\u00e9quation de la pr\u00e9cision est la suivante : Precision = TP / (TP + FP) Le rappel mesure la proportion de vrais positifs qui sont correctement identifi\u00e9s. Il est calcul\u00e9 en divisant le nombre de vrais positifs par le nombre de vrais positifs plus le nombre de faux n\u00e9gatifs. L'\u00e9quation du rappel est la suivante : Recall = TP / (TP + FN) - Le score F1 est une moyenne harmonique de la pr\u00e9cision et du rappel. Il est calcul\u00e9 en prenant la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et du rappel, o\u00f9 la pond\u00e9ration est donn\u00e9e par l'harmonique de la pr\u00e9cision et du rappel. L'\u00e9quation du score F1 est la suivante : F1 = 2 * (Precision * Recall) / (Precision + Recall) Exemple de code Python pour calculer la pr\u00e9cision, le rappel et le score F1 : from sklearn.metrics import precision_score, recall_score, f1_score y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] precision = precision_score(y_true, y_pred, average='macro') recall = recall_score(y_true, y_pred, average='macro') f1 = f1_score(y_true, y_pred, average='macro') print(\"Precision: \", precision) print(\"Recall: \", recall) print(\"F1-score: \", f1)","title":"Precision, Recall et F1-score"},{"location":"eval/#matrice-de-confusion","text":"La matrice de confusion est un outil visuel pour \u00e9valuer les performances d'un mod\u00e8le de classification. Elle montre le nombre de pr\u00e9dictions correctes et incorrectes pour chaque classe. Exemple de code Python pour afficher la matrice de confusion : from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt import seaborn as sns y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] cm = confusion_matrix(y_true, y_pred) sns.heatmap(cm, annot=True, cmap=\"Blues\") plt.xlabel('Pr\u00e9dictions') plt.ylabel('Valeurs r\u00e9elles') plt.show()","title":"Matrice de confusion"},{"location":"eval/#interpreter-les-metriques-devaluation-de-classification","text":"Tout d\u00e9pend du contexte sp\u00e9cifique de chaque projet de machine learning. Cependant, voici quelques points g\u00e9n\u00e9raux \u00e0 prendre en compte lors de l'interpr\u00e9tation de ces m\u00e9triques : Pr\u00e9cision : La pr\u00e9cision mesure le taux de pr\u00e9dictions positives correctes parmi toutes les pr\u00e9dictions positives. Par exemple, une pr\u00e9cision de 0,8 signifie que le mod\u00e8le a correctement pr\u00e9dit 80% des r\u00e9sultats positifs. Cependant, si la pr\u00e9cision est \u00e9lev\u00e9e mais que le rappel est faible, cela peut indiquer que le mod\u00e8le pr\u00e9dit trop peu de r\u00e9sultats positifs, manquant ainsi des r\u00e9sultats positifs r\u00e9els. Rappel : Le rappel mesure le taux de pr\u00e9dictions positives correctes parmi tous les r\u00e9sultats positifs r\u00e9els. Par exemple, un rappel de 0,7 signifie que le mod\u00e8le a correctement pr\u00e9dit 70% des r\u00e9sultats positifs. Cependant, si le rappel est \u00e9lev\u00e9 mais que la pr\u00e9cision est faible, cela peut indiquer que le mod\u00e8le pr\u00e9dit trop de r\u00e9sultats positifs, y compris des r\u00e9sultats faux positifs. Score F1 : Le score F1 est une moyenne harmonique de la pr\u00e9cision et du rappel. Il est g\u00e9n\u00e9ralement utilis\u00e9 lorsque l'on souhaite trouver un \u00e9quilibre entre la pr\u00e9cision et le rappel. Matrice de confusion : La matrice de confusion montre le nombre de pr\u00e9dictions correctes et incorrectes pour chaque classe. Elle permet de voir o\u00f9 le mod\u00e8le est en train de se tromper. Par exemple, si le mod\u00e8le pr\u00e9dit souvent la classe 0 alors qu'elle est r\u00e9ellement la classe 1, il peut y avoir une confusion entre ces deux classes. En g\u00e9n\u00e9ral, il est important d'\u00e9valuer le mod\u00e8le avec plusieurs m\u00e9triques pour avoir une vue d'ensemble de ses performances. Selon le contexte, certaines m\u00e9triques peuvent \u00eatre plus importantes que d'autres. Par exemple, dans les probl\u00e8mes de d\u00e9tection de fraude, un rappel \u00e9lev\u00e9 peut \u00eatre plus important que la pr\u00e9cision, car il est crucial de ne pas manquer de cas de fraude, m\u00eame si cela signifie avoir plus de faux positifs.","title":"Interpr\u00e9ter les m\u00e9triques d'\u00e9valuation de classification:"},{"location":"ml/","text":"On fait quoi avec ? Le machine learning peut \u00eatre utilis\u00e9 pour diff\u00e9rentes t\u00e2ches, selon le type de donn\u00e9es que vous avez et le probl\u00e8me que vous essayez de r\u00e9soudre. Voici un aper\u00e7u de quelques-unes des t\u00e2ches les plus courantes : R\u00e9gression : la r\u00e9gression consiste \u00e0 pr\u00e9dire une valeur num\u00e9rique en fonction d'un ou plusieurs param\u00e8tres. Par exemple, vous pouvez utiliser la r\u00e9gression pour pr\u00e9dire le prix d'une maison en fonction de son emplacement, de sa taille et de ses caract\u00e9ristiques. Classification : la classification consiste \u00e0 pr\u00e9dire une \u00e9tiquette discr\u00e8te en fonction d'un ensemble de caract\u00e9ristiques. Par exemple, vous pouvez utiliser la classification pour pr\u00e9dire si un email est du spam ou non, en fonction de son contenu et de ses caract\u00e9ristiques. Clustering : le clustering consiste \u00e0 regrouper des points de donn\u00e9es similaires dans des groupes, sans \u00e9tiquettes de classe pr\u00e9d\u00e9finies. Par exemple, vous pouvez utiliser le clustering pour regrouper les clients de votre boutique en fonction de leurs habitudes d'achat. R\u00e9duction de dimension : la r\u00e9duction de dimension consiste \u00e0 r\u00e9duire la complexit\u00e9 des donn\u00e9es en r\u00e9duisant le nombre de variables ou de caract\u00e9ristiques. Par exemple, vous pouvez utiliser la r\u00e9duction de dimension pour visualiser des donn\u00e9es \u00e0 haute dimension en deux ou trois dimensions. Types de M L Il existe deux types principaux : L'apprentissage supervis\u00e9 Dans ce type d'apprentissage, le mod\u00e8le apprend \u00e0 pr\u00e9dire une sortie \u00e0 partir d'une entr\u00e9e, en utilisant un ensemble de donn\u00e9es d'apprentissage qui contient des exemples d'entr\u00e9es et de sorties connues. Le mod\u00e8le ajuste ensuite ses param\u00e8tres pour minimiser l'erreur de pr\u00e9diction. Voici quelques mod\u00e8les de machine learning supervis\u00e9s : R\u00e9gression lin\u00e9aire La r\u00e9gression lin\u00e9aire est une m\u00e9thode de pr\u00e9diction qui permet de mod\u00e9liser la relation entre une variable cible continue et une ou plusieurs variables pr\u00e9dictives continues ou cat\u00e9gorielles. Le but de la r\u00e9gression lin\u00e9aire est de trouver la meilleure droite (ou hyperplan en dimension sup\u00e9rieure) qui repr\u00e9sente la relation entre les variables. Le mod\u00e8le peut \u00eatre utilis\u00e9 pour la pr\u00e9diction ou pour comprendre la relation entre les variables. Par exemple, dans le cas de la pr\u00e9diction de prix de vente d'une maison, les variables pr\u00e9dictives pourraient \u00eatre la superficie, le nombre de chambres, la localisation, etc. Exemple de code : from sklearn.linear_model import LinearRegression # Cr\u00e9ation d'un mod\u00e8le de r\u00e9gression lin\u00e9aire regression_model = LinearRegression() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement regression_model.fit(X_train, y_train) # Pr\u00e9diction des valeurs pour les donn\u00e9es de test y_pred = regression_model.predict(X_test) R\u00e9gression logistique La r\u00e9gression logistique est une m\u00e9thode de classification qui permet de pr\u00e9dire la probabilit\u00e9 qu'une observation appartienne \u00e0 une classe particuli\u00e8re (binaire ou multi-classes). La sortie est une probabilit\u00e9, donc la valeur pr\u00e9dite est toujours comprise entre 0 et 1. Le mod\u00e8le estime la probabilit\u00e9 de l'appartenance \u00e0 chaque classe en fonction des variables pr\u00e9dictives. Le mod\u00e8le est souvent utilis\u00e9 pour la classification de clients potentiels, de mails comme spam ou non, etc. Exemple de code : from sklearn.linear_model import LogisticRegression # Cr\u00e9ation d'un mod\u00e8le de r\u00e9gression logistique logistic_model = LogisticRegression() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement logistic_model.fit(X_train, y_train) # Pr\u00e9diction des classes pour les donn\u00e9es de test y_pred = logistic_model.predict(X_test) Arbre de d\u00e9cision Les arbres de d\u00e9cision sont une m\u00e9thode de classification et de r\u00e9gression qui permettent de cr\u00e9er un mod\u00e8le \u00e0 partir des donn\u00e9es en formes d'arbre. Les noeuds internes de l'arbre repr\u00e9sentent une condition sur les variables pr\u00e9dictives, les feuilles de l'arbre repr\u00e9sentent les pr\u00e9dictions de la variable cible. Le mod\u00e8le est facilement interpr\u00e9table et permet de comprendre comment les variables sont utilis\u00e9es pour la pr\u00e9diction. Exemple de code : from sklearn.tree import DecisionTreeClassifier # Cr\u00e9ation d'un mod\u00e8le d'arbre de d\u00e9cision tree_model = DecisionTreeClassifier() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement tree_model.fit(X_train, y_train) # Pr\u00e9diction des classes pour les donn\u00e9es de test y_pred = tree_model.predict(X_test) For\u00eat d'arbres d\u00e9cisionnels Les for\u00eats d'arbres d\u00e9cisionnels sont une m\u00e9thode de classification et de r\u00e9gression qui combine les r\u00e9sultats de plusieurs arbres de d\u00e9cision. Chaque arbre dans la for\u00eat est entra\u00een\u00e9 sur un \u00e9chantillon al\u00e9atoire des donn\u00e9es d'entra\u00eenement. Le mod\u00e8le agr\u00e8ge les pr\u00e9dictions des diff\u00e9rents arbres pour donner une pr\u00e9diction finale. Les for\u00eats d'arbres d\u00e9cisionnels sont souvent plus pr\u00e9cises que les arbres de d\u00e9cision. Voici un exemple de code utilisant Scikit-learn pour entra\u00eener un mod\u00e8le de for\u00eat al\u00e9atoire pour la classification : from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = RandomForestClassifier(max_depth=2, random_state=0) clf.fit(X, y) Les Machines \u00e0 Vecteurs de Support (SVM) Les SVM sont un algorithme d'apprentissage supervis\u00e9 utilis\u00e9 pour la classification et la r\u00e9gression. Les SVM trouvent une fronti\u00e8re de d\u00e9cision qui maximise la marge entre les deux classes. Cette fronti\u00e8re de d\u00e9cision est appel\u00e9e un hyperplan. Les SVM peuvent \u00eatre utilis\u00e9es pour des t\u00e2ches de classification avec deux classes ou plus. Les SVM ont une grande pr\u00e9cision et sont populaires dans les t\u00e2ches de classification. Ils peuvent \u00e9galement \u00eatre utilis\u00e9s pour la r\u00e9gression. Voici un exemple de code utilisant Scikit-learn pour entra\u00eener un mod\u00e8le SVM pour la classification : from sklearn import svm from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = svm.SVC(kernel='linear', C=1, random_state=0) clf.fit(X, y) L'apprentissage non supervis\u00e9 Les mod\u00e8les non supervis\u00e9s sont des algorithmes de machine learning qui n'ont pas besoin de donn\u00e9es \u00e9tiquet\u00e9es pour fonctionner. Ils sont utilis\u00e9s pour explorer les donn\u00e9es et trouver des structures cach\u00e9es, des mod\u00e8les et des corr\u00e9lations. Voici quelques exemples de mod\u00e8les non supervis\u00e9s: K-means clustering Le K-means clustering est une technique de partitionnement de donn\u00e9es qui permet de diviser un ensemble de donn\u00e9es en K groupes distincts (clusters). Chaque point de donn\u00e9es est affect\u00e9 \u00e0 un cluster en fonction de sa proximit\u00e9 avec le centre de ce cluster. Voici un exemple de code en Python pour appliquer le K-means clustering : from sklearn.cluster import KMeans import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 2) # Instancier le mod\u00e8le de clustering kmeans = KMeans(n_clusters=3) # Fitter le mod\u00e8le aux donn\u00e9es kmeans.fit(X) # Pr\u00e9dire les clusters pour de nouvelles donn\u00e9es labels = kmeans.predict(X) Analyse en composantes principales (PCA) L'analyse en composantes principales est une m\u00e9thode de r\u00e9duction de dimension qui permet de projeter des donn\u00e9es \u00e0 haute dimension sur un espace de dimension inf\u00e9rieure tout en conservant autant que possible les informations contenues dans les donn\u00e9es originales. Voici un exemple de code en Python pour appliquer l'analyse en composantes principales : from sklearn.decomposition import PCA import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 10) # Instancier le mod\u00e8le de PCA pca = PCA(n_components=3) # Fitter le mod\u00e8le aux donn\u00e9es pca.fit(X) # Transformer les donn\u00e9es pour obtenir les nouvelles dimensions X_pca = pca.transform(X) R\u00e9duction de dimension avec t-SNE t-SNE est une m\u00e9thode de r\u00e9duction de dimension qui permet de projeter des donn\u00e9es \u00e0 haute dimension sur un espace de dimension inf\u00e9rieure en conservant autant que possible les relations de proximit\u00e9 entre les donn\u00e9es. Voici un exemple de code en Python pour appliquer la r\u00e9duction de dimension avec t-SNE : from sklearn.manifold import TSNE import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 10) # Instancier le mod\u00e8le de t-SNE tsne = TSNE(n_components=2) # Fitter le mod\u00e8le aux donn\u00e9es X_tsne = tsne.fit_transform(X)","title":"Les taches et mod\u00e9les de ML"},{"location":"ml/#on-fait-quoi-avec","text":"Le machine learning peut \u00eatre utilis\u00e9 pour diff\u00e9rentes t\u00e2ches, selon le type de donn\u00e9es que vous avez et le probl\u00e8me que vous essayez de r\u00e9soudre. Voici un aper\u00e7u de quelques-unes des t\u00e2ches les plus courantes : R\u00e9gression : la r\u00e9gression consiste \u00e0 pr\u00e9dire une valeur num\u00e9rique en fonction d'un ou plusieurs param\u00e8tres. Par exemple, vous pouvez utiliser la r\u00e9gression pour pr\u00e9dire le prix d'une maison en fonction de son emplacement, de sa taille et de ses caract\u00e9ristiques. Classification : la classification consiste \u00e0 pr\u00e9dire une \u00e9tiquette discr\u00e8te en fonction d'un ensemble de caract\u00e9ristiques. Par exemple, vous pouvez utiliser la classification pour pr\u00e9dire si un email est du spam ou non, en fonction de son contenu et de ses caract\u00e9ristiques. Clustering : le clustering consiste \u00e0 regrouper des points de donn\u00e9es similaires dans des groupes, sans \u00e9tiquettes de classe pr\u00e9d\u00e9finies. Par exemple, vous pouvez utiliser le clustering pour regrouper les clients de votre boutique en fonction de leurs habitudes d'achat. R\u00e9duction de dimension : la r\u00e9duction de dimension consiste \u00e0 r\u00e9duire la complexit\u00e9 des donn\u00e9es en r\u00e9duisant le nombre de variables ou de caract\u00e9ristiques. Par exemple, vous pouvez utiliser la r\u00e9duction de dimension pour visualiser des donn\u00e9es \u00e0 haute dimension en deux ou trois dimensions.","title":"On fait quoi avec ?"},{"location":"ml/#types-de-m-l","text":"Il existe deux types principaux :","title":"Types de M L"},{"location":"ml/#lapprentissage-supervise","text":"Dans ce type d'apprentissage, le mod\u00e8le apprend \u00e0 pr\u00e9dire une sortie \u00e0 partir d'une entr\u00e9e, en utilisant un ensemble de donn\u00e9es d'apprentissage qui contient des exemples d'entr\u00e9es et de sorties connues. Le mod\u00e8le ajuste ensuite ses param\u00e8tres pour minimiser l'erreur de pr\u00e9diction. Voici quelques mod\u00e8les de machine learning supervis\u00e9s :","title":"L'apprentissage supervis\u00e9"},{"location":"ml/#regression-lineaire","text":"La r\u00e9gression lin\u00e9aire est une m\u00e9thode de pr\u00e9diction qui permet de mod\u00e9liser la relation entre une variable cible continue et une ou plusieurs variables pr\u00e9dictives continues ou cat\u00e9gorielles. Le but de la r\u00e9gression lin\u00e9aire est de trouver la meilleure droite (ou hyperplan en dimension sup\u00e9rieure) qui repr\u00e9sente la relation entre les variables. Le mod\u00e8le peut \u00eatre utilis\u00e9 pour la pr\u00e9diction ou pour comprendre la relation entre les variables. Par exemple, dans le cas de la pr\u00e9diction de prix de vente d'une maison, les variables pr\u00e9dictives pourraient \u00eatre la superficie, le nombre de chambres, la localisation, etc. Exemple de code : from sklearn.linear_model import LinearRegression # Cr\u00e9ation d'un mod\u00e8le de r\u00e9gression lin\u00e9aire regression_model = LinearRegression() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement regression_model.fit(X_train, y_train) # Pr\u00e9diction des valeurs pour les donn\u00e9es de test y_pred = regression_model.predict(X_test)","title":"R\u00e9gression lin\u00e9aire"},{"location":"ml/#regression-logistique","text":"La r\u00e9gression logistique est une m\u00e9thode de classification qui permet de pr\u00e9dire la probabilit\u00e9 qu'une observation appartienne \u00e0 une classe particuli\u00e8re (binaire ou multi-classes). La sortie est une probabilit\u00e9, donc la valeur pr\u00e9dite est toujours comprise entre 0 et 1. Le mod\u00e8le estime la probabilit\u00e9 de l'appartenance \u00e0 chaque classe en fonction des variables pr\u00e9dictives. Le mod\u00e8le est souvent utilis\u00e9 pour la classification de clients potentiels, de mails comme spam ou non, etc. Exemple de code : from sklearn.linear_model import LogisticRegression # Cr\u00e9ation d'un mod\u00e8le de r\u00e9gression logistique logistic_model = LogisticRegression() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement logistic_model.fit(X_train, y_train) # Pr\u00e9diction des classes pour les donn\u00e9es de test y_pred = logistic_model.predict(X_test)","title":"R\u00e9gression logistique"},{"location":"ml/#arbre-de-decision","text":"Les arbres de d\u00e9cision sont une m\u00e9thode de classification et de r\u00e9gression qui permettent de cr\u00e9er un mod\u00e8le \u00e0 partir des donn\u00e9es en formes d'arbre. Les noeuds internes de l'arbre repr\u00e9sentent une condition sur les variables pr\u00e9dictives, les feuilles de l'arbre repr\u00e9sentent les pr\u00e9dictions de la variable cible. Le mod\u00e8le est facilement interpr\u00e9table et permet de comprendre comment les variables sont utilis\u00e9es pour la pr\u00e9diction. Exemple de code : from sklearn.tree import DecisionTreeClassifier # Cr\u00e9ation d'un mod\u00e8le d'arbre de d\u00e9cision tree_model = DecisionTreeClassifier() # Entra\u00eenement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement tree_model.fit(X_train, y_train) # Pr\u00e9diction des classes pour les donn\u00e9es de test y_pred = tree_model.predict(X_test)","title":"Arbre de d\u00e9cision"},{"location":"ml/#foret-darbres-decisionnels","text":"Les for\u00eats d'arbres d\u00e9cisionnels sont une m\u00e9thode de classification et de r\u00e9gression qui combine les r\u00e9sultats de plusieurs arbres de d\u00e9cision. Chaque arbre dans la for\u00eat est entra\u00een\u00e9 sur un \u00e9chantillon al\u00e9atoire des donn\u00e9es d'entra\u00eenement. Le mod\u00e8le agr\u00e8ge les pr\u00e9dictions des diff\u00e9rents arbres pour donner une pr\u00e9diction finale. Les for\u00eats d'arbres d\u00e9cisionnels sont souvent plus pr\u00e9cises que les arbres de d\u00e9cision. Voici un exemple de code utilisant Scikit-learn pour entra\u00eener un mod\u00e8le de for\u00eat al\u00e9atoire pour la classification : from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = RandomForestClassifier(max_depth=2, random_state=0) clf.fit(X, y)","title":"For\u00eat d'arbres d\u00e9cisionnels"},{"location":"ml/#les-machines-a-vecteurs-de-support-svm","text":"Les SVM sont un algorithme d'apprentissage supervis\u00e9 utilis\u00e9 pour la classification et la r\u00e9gression. Les SVM trouvent une fronti\u00e8re de d\u00e9cision qui maximise la marge entre les deux classes. Cette fronti\u00e8re de d\u00e9cision est appel\u00e9e un hyperplan. Les SVM peuvent \u00eatre utilis\u00e9es pour des t\u00e2ches de classification avec deux classes ou plus. Les SVM ont une grande pr\u00e9cision et sont populaires dans les t\u00e2ches de classification. Ils peuvent \u00e9galement \u00eatre utilis\u00e9s pour la r\u00e9gression. Voici un exemple de code utilisant Scikit-learn pour entra\u00eener un mod\u00e8le SVM pour la classification : from sklearn import svm from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = svm.SVC(kernel='linear', C=1, random_state=0) clf.fit(X, y)","title":"Les Machines \u00e0 Vecteurs de Support (SVM)"},{"location":"ml/#lapprentissage-non-supervise","text":"Les mod\u00e8les non supervis\u00e9s sont des algorithmes de machine learning qui n'ont pas besoin de donn\u00e9es \u00e9tiquet\u00e9es pour fonctionner. Ils sont utilis\u00e9s pour explorer les donn\u00e9es et trouver des structures cach\u00e9es, des mod\u00e8les et des corr\u00e9lations. Voici quelques exemples de mod\u00e8les non supervis\u00e9s:","title":"L'apprentissage non supervis\u00e9"},{"location":"ml/#k-means-clustering","text":"Le K-means clustering est une technique de partitionnement de donn\u00e9es qui permet de diviser un ensemble de donn\u00e9es en K groupes distincts (clusters). Chaque point de donn\u00e9es est affect\u00e9 \u00e0 un cluster en fonction de sa proximit\u00e9 avec le centre de ce cluster. Voici un exemple de code en Python pour appliquer le K-means clustering : from sklearn.cluster import KMeans import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 2) # Instancier le mod\u00e8le de clustering kmeans = KMeans(n_clusters=3) # Fitter le mod\u00e8le aux donn\u00e9es kmeans.fit(X) # Pr\u00e9dire les clusters pour de nouvelles donn\u00e9es labels = kmeans.predict(X)","title":"K-means clustering"},{"location":"ml/#analyse-en-composantes-principales-pca","text":"L'analyse en composantes principales est une m\u00e9thode de r\u00e9duction de dimension qui permet de projeter des donn\u00e9es \u00e0 haute dimension sur un espace de dimension inf\u00e9rieure tout en conservant autant que possible les informations contenues dans les donn\u00e9es originales. Voici un exemple de code en Python pour appliquer l'analyse en composantes principales : from sklearn.decomposition import PCA import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 10) # Instancier le mod\u00e8le de PCA pca = PCA(n_components=3) # Fitter le mod\u00e8le aux donn\u00e9es pca.fit(X) # Transformer les donn\u00e9es pour obtenir les nouvelles dimensions X_pca = pca.transform(X)","title":"Analyse en composantes principales (PCA)"},{"location":"ml/#reduction-de-dimension-avec-t-sne","text":"t-SNE est une m\u00e9thode de r\u00e9duction de dimension qui permet de projeter des donn\u00e9es \u00e0 haute dimension sur un espace de dimension inf\u00e9rieure en conservant autant que possible les relations de proximit\u00e9 entre les donn\u00e9es. Voici un exemple de code en Python pour appliquer la r\u00e9duction de dimension avec t-SNE : from sklearn.manifold import TSNE import numpy as np # G\u00e9n\u00e9rer des donn\u00e9es al\u00e9atoires X = np.random.rand(100, 10) # Instancier le mod\u00e8le de t-SNE tsne = TSNE(n_components=2) # Fitter le mod\u00e8le aux donn\u00e9es X_tsne = tsne.fit_transform(X)","title":"R\u00e9duction de dimension avec t-SNE"},{"location":"numpy/","text":"NumPy NumPy est l'abr\u00e9viation de Numerical Python. C'est une biblioth\u00e8que/package Python utilis\u00e9e pour travailler avec des tableaux qui contiennent des classes, des fonctions, des variables, une grande biblioth\u00e8que de fonctions math\u00e9matiques, etc. pour travailler avec des calculs scientifiques. Il peut \u00eatre utilis\u00e9 pour cr\u00e9er un tableau \"n\" dimensionnel o\u00f9 \"n\" est un entier quelconque. Pourquoi NumPy ? En Python, nous avons des listes qui servent de tableaux, mais elles sont lentes. NumPy vise \u00e0 fournir un objet de tableau qui est jusqu'\u00e0 50 fois plus rapide qu'une liste Python traditionnelle. L'objet de tableau dans NumPy s'appelle ndarray, il fournit de nombreuses fonctions de soutien qui rendent le travail avec ndarray tr\u00e8s facile. Les tableaux sont tr\u00e8s fr\u00e9quemment utilis\u00e9s en science des donn\u00e9es, o\u00f9 la vitesse et les ressources sont tr\u00e8s importantes. Ce qui rend les tableaux NumPy plus rapides que les listes : les tableaux NumPy sont stock\u00e9s \u00e0 un endroit continu en m\u00e9moire contrairement aux listes, de sorte que les processus peuvent y acc\u00e9der et les manipuler tr\u00e8s efficacement. Ce comportement est appel\u00e9 la localit\u00e9 de r\u00e9f\u00e9rence. C'est la principale raison pour laquelle NumPy est plus rapide que les listes. Il est \u00e9galement optimis\u00e9 pour fonctionner avec les derni\u00e8res architectures de processeur. Installation de NumPy Pour installer NumPy, vous pouvez utiliser pip, l'installateur de paquets Python. Ouvrez votre terminal ou votre invite de commande et entrez la commande suivante : Copy code pip3 install numpy Importation de NumPy Il existe deux fa\u00e7ons d'importer NumPy. Exemple de code : python Copy code cela importera l'ensemble du module NumPy. import numpy as np cela importera toutes les classes, les objets, les variables, etc. du package NumPy from numpy import* NumPy est g\u00e9n\u00e9ralement import\u00e9 sous l'alias np. alias : en Python, les alias sont un nom alternatif pour faire r\u00e9f\u00e9rence \u00e0 la m\u00eame chose. Cr\u00e9ation de tableaux NumPy L'objet de tableau dans NumPy s'appelle ndarray. Nous pouvons cr\u00e9er un objet ndarray NumPy en utilisant la fonction array(). Les tableaux NumPy peuvent \u00eatre cr\u00e9\u00e9s de plusieurs mani\u00e8res. Voici quelques-unes des m\u00e9thodes les plus courantes : Utilisation de la fonction numpy.array() pour cr\u00e9er un tableau \u00e0 partir d'une liste/d'un tuple : css Copy code a = np.array([1, 2, 3]) Utilisation de la fonction numpy.zeros() pour cr\u00e9er un tableau rempli de z\u00e9ros : bash Copy code b = np.zeros((2, 3)) Utilisation de la fonction numpy.ones() pour cr\u00e9er un tableau rempli de uns : bash Copy code c = np.ones((2, 3)) Utilisation de la fonction numpy.random.randint() : renvoie un tableau d'entiers al\u00e9atoires entre les deux nombres donn\u00e9s. lua Copy code d = np.random.randint(0, 10) Utilisation de la fonction numpy.random.rand() pour cr\u00e9er un tableau de valeurs al\u00e9atoires : lua Copy code e = np.random.rand(2, 3) Dimensions des tableaux NumPy","title":"NumPy"},{"location":"numpy/#numpy","text":"NumPy est l'abr\u00e9viation de Numerical Python. C'est une biblioth\u00e8que/package Python utilis\u00e9e pour travailler avec des tableaux qui contiennent des classes, des fonctions, des variables, une grande biblioth\u00e8que de fonctions math\u00e9matiques, etc. pour travailler avec des calculs scientifiques. Il peut \u00eatre utilis\u00e9 pour cr\u00e9er un tableau \"n\" dimensionnel o\u00f9 \"n\" est un entier quelconque. Pourquoi NumPy ? En Python, nous avons des listes qui servent de tableaux, mais elles sont lentes. NumPy vise \u00e0 fournir un objet de tableau qui est jusqu'\u00e0 50 fois plus rapide qu'une liste Python traditionnelle. L'objet de tableau dans NumPy s'appelle ndarray, il fournit de nombreuses fonctions de soutien qui rendent le travail avec ndarray tr\u00e8s facile. Les tableaux sont tr\u00e8s fr\u00e9quemment utilis\u00e9s en science des donn\u00e9es, o\u00f9 la vitesse et les ressources sont tr\u00e8s importantes. Ce qui rend les tableaux NumPy plus rapides que les listes : les tableaux NumPy sont stock\u00e9s \u00e0 un endroit continu en m\u00e9moire contrairement aux listes, de sorte que les processus peuvent y acc\u00e9der et les manipuler tr\u00e8s efficacement. Ce comportement est appel\u00e9 la localit\u00e9 de r\u00e9f\u00e9rence. C'est la principale raison pour laquelle NumPy est plus rapide que les listes. Il est \u00e9galement optimis\u00e9 pour fonctionner avec les derni\u00e8res architectures de processeur. Installation de NumPy Pour installer NumPy, vous pouvez utiliser pip, l'installateur de paquets Python. Ouvrez votre terminal ou votre invite de commande et entrez la commande suivante : Copy code pip3 install numpy Importation de NumPy Il existe deux fa\u00e7ons d'importer NumPy. Exemple de code : python Copy code","title":"NumPy"},{"location":"numpy/#cela-importera-lensemble-du-module-numpy","text":"import numpy as np","title":"cela importera l'ensemble du module NumPy."},{"location":"numpy/#cela-importera-toutes-les-classes-les-objets-les-variables-etc-du-package-numpy","text":"from numpy import* NumPy est g\u00e9n\u00e9ralement import\u00e9 sous l'alias np. alias : en Python, les alias sont un nom alternatif pour faire r\u00e9f\u00e9rence \u00e0 la m\u00eame chose. Cr\u00e9ation de tableaux NumPy L'objet de tableau dans NumPy s'appelle ndarray. Nous pouvons cr\u00e9er un objet ndarray NumPy en utilisant la fonction array(). Les tableaux NumPy peuvent \u00eatre cr\u00e9\u00e9s de plusieurs mani\u00e8res. Voici quelques-unes des m\u00e9thodes les plus courantes : Utilisation de la fonction numpy.array() pour cr\u00e9er un tableau \u00e0 partir d'une liste/d'un tuple : css Copy code a = np.array([1, 2, 3]) Utilisation de la fonction numpy.zeros() pour cr\u00e9er un tableau rempli de z\u00e9ros : bash Copy code b = np.zeros((2, 3)) Utilisation de la fonction numpy.ones() pour cr\u00e9er un tableau rempli de uns : bash Copy code c = np.ones((2, 3)) Utilisation de la fonction numpy.random.randint() : renvoie un tableau d'entiers al\u00e9atoires entre les deux nombres donn\u00e9s. lua Copy code d = np.random.randint(0, 10) Utilisation de la fonction numpy.random.rand() pour cr\u00e9er un tableau de valeurs al\u00e9atoires : lua Copy code e = np.random.rand(2, 3) Dimensions des tableaux NumPy","title":"cela importera toutes les classes, les objets, les variables, etc. du package NumPy"},{"location":"pd/","text":"pd","title":"Pd"},{"location":"plt/","text":"","title":"Plt"},{"location":"sns/","text":"","title":"Sns"},{"location":"docker/docker_best_practice/","text":"Best Practices Best Practices for design and optimize containers Use an appropriate base image The base image you choose can greatly affect the size and security of your final image. Choose a minimal base image and only include what you need to minimize the attack surface and reduce the image size. One popular base image is Alpine Linux. Alpine Linux is a lightweight Linux distribution that is designed to be small and efficient. It is commonly used for Docker images because of its small size, which makes it ideal for running containers with limited resources. Another base image that is commonly used is the slim version of the official images provided by different software vendors. For example, the official Python image has a slim version, which is a smaller image with only the necessary dependencies required to run Python applications. This means that you can reduce the size of your Docker image by using the slim version instead of the full version. However, it is important to keep in mind that using a base image that is too small can sometimes cause issues. Avoid running as root Running containers as the root user is considered bad practice because it poses a security risk. By default, the root user inside a container has the same privileges as the root user on the host machine. This means that if an attacker gains control of a container running as root, they could potentially escalate their privileges to the host machine. To avoid running containers as root, you can specify a non-root user in your Dockerfile using the USER instruction. For example: FROM python:3.9-slim # Create a non-root user RUN useradd --create-home myuser WORKDIR /home/myuser # Switch to the non-root user USER myuser # Copy application files and install dependencies COPY requirements.txt . RUN pip install -r requirements.txt # Copy the rest of the application code COPY . . # Run the application CMD [ \"python\" , \"app.py\" ] In this example, we create a non-root user called myuser and switch to that user using the USER instruction. This user is then used to run the application inside the container. Keep layers small When building a Docker image, each line in the Dockerfile creates a new layer in the final image. Layers are stacked on top of each other to create the final image. Each layer only stores the changes made on top of the previous layer, which results in a more efficient use of disk space and a faster build time. Keeping layers small is important because it can make it easier to update or modify specific parts of the image without rebuilding the entire image. This can be especially important when dealing with large applications that have many dependencies. To keep layers small, it is best to group related commands together in a single line in the Dockerfile. For example, instead of installing several packages in separate RUN commands, it is better to install them all in a single RUN command. This will result in fewer layers and a smaller image size. It is also important to clean up any temporary files created during the build process, as these files can add unnecessary weight to the image. The RUN command should be followed by a CLEANUP command to remove any unwanted files and dependencies. Additionally, using the --no-cache flag when building an image can help to reduce the size of layers, as it prevents Docker from caching layers and forces it to rebuild each layer from scratch. Use multi-stage builds Multi-stage builds are a way to optimize your Docker images and reduce their size. It allows you to use multiple FROM statements in your Dockerfile, each of which specifies a different base image. Here's an example of how you might use multi-stage builds to build a Python application using Flask: # Use an official Python runtime as a parent image FROM python:3.8-slim-buster AS base # Set the working directory to /app WORKDIR /app # Copy the requirements file to the working directory COPY requirements.txt . # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir --upgrade pip && \\ pip install --no-cache-dir -r requirements.txt # Use a smaller image as the final base image FROM python:3.8-slim-buster # Set the working directory to /app WORKDIR /app # Copy the requirements file and installed packages from the previous stage COPY --from = base /usr/local/lib/python3.8/site-packages /usr/local/lib/python3.8/site-packages # Copy the rest of the application code to the working directory COPY . . # Start the Flask application CMD [ \"python\" , \"app.py\" ] In this example, the Dockerfile uses two stages. The first stage starts with the official Python 3.8 slim-buster image, sets the working directory to /app , and copies the requirements.txt file to the working directory. It then installs the required packages specified in the requirements.txt file and saves them to the image. The second stage starts with the same Python 3.8 slim-buster image as the final base image. It sets the working directory to /app , copies the installed packages from the first stage to the image, copies the rest of the application code to the working directory, and starts the Flask application. Using multi-stage builds can significantly reduce the size of your Docker images because you only include the necessary files and dependencies in the final image. In this example, the final image only includes the installed Python packages and the application code, which makes it much smaller than if it included the entire Python runtime. Overall, using multi-stage builds is a best practice for optimizing your Docker images and reducing their size. Use caching to speed up builds In Docker, every instruction in a Dockerfile creates a layer like we seen before. When a Dockerfile is built, Docker caches each layer, so that if the same instruction is used in a future build, Docker can use the cached layer instead of re-executing the instruction. This can greatly speed up the build process. One way to take advantage of caching is to order the instructions in the Dockerfile such that the ones that change frequently are at the end, while the ones that change less frequently are at the beginning. For example, you might start with a base image, then copy in your application code, and finally install any dependencies. It's also possible to explicitly tell Docker to use a cached layer with the --cache-from flag. This can be useful if you have multiple Dockerfiles that share some of the same layers. For example, if you have a base image that is used by multiple applications, you can build that image once and then use it as the cache for future builds of the applications. Here's an example of how to use caching to speed up a Docker build: # Use an official Python runtime as a parent image FROM python:3.9-slim-buster # Set the working directory to /app WORKDIR /app # Copy the requirements file into the container COPY requirements.txt . # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir -r requirements.txt # Copy the rest of the application code into the container COPY . . # Expose port 80 for the web application EXPOSE 80 # Start the web application CMD [ \"python\" , \"app.py\" ] In this example, the COPY requirements.txt . and RUN pip install --no-cache-dir -r requirements.txt instructions are near the beginning of the Dockerfile because they change less frequently than the application code. This means that Docker can cache those layers and reuse them in future builds, even if the application code has changed. Clean up after yourself Cleaning up after yourself is an important best practice for Docker. This means removing any unused images, containers, volumes, and networks that are no longer needed. Not only does it save disk space, but it also ensures that the Docker environment is not cluttered with unnecessary artifacts that may cause conflicts or security issues. Here are some tips for cleaning up after yourself in Docker: Remove unused containers : To remove unused containers, use the docker container prune command. This command removes all stopped containers. If you want to remove a specific container, use the docker rm command followed by the container ID. Remove unused images : To remove unused images, use the docker image prune command. This command removes all images that are not associated with any container. If you want to remove a specific image, use the docker rmi command followed by the image ID. Remove unused volumes : To remove unused volumes, use the docker volume prune command. This command removes all volumes that are not associated with any container. If you want to remove a specific volume, use the docker volume rm command followed by the volume name. Remove unused networks : To remove unused networks, use the docker network prune command. This command removes all networks that are not associated with any container. If you want to remove a specific network, use the docker network rm command followed by the network name. Use --rm option : When running containers, use the --rm option to automatically remove the container when it exits. This is especially useful for testing and development environments where you don't need to keep the container around. By following these best cleaning practices, you can keep your Docker environment clean and avoid cluttering it with unused artifacts. Consider security Security is an important consideration when it comes to Docker containers. Here are some best practices to keep in mind: Use the latest version of the base image : It is important to use the latest version of the base image, as this will ensure that any security vulnerabilities in the base image have been patched. Avoid using root user : Running a container as root can be risky, as it can potentially allow an attacker to gain access to the host system. Instead, use a non-root user. Limit container capabilities : By default, containers have access to all capabilities of the host system. It is important to limit the capabilities of the container to only what it needs. Use a minimal base image : Using a minimal base image, such as Alpine, reduces the attack surface of the container by reducing the number of packages installed. Keep containers up to date : It is important to keep containers up to date with security patches and updates. Scan images for vulnerabilities : Use a vulnerability scanner to scan images for known vulnerabilities. This will help to identify any potential security issues. Consider network security : Secure network access to containers by using network segmentation, firewalls, and VPNs. A common practice is to run containers inside your cloud provider's VPC. Quick scan a container First, install Trivy by following the installation instructions for your operating system from the official Trivy GitHub page: https://github.com/aquasecurity/trivy Then Pull a Docker image that you want to scan, for example, the official Python image: docker pull python:3.9-slim Run Trivy against the image: trivy image python:3.9-slim This will scan the Python image and report any vulnerabilities found in the image's base image or any installed packages. You can also scan a Dockerfile directly using the --file option: trivy --file Dockerfile This will scan the Dockerfile and report any vulnerabilities found in the base image or any installed packages. Note that scanning Docker images is just one part of a comprehensive security strategy for containerized applications. It's also important to ensure that your containers are configured securely, that you use strong authentication and authorization mechanisms, and that you regularly apply security updates and patches to your container images. By following these best practices, you can help to ensure that your Docker containers are secure and less vulnerable to attack. Debugging and Troubleshooting Check container logs : Container logs can provide valuable information on what is happening inside the container. Use docker logs <container-id> to view container logs and troubleshoot issues. Use docker exec to troubleshoot running containers : docker exec allows you to run commands inside a running container, which can be useful for troubleshooting issues. Check container health : Use docker ps to check the status of running containers. If a container is not running, use docker ps -a to view all containers, including stopped ones. Check resource utilization : Docker containers can consume a lot of resources. Use docker stats to view resource utilization for running containers. Use the correct Docker command : Use the correct Docker command for the task at hand. For example, docker stop will gracefully stop a container, while docker kill will forcibly stop a container. Check networking : If your container is not communicating with other containers or services, check the networking configuration. Use docker network ls to view available networks and docker network inspect to view details of a specific network. Consider container security : Ensure that your container is running in a secure environment and follow security best practices for your application like we have seen before. By following best practices for Dockerfile design and container optimization and knowing how to troubleshoot common issues with Docker containers, you can ensure that your Docker containers are running smoothly and securely.","title":"Best Practices"},{"location":"docker/docker_best_practice/#best-practices","text":"","title":"Best Practices"},{"location":"docker/docker_best_practice/#best-practices-for-design-and-optimize-containers","text":"","title":"Best Practices for design and optimize containers"},{"location":"docker/docker_best_practice/#use-an-appropriate-base-image","text":"The base image you choose can greatly affect the size and security of your final image. Choose a minimal base image and only include what you need to minimize the attack surface and reduce the image size. One popular base image is Alpine Linux. Alpine Linux is a lightweight Linux distribution that is designed to be small and efficient. It is commonly used for Docker images because of its small size, which makes it ideal for running containers with limited resources. Another base image that is commonly used is the slim version of the official images provided by different software vendors. For example, the official Python image has a slim version, which is a smaller image with only the necessary dependencies required to run Python applications. This means that you can reduce the size of your Docker image by using the slim version instead of the full version. However, it is important to keep in mind that using a base image that is too small can sometimes cause issues.","title":"Use an appropriate base image"},{"location":"docker/docker_best_practice/#avoid-running-as-root","text":"Running containers as the root user is considered bad practice because it poses a security risk. By default, the root user inside a container has the same privileges as the root user on the host machine. This means that if an attacker gains control of a container running as root, they could potentially escalate their privileges to the host machine. To avoid running containers as root, you can specify a non-root user in your Dockerfile using the USER instruction. For example: FROM python:3.9-slim # Create a non-root user RUN useradd --create-home myuser WORKDIR /home/myuser # Switch to the non-root user USER myuser # Copy application files and install dependencies COPY requirements.txt . RUN pip install -r requirements.txt # Copy the rest of the application code COPY . . # Run the application CMD [ \"python\" , \"app.py\" ] In this example, we create a non-root user called myuser and switch to that user using the USER instruction. This user is then used to run the application inside the container.","title":"Avoid running as root"},{"location":"docker/docker_best_practice/#keep-layers-small","text":"When building a Docker image, each line in the Dockerfile creates a new layer in the final image. Layers are stacked on top of each other to create the final image. Each layer only stores the changes made on top of the previous layer, which results in a more efficient use of disk space and a faster build time. Keeping layers small is important because it can make it easier to update or modify specific parts of the image without rebuilding the entire image. This can be especially important when dealing with large applications that have many dependencies. To keep layers small, it is best to group related commands together in a single line in the Dockerfile. For example, instead of installing several packages in separate RUN commands, it is better to install them all in a single RUN command. This will result in fewer layers and a smaller image size. It is also important to clean up any temporary files created during the build process, as these files can add unnecessary weight to the image. The RUN command should be followed by a CLEANUP command to remove any unwanted files and dependencies. Additionally, using the --no-cache flag when building an image can help to reduce the size of layers, as it prevents Docker from caching layers and forces it to rebuild each layer from scratch.","title":"Keep layers small"},{"location":"docker/docker_best_practice/#use-multi-stage-builds","text":"Multi-stage builds are a way to optimize your Docker images and reduce their size. It allows you to use multiple FROM statements in your Dockerfile, each of which specifies a different base image. Here's an example of how you might use multi-stage builds to build a Python application using Flask: # Use an official Python runtime as a parent image FROM python:3.8-slim-buster AS base # Set the working directory to /app WORKDIR /app # Copy the requirements file to the working directory COPY requirements.txt . # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir --upgrade pip && \\ pip install --no-cache-dir -r requirements.txt # Use a smaller image as the final base image FROM python:3.8-slim-buster # Set the working directory to /app WORKDIR /app # Copy the requirements file and installed packages from the previous stage COPY --from = base /usr/local/lib/python3.8/site-packages /usr/local/lib/python3.8/site-packages # Copy the rest of the application code to the working directory COPY . . # Start the Flask application CMD [ \"python\" , \"app.py\" ] In this example, the Dockerfile uses two stages. The first stage starts with the official Python 3.8 slim-buster image, sets the working directory to /app , and copies the requirements.txt file to the working directory. It then installs the required packages specified in the requirements.txt file and saves them to the image. The second stage starts with the same Python 3.8 slim-buster image as the final base image. It sets the working directory to /app , copies the installed packages from the first stage to the image, copies the rest of the application code to the working directory, and starts the Flask application. Using multi-stage builds can significantly reduce the size of your Docker images because you only include the necessary files and dependencies in the final image. In this example, the final image only includes the installed Python packages and the application code, which makes it much smaller than if it included the entire Python runtime. Overall, using multi-stage builds is a best practice for optimizing your Docker images and reducing their size.","title":"Use multi-stage builds"},{"location":"docker/docker_best_practice/#use-caching-to-speed-up-builds","text":"In Docker, every instruction in a Dockerfile creates a layer like we seen before. When a Dockerfile is built, Docker caches each layer, so that if the same instruction is used in a future build, Docker can use the cached layer instead of re-executing the instruction. This can greatly speed up the build process. One way to take advantage of caching is to order the instructions in the Dockerfile such that the ones that change frequently are at the end, while the ones that change less frequently are at the beginning. For example, you might start with a base image, then copy in your application code, and finally install any dependencies. It's also possible to explicitly tell Docker to use a cached layer with the --cache-from flag. This can be useful if you have multiple Dockerfiles that share some of the same layers. For example, if you have a base image that is used by multiple applications, you can build that image once and then use it as the cache for future builds of the applications. Here's an example of how to use caching to speed up a Docker build: # Use an official Python runtime as a parent image FROM python:3.9-slim-buster # Set the working directory to /app WORKDIR /app # Copy the requirements file into the container COPY requirements.txt . # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir -r requirements.txt # Copy the rest of the application code into the container COPY . . # Expose port 80 for the web application EXPOSE 80 # Start the web application CMD [ \"python\" , \"app.py\" ] In this example, the COPY requirements.txt . and RUN pip install --no-cache-dir -r requirements.txt instructions are near the beginning of the Dockerfile because they change less frequently than the application code. This means that Docker can cache those layers and reuse them in future builds, even if the application code has changed.","title":"Use caching to speed up builds"},{"location":"docker/docker_best_practice/#clean-up-after-yourself","text":"Cleaning up after yourself is an important best practice for Docker. This means removing any unused images, containers, volumes, and networks that are no longer needed. Not only does it save disk space, but it also ensures that the Docker environment is not cluttered with unnecessary artifacts that may cause conflicts or security issues. Here are some tips for cleaning up after yourself in Docker: Remove unused containers : To remove unused containers, use the docker container prune command. This command removes all stopped containers. If you want to remove a specific container, use the docker rm command followed by the container ID. Remove unused images : To remove unused images, use the docker image prune command. This command removes all images that are not associated with any container. If you want to remove a specific image, use the docker rmi command followed by the image ID. Remove unused volumes : To remove unused volumes, use the docker volume prune command. This command removes all volumes that are not associated with any container. If you want to remove a specific volume, use the docker volume rm command followed by the volume name. Remove unused networks : To remove unused networks, use the docker network prune command. This command removes all networks that are not associated with any container. If you want to remove a specific network, use the docker network rm command followed by the network name. Use --rm option : When running containers, use the --rm option to automatically remove the container when it exits. This is especially useful for testing and development environments where you don't need to keep the container around. By following these best cleaning practices, you can keep your Docker environment clean and avoid cluttering it with unused artifacts.","title":"Clean up after yourself"},{"location":"docker/docker_best_practice/#consider-security","text":"Security is an important consideration when it comes to Docker containers. Here are some best practices to keep in mind: Use the latest version of the base image : It is important to use the latest version of the base image, as this will ensure that any security vulnerabilities in the base image have been patched. Avoid using root user : Running a container as root can be risky, as it can potentially allow an attacker to gain access to the host system. Instead, use a non-root user. Limit container capabilities : By default, containers have access to all capabilities of the host system. It is important to limit the capabilities of the container to only what it needs. Use a minimal base image : Using a minimal base image, such as Alpine, reduces the attack surface of the container by reducing the number of packages installed. Keep containers up to date : It is important to keep containers up to date with security patches and updates. Scan images for vulnerabilities : Use a vulnerability scanner to scan images for known vulnerabilities. This will help to identify any potential security issues. Consider network security : Secure network access to containers by using network segmentation, firewalls, and VPNs. A common practice is to run containers inside your cloud provider's VPC.","title":"Consider security"},{"location":"docker/docker_best_practice/#quick-scan-a-container","text":"First, install Trivy by following the installation instructions for your operating system from the official Trivy GitHub page: https://github.com/aquasecurity/trivy Then Pull a Docker image that you want to scan, for example, the official Python image: docker pull python:3.9-slim Run Trivy against the image: trivy image python:3.9-slim This will scan the Python image and report any vulnerabilities found in the image's base image or any installed packages. You can also scan a Dockerfile directly using the --file option: trivy --file Dockerfile This will scan the Dockerfile and report any vulnerabilities found in the base image or any installed packages. Note that scanning Docker images is just one part of a comprehensive security strategy for containerized applications. It's also important to ensure that your containers are configured securely, that you use strong authentication and authorization mechanisms, and that you regularly apply security updates and patches to your container images. By following these best practices, you can help to ensure that your Docker containers are secure and less vulnerable to attack.","title":"Quick scan a container"},{"location":"docker/docker_best_practice/#debugging-and-troubleshooting","text":"Check container logs : Container logs can provide valuable information on what is happening inside the container. Use docker logs <container-id> to view container logs and troubleshoot issues. Use docker exec to troubleshoot running containers : docker exec allows you to run commands inside a running container, which can be useful for troubleshooting issues. Check container health : Use docker ps to check the status of running containers. If a container is not running, use docker ps -a to view all containers, including stopped ones. Check resource utilization : Docker containers can consume a lot of resources. Use docker stats to view resource utilization for running containers. Use the correct Docker command : Use the correct Docker command for the task at hand. For example, docker stop will gracefully stop a container, while docker kill will forcibly stop a container. Check networking : If your container is not communicating with other containers or services, check the networking configuration. Use docker network ls to view available networks and docker network inspect to view details of a specific network. Consider container security : Ensure that your container is running in a secure environment and follow security best practices for your application like we have seen before. By following best practices for Dockerfile design and container optimization and knowing how to troubleshoot common issues with Docker containers, you can ensure that your Docker containers are running smoothly and securely.","title":"Debugging and Troubleshooting"},{"location":"docker/docker_commands/","text":"Docker CLI Commands The Docker CLI (Command Line Interface) provides a set of commands for working with Docker images and containers. These commands are used to build, run, manage, and interact with Docker images and containers. Here are some of the most common Docker CLI commands: docker build This command is used to build a Docker image from a Dockerfile. Example: docker build -t myimage . This command builds a Docker image from the Dockerfile in the current directory and tags it with the name myimage . The option -t or --tag : Sets the name and optionally a tag for the Docker image. docker build options Tag -t Example: docker build -t myimage:latest . or docker build -t myimage:01 . This command builds a Docker image from the Dockerfile in the current directory, tags it with the name myimage and the latest or 01 tags. File -f The option -f , --file : Specifies the name and location of the Dockerfile to use. Example: docker build -t myimage:latest -f path/to/Dockerfile.dev . This command builds a Docker image from the Dockerfile located at path/to/Dockerfile.dev , tags it with the name myimage and the latest tag. Cache --no-cache --no-cache : Disables caching during the build process. Example: docker build --no-cache -t myimage:latest . This command builds a Docker image from the Dockerfile in the current directory, without using any cached layers. docker run This command is used to run a Docker container from a Docker image. Example: docker run --name mycontainer myimage This command runs a Docker container from the myimage Docker image and names the container mycontainer with the tag --name . docker run options Tag -d The option -d , --detach : Runs the container in detached mode, in the background so you can use your terminal as you want is not stuck in the process. Example: docker run -d myimage This command runs the myimage Docker image in detached mode, in the background. Tag -p The option -p , --publish : Publishes a container's port(s) to the host machine. Example: docker run -p 80:80 myimage This command runs the myimage Docker image and maps port 80 inside the container to port 80 on the host machine. Tag --name The option --name : Assigns a name to the container. Example: docker run --name mycontainer myimage This command runs the myimage Docker image and assigns the name mycontainer to the resulting container. Tag -e The option -e , --env : Sets environment variables inside the container. Example: docker run -e MYVAR=myvalue myimage This command runs the myimage Docker image and sets the environment variable MYVAR to myvalue inside the container. Tag -v The option -v , --volume : Mounts a volume from the host machine into the container. Example: docker run -v /path/on/host:/path/in/container myimage This command runs the myimage Docker image and mounts the directory /path/on/host on the host machine to the directory /path/in/container inside the container. Tag -it The option -it , --interactive : Runs the container in interactive mode, allowing input from the user. Example: docker run -it myimage /bin/bash This command runs the myimage Docker image in interactive mode and starts a bash shell inside the container. You can of course use multiple tags like : docker run -d --name mycontainer -p 80:80 myimage This command runs a Docker container from the myimage Docker image in detached mode ( -d ), names the container mycontainer ( --name ), maps port 80 on the host machine to port 80 inside the container ( -p ), and uses the myimage Docker image as the container's base image. docker ps This command is used to list running Docker containers. Example: docker ps This command lists all running Docker containers. You can also list the exited container with the -a option , it is very usefull in case you want to debug a container. docker ps -a docker stop This command is used to stop a running Docker container. Example: docker stop mycontainer This command stops the mycontainer Docker container. docker rm This command is used to remove a stopped Docker container. Example: docker rm mycontainer This command removes the mycontainer Docker container. docker images This command is used to list Docker images. Example: docker images This command lists all Docker images on the local machine. docker rmi This command is used to remove a Docker image. Example: docker rmi myimage This command removes the myimage Docker image. docker exec This command is used to execute a command inside a running Docker container. Example: docker exec mycontainer ls /app This command executes the ls /app command inside the mycontainer Docker container. docker logs This command is used to view the logs for a Docker container. Example: docker logs mycontainer This command displays the logs for the mycontainer Docker container. docker inspect This command is used to view detailed information about a Docker object, such as a container or image. Example: docker inspect mycontainer This command displays detailed information about the mycontainer Docker container. docker pull This command is used to pull a Docker image from a registry. Example: docker pull nginx:latest This command pulls the latest version of the nginx Docker image from the Docker Hub registry. docker push This command is used to push a Docker image to a registry. Example: docker push myregistry/myimage:latest This command pushes the myimage Docker image with the latest tag to the myregistry Docker registry. Wrap-up These are just a few of the most common Docker CLI commands. There are many other commands available that can be used for more advanced use cases, such as networking, volumes, and swarm management. By mastering these basic Docker CLI commands, you can get started with Docker and start","title":"Docker CLI Commands"},{"location":"docker/docker_commands/#docker-cli-commands","text":"The Docker CLI (Command Line Interface) provides a set of commands for working with Docker images and containers. These commands are used to build, run, manage, and interact with Docker images and containers. Here are some of the most common Docker CLI commands:","title":"Docker CLI Commands"},{"location":"docker/docker_commands/#docker-build","text":"This command is used to build a Docker image from a Dockerfile. Example: docker build -t myimage . This command builds a Docker image from the Dockerfile in the current directory and tags it with the name myimage . The option -t or --tag : Sets the name and optionally a tag for the Docker image.","title":"docker build"},{"location":"docker/docker_commands/#docker-build-options","text":"","title":"docker build options"},{"location":"docker/docker_commands/#tag-t","text":"Example: docker build -t myimage:latest . or docker build -t myimage:01 . This command builds a Docker image from the Dockerfile in the current directory, tags it with the name myimage and the latest or 01 tags.","title":"Tag -t"},{"location":"docker/docker_commands/#file-f","text":"The option -f , --file : Specifies the name and location of the Dockerfile to use. Example: docker build -t myimage:latest -f path/to/Dockerfile.dev . This command builds a Docker image from the Dockerfile located at path/to/Dockerfile.dev , tags it with the name myimage and the latest tag.","title":"File -f"},{"location":"docker/docker_commands/#cache-no-cache","text":"--no-cache : Disables caching during the build process. Example: docker build --no-cache -t myimage:latest . This command builds a Docker image from the Dockerfile in the current directory, without using any cached layers.","title":"Cache --no-cache"},{"location":"docker/docker_commands/#docker-run","text":"This command is used to run a Docker container from a Docker image. Example: docker run --name mycontainer myimage This command runs a Docker container from the myimage Docker image and names the container mycontainer with the tag --name .","title":"docker run"},{"location":"docker/docker_commands/#docker-run-options","text":"","title":"docker run options"},{"location":"docker/docker_commands/#tag-d","text":"The option -d , --detach : Runs the container in detached mode, in the background so you can use your terminal as you want is not stuck in the process. Example: docker run -d myimage This command runs the myimage Docker image in detached mode, in the background.","title":"Tag -d"},{"location":"docker/docker_commands/#tag-p","text":"The option -p , --publish : Publishes a container's port(s) to the host machine. Example: docker run -p 80:80 myimage This command runs the myimage Docker image and maps port 80 inside the container to port 80 on the host machine.","title":"Tag -p"},{"location":"docker/docker_commands/#tag-name","text":"The option --name : Assigns a name to the container. Example: docker run --name mycontainer myimage This command runs the myimage Docker image and assigns the name mycontainer to the resulting container.","title":"Tag --name"},{"location":"docker/docker_commands/#tag-e","text":"The option -e , --env : Sets environment variables inside the container. Example: docker run -e MYVAR=myvalue myimage This command runs the myimage Docker image and sets the environment variable MYVAR to myvalue inside the container.","title":"Tag -e"},{"location":"docker/docker_commands/#tag-v","text":"The option -v , --volume : Mounts a volume from the host machine into the container. Example: docker run -v /path/on/host:/path/in/container myimage This command runs the myimage Docker image and mounts the directory /path/on/host on the host machine to the directory /path/in/container inside the container.","title":"Tag -v"},{"location":"docker/docker_commands/#tag-it","text":"The option -it , --interactive : Runs the container in interactive mode, allowing input from the user. Example: docker run -it myimage /bin/bash This command runs the myimage Docker image in interactive mode and starts a bash shell inside the container. You can of course use multiple tags like : docker run -d --name mycontainer -p 80:80 myimage This command runs a Docker container from the myimage Docker image in detached mode ( -d ), names the container mycontainer ( --name ), maps port 80 on the host machine to port 80 inside the container ( -p ), and uses the myimage Docker image as the container's base image.","title":"Tag -it"},{"location":"docker/docker_commands/#docker-ps","text":"This command is used to list running Docker containers. Example: docker ps This command lists all running Docker containers. You can also list the exited container with the -a option , it is very usefull in case you want to debug a container. docker ps -a","title":"docker ps"},{"location":"docker/docker_commands/#docker-stop","text":"This command is used to stop a running Docker container. Example: docker stop mycontainer This command stops the mycontainer Docker container.","title":"docker stop"},{"location":"docker/docker_commands/#docker-rm","text":"This command is used to remove a stopped Docker container. Example: docker rm mycontainer This command removes the mycontainer Docker container.","title":"docker rm"},{"location":"docker/docker_commands/#docker-images","text":"This command is used to list Docker images. Example: docker images This command lists all Docker images on the local machine.","title":"docker images"},{"location":"docker/docker_commands/#docker-rmi","text":"This command is used to remove a Docker image. Example: docker rmi myimage This command removes the myimage Docker image.","title":"docker rmi"},{"location":"docker/docker_commands/#docker-exec","text":"This command is used to execute a command inside a running Docker container. Example: docker exec mycontainer ls /app This command executes the ls /app command inside the mycontainer Docker container.","title":"docker exec"},{"location":"docker/docker_commands/#docker-logs","text":"This command is used to view the logs for a Docker container. Example: docker logs mycontainer This command displays the logs for the mycontainer Docker container.","title":"docker logs"},{"location":"docker/docker_commands/#docker-inspect","text":"This command is used to view detailed information about a Docker object, such as a container or image. Example: docker inspect mycontainer This command displays detailed information about the mycontainer Docker container.","title":"docker inspect"},{"location":"docker/docker_commands/#docker-pull","text":"This command is used to pull a Docker image from a registry. Example: docker pull nginx:latest This command pulls the latest version of the nginx Docker image from the Docker Hub registry.","title":"docker pull"},{"location":"docker/docker_commands/#docker-push","text":"This command is used to push a Docker image to a registry. Example: docker push myregistry/myimage:latest This command pushes the myimage Docker image with the latest tag to the myregistry Docker registry.","title":"docker push"},{"location":"docker/docker_commands/#wrap-up","text":"These are just a few of the most common Docker CLI commands. There are many other commands available that can be used for more advanced use cases, such as networking, volumes, and swarm management. By mastering these basic Docker CLI commands, you can get started with Docker and start","title":"Wrap-up"},{"location":"docker/docker_compose/","text":"Docker-compose What is Docker Compose and Why Use It? Docker Compose is a tool that allows you to define and run multi-container Docker applications. It makes it easy to start and stop multiple containers with a single command, and provides a way to configure the containers and their relationships to each other. Docker Compose is particularly useful for running complex applications that are made up of multiple services, each with its own requirements and dependencies. By using Docker Compose, you can define the configuration for all of these services in a single file, making it easier to manage and deploy your application. YAML syntax YAML (short for \"YAML Ain't Markup Language\") is a human-readable data serialization language. It is often used for configuration files and data exchange between different programming languages . YAML is designed to be easily read by humans and can be used for complex or simple data structures. Docker Compose uses YAML syntax for its configuration files because it is easy to read and write . Docker Compose configuration files define all the services that make up an application, as well as any associated networks, volumes, and environment variables. By using YAML syntax, it allows developers to easily define the relationships between the different parts of an application and deploy it consistently across different environments. Few this to know about yaml syntax : YAML files use indentation to denote hierarchy , instead of curly braces like JSON or XML. The syntax is strict about indentation, so it's important to use consistent spacing (usually 2 or 4 spaces) for each level of hierarchy. Key-value pairs are written as key: value, with the key and value separated by a colon and a space. Lists are denoted by a dash (-) followed by a space , and can contain any type of value. Comments can be added using the # symbol. Here's an example YAML file that defines a simple docker-compose web service: version : '3' services : web : image : nginx:latest ports : - \"8080:80\" In this file: version specifies the Docker Compose file version. services is a list of Docker services to be created and run. web is the name of the first service. image specifies the Docker image to be used for the service. ports maps a port on the host machine to a port in the container. \"8080:80\" maps port 8080 on the host (your local machine or virtual machine in case you are in a VM) to port 80 in the container. This is just a basic example, but hopefully it gives you an idea of how the YAML syntax works but it will be helpful for the next part. Docker Compose for a simple Python App and Redis database Let's create a two containers application with a docker-compose.yml file with a python app and a redis database in order to count how many times the page is reload. First thing first, write our app.py script : from flask import Flask from redis import Redis app = Flask ( __name__ ) redis = Redis ( host = 'redis-container' , port = 6379 ) @app . route ( '/' ) def hello (): redis . incr ( 'hits' ) return ' - - - This basic web page has been viewed {} time(s) - - -' . format ( redis . get ( 'hits' )) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" , debug = True ) and the requirements.txt file : flask redis This is a simple Python Flask web application that increments a counter each time the / route is accessed and displays the number of times it has been accessed. The application uses Redis as a datastore to store the hit counter. Here is how our app.py script works: The Flask library is imported, which allows us to create a web application. The Redis library is imported, which allows us to connect to a Redis instance and manipulate data. The Flask application is created and the Redis client is initialized, connecting to the Redis container named \"redis-container\" at port 6379. A route for the / endpoint is defined. When this route is accessed, the hit counter in Redis is incremented and the current count is displayed on the page. Finally, the application is run, listening on all network interfaces (0.0.0.0) on port 5000 and with debugging enabled. Then, we must write a Dockerfile : FROM python:3.6 WORKDIR /app COPY . . RUN pip install -r requirements.txt CMD python app.py Like before this is a simple Dockerfile for a python application. Write the docker-compose.yml of our app version : '3' services : web : build : ./app ports : - \"5000:5000\" volumes : - ./app:/app depends_on : - redis-container redis-container : image : redis This script is a Docker Compose file that describes two services that will be run in Docker containers: a web service and a Redis service. The web service is defined by the web service block. It specifies that the web service should be built from the Dockerfile in the ./app directory, and should expose port 5000 on the host machine. The volumes directive maps the ./app directory on the host to the /app directory in the container, allowing changes to the code to be immediately reflected in the container. The depends_on directive specifies that the web service should not start until the Redis service is running. The Redis service is defined by the redis-container block. It specifies that the Redis image should be used to create the service. This is the architeture of our project : . |_docker-compose.yml |_app |_Dockerfile |_requirements.txt |_app.py Together, these services can be started with the docker-compose up command, which will build and start the web and Redis containers, and connect them together on a default Docker network. You can also run your project in background with -d option then you should see your containers up and running with the command docker ps Docker Compose for a Python App and PostgreSQL Now that we understand how two containers works together let's code an application with a more efficient database : postgreSQL. What is PostgreSQL PostgreSQL, also known as Postgres, is a powerful and open-source relational database management system. It uses and extends the SQL language and provides many features such as support for JSON and other NoSQL features, scalability, and extensibility. It can run on various platforms such as Windows, macOS, Linux, and Unix. Many organizations use Postgres for their data storage needs due to its reliability, robustness, and community support. Set up the project To create a Docker Compose file for your Python app, you'll need to define the services that make up your application. Each service is defined in the Docker Compose file as a separate block of configuration. In this example we will take the nortwhind database here as base for our database service. Download or git clone the nortwhind database here and open the docker-compose.yml file bellow who define two services, one for a monitoring application pgadmin and one for a PostgreSQL database db : version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db networks : db : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local Let's break down this Docker Compose file: version: '3' : This specifies the version of the Docker Compose file format that we're using. services : This is where we define the services that make up our application. db : The Postgres database service. container_name : Sets the name of the container to db. image : postgres:latest: Specifies the image to use for the container. environment : Sets environment variables for the container. POSTGRES_DB : northwind: Specifies the name of the database to create. POSTGRES_USER : postgres: Specifies the username for the database. POSTGRES_PASSWORD : postgres: Specifies the password for the database. volumes : Mounts volumes for the container. postgresql_bin:/usr/lib/postgresql : Mounts the PostgreSQL binaries. postgresql_data:/var/lib/postgresql/data : Mounts the PostgreSQL data directory. ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql : Copies the northwind.sql script into the container for initializing the database. ./files:/files : Mounts the files directory into the container. ports : Maps ports between the container and the host. 55432:5432 : Maps port 5432 inside the container to port 55432 on the host. networks : Specifies the networks to connect the container to. db : Connects the container to the db network. pgadmin : The pgAdmin web interface service in order to visualize our database image : dpage/pgadmin4 : Specifies the image to use for the container. environment : Sets environment variables for the container. PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org: Specifies the default email for pgAdmin. PGADMIN_DEFAULT_PASSWORD : postgres: Specifies the default password for pgAdmin. PGADMIN_LISTEN_PORT: 5050 : Specifies the port for pgAdmin to listen on. PGADMIN_CONFIG_SERVER_MODE : 'False': Disables server mode for pgAdmin. volumes : Mounts volumes for the container. ports : Maps ports between the container and the host. 5050:5050 : Maps port 5050 inside the container to port 5050 on the host. networks : Specifies the networks to create. db : Creates the db network. driver: bridge : Specifies the driver to use for the network, this is the standard driver \ud83e\udd13 volumes : Specifies the volumes to create, see volume part in the table of content for more detailed Once you've defined your Docker Compose file, you can use the docker-compose up command to start and stop your application. Here are some of the most common commands: docker-compose up : This command starts your application and attaches your terminal to the logs of all running containers. You can use Ctrl+C to stop the containers and exit. docker-compose up -d : This command starts your application in detached mode, which means that it runs in the background. You can use docker-compose logs to view the logs of your containers. docker-compose down : This command stops and removes all containers, networks, and volumes that were created by docker-compose up. docker-compose ps : This command lists all running containers in your Docker Compose application. docker-compose build : This command builds the images for all of the services in your Docker Compose file. By using Docker Compose, you can easily start and stop multiple containers with a single command, and manage the configuration of all of your services in a single file. This makes it easier to manage and deploy complex applications that are made up of multiple services. PgAdmin interface First let's confirm our containers are up and running by taping docker ps command. If you see the container running like : a76abdcbf8da dpage/pgadmin4 \"/entrypoint.sh\" About an hour ago Up About an hour 80 /tcp, 443 /tcp, 0 .0.0.0:5050->5050/tcp, :::5050->5050/tcp To see our database go to : (localhost:5050)[http://localhost:5050] and write a random password (like root) then register our database by running the following command : Add a new server in PgAdmin In the general Tab, write the paramater Name = db In the Connection Tab write the following parameters : Host name: db Username: postgres Password: postgres Then, select database \"northwind\" and you can now see all the tables and metadata \ud83e\udd73 Add a Python app from fastapi import FastAPI from sqlalchemy import create_engine , text from sqlalchemy.orm import sessionmaker app = FastAPI () engine = create_engine ( 'postgresql://postgres:postgres@db/northwind' ) Session = sessionmaker ( bind = engine ) @app . get ( '/' ) def read_root (): session = Session () result = session . execute ( text ( 'SELECT customer_id, company_name, contact_name FROM customers LIMIT 10' )) return { 'Customers info' : [ dict ( customerid = row [ 0 ], companyname = row [ 1 ], contactname = row [ 2 ]) for row in result ]} This application uses the FastAPI framework to define a simple endpoint that returns a JSON response with a greeting and a value from a PostgreSQL database. To run this application using Docker Compose, you'll need to save this file as main.py in the same directory as your Dockerfile, and update your Docker Compose file to include the following environment variable for the app service and dependence : environment : DB_HOST : db depends_on : - db This environment variable tells the application where to find the PostgreSQL database and tell the application to wait for the lunch of the db service. Integrate our app to the docker-compose.yml file version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db app : build : context : . dockerfile : Dockerfile environment : DB_HOST : db depends_on : - db ports : - \"8000:8000\" networks : - db networks : db : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local The app service is a container that will host a Python application that uses the northwind database created by the db service. The app container will be built using the Dockerfile located in the same directory as the docker-compose.yml file. The depends_on property indicates that the app container must be started after the db container is running. The environment property sets the DB_HOST environment variable, which is used in the application to connect to the db container. The ports property maps port 8000 of the app container to port 8000 of the host machine, allowing access to the FastAPI application from a web browser. The networks property specifies that the app container is connected to the db network, allowing communication between the application and the database. Once you've made these changes, you can start your application using Docker Compose with the following command: docker-compose up --build This will start both the db and app services and rebuild it just in case, and you should be able to access the application by visiting http://localhost:8000 in your web browser. Stop docker-compose Stop the server that was launched by docker compose up via Ctrl-C if you are in interactive mode, then remove the containers via: docker-compose down or just go to the root of your repository and run docker-compose down","title":"Docker-compose"},{"location":"docker/docker_compose/#docker-compose","text":"","title":"Docker-compose"},{"location":"docker/docker_compose/#what-is-docker-compose-and-why-use-it","text":"Docker Compose is a tool that allows you to define and run multi-container Docker applications. It makes it easy to start and stop multiple containers with a single command, and provides a way to configure the containers and their relationships to each other. Docker Compose is particularly useful for running complex applications that are made up of multiple services, each with its own requirements and dependencies. By using Docker Compose, you can define the configuration for all of these services in a single file, making it easier to manage and deploy your application.","title":"What is Docker Compose and Why Use It?"},{"location":"docker/docker_compose/#yaml-syntax","text":"YAML (short for \"YAML Ain't Markup Language\") is a human-readable data serialization language. It is often used for configuration files and data exchange between different programming languages . YAML is designed to be easily read by humans and can be used for complex or simple data structures. Docker Compose uses YAML syntax for its configuration files because it is easy to read and write . Docker Compose configuration files define all the services that make up an application, as well as any associated networks, volumes, and environment variables. By using YAML syntax, it allows developers to easily define the relationships between the different parts of an application and deploy it consistently across different environments. Few this to know about yaml syntax : YAML files use indentation to denote hierarchy , instead of curly braces like JSON or XML. The syntax is strict about indentation, so it's important to use consistent spacing (usually 2 or 4 spaces) for each level of hierarchy. Key-value pairs are written as key: value, with the key and value separated by a colon and a space. Lists are denoted by a dash (-) followed by a space , and can contain any type of value. Comments can be added using the # symbol. Here's an example YAML file that defines a simple docker-compose web service: version : '3' services : web : image : nginx:latest ports : - \"8080:80\" In this file: version specifies the Docker Compose file version. services is a list of Docker services to be created and run. web is the name of the first service. image specifies the Docker image to be used for the service. ports maps a port on the host machine to a port in the container. \"8080:80\" maps port 8080 on the host (your local machine or virtual machine in case you are in a VM) to port 80 in the container. This is just a basic example, but hopefully it gives you an idea of how the YAML syntax works but it will be helpful for the next part.","title":"YAML syntax"},{"location":"docker/docker_compose/#docker-compose-for-a-simple-python-app-and-redis-database","text":"Let's create a two containers application with a docker-compose.yml file with a python app and a redis database in order to count how many times the page is reload. First thing first, write our app.py script : from flask import Flask from redis import Redis app = Flask ( __name__ ) redis = Redis ( host = 'redis-container' , port = 6379 ) @app . route ( '/' ) def hello (): redis . incr ( 'hits' ) return ' - - - This basic web page has been viewed {} time(s) - - -' . format ( redis . get ( 'hits' )) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" , debug = True ) and the requirements.txt file : flask redis This is a simple Python Flask web application that increments a counter each time the / route is accessed and displays the number of times it has been accessed. The application uses Redis as a datastore to store the hit counter. Here is how our app.py script works: The Flask library is imported, which allows us to create a web application. The Redis library is imported, which allows us to connect to a Redis instance and manipulate data. The Flask application is created and the Redis client is initialized, connecting to the Redis container named \"redis-container\" at port 6379. A route for the / endpoint is defined. When this route is accessed, the hit counter in Redis is incremented and the current count is displayed on the page. Finally, the application is run, listening on all network interfaces (0.0.0.0) on port 5000 and with debugging enabled. Then, we must write a Dockerfile : FROM python:3.6 WORKDIR /app COPY . . RUN pip install -r requirements.txt CMD python app.py Like before this is a simple Dockerfile for a python application.","title":"Docker Compose for a simple Python App and Redis database"},{"location":"docker/docker_compose/#write-the-docker-composeyml-of-our-app","text":"version : '3' services : web : build : ./app ports : - \"5000:5000\" volumes : - ./app:/app depends_on : - redis-container redis-container : image : redis This script is a Docker Compose file that describes two services that will be run in Docker containers: a web service and a Redis service. The web service is defined by the web service block. It specifies that the web service should be built from the Dockerfile in the ./app directory, and should expose port 5000 on the host machine. The volumes directive maps the ./app directory on the host to the /app directory in the container, allowing changes to the code to be immediately reflected in the container. The depends_on directive specifies that the web service should not start until the Redis service is running. The Redis service is defined by the redis-container block. It specifies that the Redis image should be used to create the service. This is the architeture of our project : . |_docker-compose.yml |_app |_Dockerfile |_requirements.txt |_app.py Together, these services can be started with the docker-compose up command, which will build and start the web and Redis containers, and connect them together on a default Docker network. You can also run your project in background with -d option then you should see your containers up and running with the command docker ps","title":"Write the docker-compose.yml of our app"},{"location":"docker/docker_compose/#docker-compose-for-a-python-app-and-postgresql","text":"Now that we understand how two containers works together let's code an application with a more efficient database : postgreSQL.","title":"Docker Compose for a Python App and PostgreSQL"},{"location":"docker/docker_compose/#what-is-postgresql","text":"PostgreSQL, also known as Postgres, is a powerful and open-source relational database management system. It uses and extends the SQL language and provides many features such as support for JSON and other NoSQL features, scalability, and extensibility. It can run on various platforms such as Windows, macOS, Linux, and Unix. Many organizations use Postgres for their data storage needs due to its reliability, robustness, and community support.","title":"What is PostgreSQL"},{"location":"docker/docker_compose/#set-up-the-project","text":"To create a Docker Compose file for your Python app, you'll need to define the services that make up your application. Each service is defined in the Docker Compose file as a separate block of configuration. In this example we will take the nortwhind database here as base for our database service. Download or git clone the nortwhind database here and open the docker-compose.yml file bellow who define two services, one for a monitoring application pgadmin and one for a PostgreSQL database db : version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db networks : db : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local Let's break down this Docker Compose file: version: '3' : This specifies the version of the Docker Compose file format that we're using. services : This is where we define the services that make up our application. db : The Postgres database service. container_name : Sets the name of the container to db. image : postgres:latest: Specifies the image to use for the container. environment : Sets environment variables for the container. POSTGRES_DB : northwind: Specifies the name of the database to create. POSTGRES_USER : postgres: Specifies the username for the database. POSTGRES_PASSWORD : postgres: Specifies the password for the database. volumes : Mounts volumes for the container. postgresql_bin:/usr/lib/postgresql : Mounts the PostgreSQL binaries. postgresql_data:/var/lib/postgresql/data : Mounts the PostgreSQL data directory. ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql : Copies the northwind.sql script into the container for initializing the database. ./files:/files : Mounts the files directory into the container. ports : Maps ports between the container and the host. 55432:5432 : Maps port 5432 inside the container to port 55432 on the host. networks : Specifies the networks to connect the container to. db : Connects the container to the db network. pgadmin : The pgAdmin web interface service in order to visualize our database image : dpage/pgadmin4 : Specifies the image to use for the container. environment : Sets environment variables for the container. PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org: Specifies the default email for pgAdmin. PGADMIN_DEFAULT_PASSWORD : postgres: Specifies the default password for pgAdmin. PGADMIN_LISTEN_PORT: 5050 : Specifies the port for pgAdmin to listen on. PGADMIN_CONFIG_SERVER_MODE : 'False': Disables server mode for pgAdmin. volumes : Mounts volumes for the container. ports : Maps ports between the container and the host. 5050:5050 : Maps port 5050 inside the container to port 5050 on the host. networks : Specifies the networks to create. db : Creates the db network. driver: bridge : Specifies the driver to use for the network, this is the standard driver \ud83e\udd13 volumes : Specifies the volumes to create, see volume part in the table of content for more detailed Once you've defined your Docker Compose file, you can use the docker-compose up command to start and stop your application. Here are some of the most common commands: docker-compose up : This command starts your application and attaches your terminal to the logs of all running containers. You can use Ctrl+C to stop the containers and exit. docker-compose up -d : This command starts your application in detached mode, which means that it runs in the background. You can use docker-compose logs to view the logs of your containers. docker-compose down : This command stops and removes all containers, networks, and volumes that were created by docker-compose up. docker-compose ps : This command lists all running containers in your Docker Compose application. docker-compose build : This command builds the images for all of the services in your Docker Compose file. By using Docker Compose, you can easily start and stop multiple containers with a single command, and manage the configuration of all of your services in a single file. This makes it easier to manage and deploy complex applications that are made up of multiple services.","title":"Set up the project"},{"location":"docker/docker_compose/#pgadmin-interface","text":"First let's confirm our containers are up and running by taping docker ps command. If you see the container running like : a76abdcbf8da dpage/pgadmin4 \"/entrypoint.sh\" About an hour ago Up About an hour 80 /tcp, 443 /tcp, 0 .0.0.0:5050->5050/tcp, :::5050->5050/tcp To see our database go to : (localhost:5050)[http://localhost:5050] and write a random password (like root) then register our database by running the following command : Add a new server in PgAdmin In the general Tab, write the paramater Name = db In the Connection Tab write the following parameters : Host name: db Username: postgres Password: postgres Then, select database \"northwind\" and you can now see all the tables and metadata \ud83e\udd73","title":"PgAdmin interface"},{"location":"docker/docker_compose/#add-a-python-app","text":"from fastapi import FastAPI from sqlalchemy import create_engine , text from sqlalchemy.orm import sessionmaker app = FastAPI () engine = create_engine ( 'postgresql://postgres:postgres@db/northwind' ) Session = sessionmaker ( bind = engine ) @app . get ( '/' ) def read_root (): session = Session () result = session . execute ( text ( 'SELECT customer_id, company_name, contact_name FROM customers LIMIT 10' )) return { 'Customers info' : [ dict ( customerid = row [ 0 ], companyname = row [ 1 ], contactname = row [ 2 ]) for row in result ]} This application uses the FastAPI framework to define a simple endpoint that returns a JSON response with a greeting and a value from a PostgreSQL database. To run this application using Docker Compose, you'll need to save this file as main.py in the same directory as your Dockerfile, and update your Docker Compose file to include the following environment variable for the app service and dependence : environment : DB_HOST : db depends_on : - db This environment variable tells the application where to find the PostgreSQL database and tell the application to wait for the lunch of the db service.","title":"Add a Python app"},{"location":"docker/docker_compose/#integrate-our-app-to-the-docker-composeyml-file","text":"version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db app : build : context : . dockerfile : Dockerfile environment : DB_HOST : db depends_on : - db ports : - \"8000:8000\" networks : - db networks : db : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local The app service is a container that will host a Python application that uses the northwind database created by the db service. The app container will be built using the Dockerfile located in the same directory as the docker-compose.yml file. The depends_on property indicates that the app container must be started after the db container is running. The environment property sets the DB_HOST environment variable, which is used in the application to connect to the db container. The ports property maps port 8000 of the app container to port 8000 of the host machine, allowing access to the FastAPI application from a web browser. The networks property specifies that the app container is connected to the db network, allowing communication between the application and the database. Once you've made these changes, you can start your application using Docker Compose with the following command: docker-compose up --build This will start both the db and app services and rebuild it just in case, and you should be able to access the application by visiting http://localhost:8000 in your web browser.","title":"Integrate our app to the docker-compose.yml file"},{"location":"docker/docker_compose/#stop-docker-compose","text":"Stop the server that was launched by docker compose up via Ctrl-C if you are in interactive mode, then remove the containers via: docker-compose down or just go to the root of your repository and run docker-compose down","title":"Stop docker-compose"},{"location":"docker/docker_https/","text":"Docker and HTTPS What is HTTPS HTTPS stands for Hypertext Transfer Protocol Secure, which is an extension of the HTTP protocol used for secure communication over the internet. It is a way of encrypting the data that is transmitted between a web browser and a web server, making it more difficult for attackers to intercept and steal sensitive information, such as login credentials or credit card numbers. To enable HTTPS on a website, you need to obtain an SSL (Secure Sockets Layer) certificate. An SSL certificate is a digital certificate that verifies the identity of a website and encrypts the data transmitted between the web server and the client's browser. SSL certificates are issued by trusted certificate authorities (CA), such as Let's Encrypt, Comodo, and Symantec. There are several ways to get an SSL certificate, depending on your needs and budget. Here are some options: Let's Encrypt : Let's Encrypt is a free and open certificate authority that provides SSL certificates for websites. It is widely used and trusted, and can be easily integrated with many web servers, including Apache and Nginx. Paid SSL certificates : There are many companies that offer paid SSL certificates, including Comodo, Symantec, and DigiCert. These certificates usually provide a higher level of validation and come with more advanced features, such as extended validation and wildcard certificates. Cloud hosting providers : Many cloud hosting providers, such as AWS, Google Cloud, and Azure, offer SSL certificates as part of their hosting packages. These certificates are often managed by the hosting provider, making it easier to install and renew them. To obtain an SSL certificate, you typically need to generate a certificate signing request (CSR) on your web server, which contains information about your website and your public key. You then submit the CSR to a certificate authority, which will verify your identity and issue a certificate. Once you receive the certificate, you need to install it on your web server and configure your server to use HTTPS. Keep in mind that SSL certificates have expiration dates and need to be renewed periodically, usually every one or two years. It's also important to ensure that your web server and applications are configured correctly to use HTTPS, and to keep your server and software up to date to address any security vulnerabilities. Add HTTPS to our Python/Postgres app Let's take our project from the previous section and add an Nginx service \ud83e\udd13 Nginx Nginx is a popular open-source web server that can also function as a reverse proxy, load balancer, and HTTP cache. It is known for its high performance, stability, and ability to handle a large number of simultaneous connections. Developers should be familiar with Nginx because it is commonly used as a frontend web server in production environments. In addition to its performance benefits, Nginx is also highly customizable and can be used to handle complex routing, authentication, and security configurations. Nginx also integrates well with many popular web frameworks and technologies, making it a valuable tool for developers who are building web applications. By leveraging Nginx's features, developers can improve the performance, scalability, and security of their applications. Generate a free ssl certificate First go to the root of your project and create a new directory call certs and then run this command : openssl req -x509 -newkey rsa:4096 -keyout certs/key.pem -out certs/cert.pem -days 365 -nodes This will create a certs folder and generate a self-signed SSL certificate with a private key ( key.pem ) and a public certificate ( cert.pem ) that are valid for 365 days. Then add execution right to the private key in order to be executed by our nginx service inside our Dockerfile with the following command : chmod +x certs/key.pem Create an nginx.conf file Go to the root of your project and create a new file called nginx.conf events {} http { upstream app { server app:8000 ; } server { listen 80 ; listen [ :: ]: 80 ; server_name yourdomain . com ; return 301 https: // $host$request_uri ; } server { listen 443 ssl http2 ; listen [ :: ]: 443 ssl http2 ; server_name yourdomain . com ; ssl_certificate /etc/ss l /certs/c ert . pem ; ssl_certificate_key /etc/ss l /certs/ key . pem ; location / { proxy_pass http: // app ; proxy_set_header Host $host ; proxy_set_header X - Real - IP $remote_addr ; proxy_set_header X - Forwarded - Proto https ; proxy_set_header X - Forwarded - For $proxy_add_x_forwarded_for ; } } } We will not dig in detail this script in this course section just make sure to replace yourdomain.com with your actual domain name, and update the SSL certificate file paths to match your file names and folder locations. Refactor our docker-compose.yml file version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db app : build : context : . dockerfile : Dockerfile environment : DB_HOST : db depends_on : - db ports : - \"8000:8000\" networks : - db - mynetwork nginx : image : nginx:latest ports : - \"80:80\" - \"443:443\" volumes : - ./nginx.conf:/etc/nginx/nginx.conf - ./certs:/etc/ssl/certs networks : - mynetwork depends_on : - app restart : always networks : db : driver : bridge mynetwork : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local Let's talk about the ppart of the docker-compose file defines a service named nginx that is based on the nginx:latest Docker image. Here is a detailed explanation of each section: image: nginx:latest : This line specifies the Docker image to use for the nginx service, which is nginx:latest . ports : This section maps the container ports to the host ports. It exposes the container ports 80 and 443 to the host machine. volumes : This section maps the host directories or files to the container directories or files. Here, it mounts the nginx.conf file from the current directory into the container's /etc/nginx/nginx.conf path. It also mounts the certs directory from the current directory into the container's /etc/ssl/certs path. networks : This section specifies the networks to which this service is attached. In this case, it is attached to the mynetwork network. depends_on : This section specifies that the nginx service depends on the app service to start. This means that the app service will be started before the nginx service. restart : This line specifies that the container should always be restarted if it stops for any reason. The networks section defines two networks: db and mynetwork . The networks are bridge driver type Then you can run docker-compose up -d and you should see the nginx forwarding our app and all our container like this : fe86842a56e1 nginx:latest \"/docker-entrypoint.\u2026\" 19 seconds ago Up 18 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp d05f1ca65c2f northwind_psql_app \"uvicorn app:app --h\u2026\" 19 seconds ago Up 18 seconds 80/tcp, 0.0.0.0:8000->8000/tcp, :::8000->8000/tcp northwind_psql_app_1 51553cc3b247 postgres:latest \"docker-entrypoint.s\u2026\" 9 hours ago Up 19 seconds 0.0.0.0:55432->5432/tcp, :::55432->5432/tcp db a76abdcbf8da dpage/pgadmin4 \"/entrypoint.sh\" 9 hours ago Up 19 seconds 80/tcp, 443/tcp, 0.0.0.0:5050->5050/tcp, :::5050->5050/tcp pgadmin Congrats our app service is now running in HTTPS \ud83e\udd73","title":"Docker and HTTPS"},{"location":"docker/docker_https/#docker-and-https","text":"","title":"Docker and HTTPS"},{"location":"docker/docker_https/#what-is-https","text":"HTTPS stands for Hypertext Transfer Protocol Secure, which is an extension of the HTTP protocol used for secure communication over the internet. It is a way of encrypting the data that is transmitted between a web browser and a web server, making it more difficult for attackers to intercept and steal sensitive information, such as login credentials or credit card numbers. To enable HTTPS on a website, you need to obtain an SSL (Secure Sockets Layer) certificate. An SSL certificate is a digital certificate that verifies the identity of a website and encrypts the data transmitted between the web server and the client's browser. SSL certificates are issued by trusted certificate authorities (CA), such as Let's Encrypt, Comodo, and Symantec. There are several ways to get an SSL certificate, depending on your needs and budget. Here are some options: Let's Encrypt : Let's Encrypt is a free and open certificate authority that provides SSL certificates for websites. It is widely used and trusted, and can be easily integrated with many web servers, including Apache and Nginx. Paid SSL certificates : There are many companies that offer paid SSL certificates, including Comodo, Symantec, and DigiCert. These certificates usually provide a higher level of validation and come with more advanced features, such as extended validation and wildcard certificates. Cloud hosting providers : Many cloud hosting providers, such as AWS, Google Cloud, and Azure, offer SSL certificates as part of their hosting packages. These certificates are often managed by the hosting provider, making it easier to install and renew them. To obtain an SSL certificate, you typically need to generate a certificate signing request (CSR) on your web server, which contains information about your website and your public key. You then submit the CSR to a certificate authority, which will verify your identity and issue a certificate. Once you receive the certificate, you need to install it on your web server and configure your server to use HTTPS. Keep in mind that SSL certificates have expiration dates and need to be renewed periodically, usually every one or two years. It's also important to ensure that your web server and applications are configured correctly to use HTTPS, and to keep your server and software up to date to address any security vulnerabilities.","title":"What is HTTPS"},{"location":"docker/docker_https/#add-https-to-our-pythonpostgres-app","text":"Let's take our project from the previous section and add an Nginx service \ud83e\udd13","title":"Add HTTPS to our Python/Postgres app"},{"location":"docker/docker_https/#nginx","text":"Nginx is a popular open-source web server that can also function as a reverse proxy, load balancer, and HTTP cache. It is known for its high performance, stability, and ability to handle a large number of simultaneous connections. Developers should be familiar with Nginx because it is commonly used as a frontend web server in production environments. In addition to its performance benefits, Nginx is also highly customizable and can be used to handle complex routing, authentication, and security configurations. Nginx also integrates well with many popular web frameworks and technologies, making it a valuable tool for developers who are building web applications. By leveraging Nginx's features, developers can improve the performance, scalability, and security of their applications.","title":"Nginx"},{"location":"docker/docker_https/#generate-a-free-ssl-certificate","text":"First go to the root of your project and create a new directory call certs and then run this command : openssl req -x509 -newkey rsa:4096 -keyout certs/key.pem -out certs/cert.pem -days 365 -nodes This will create a certs folder and generate a self-signed SSL certificate with a private key ( key.pem ) and a public certificate ( cert.pem ) that are valid for 365 days. Then add execution right to the private key in order to be executed by our nginx service inside our Dockerfile with the following command : chmod +x certs/key.pem","title":"Generate a free ssl certificate"},{"location":"docker/docker_https/#create-an-nginxconf-file","text":"Go to the root of your project and create a new file called nginx.conf events {} http { upstream app { server app:8000 ; } server { listen 80 ; listen [ :: ]: 80 ; server_name yourdomain . com ; return 301 https: // $host$request_uri ; } server { listen 443 ssl http2 ; listen [ :: ]: 443 ssl http2 ; server_name yourdomain . com ; ssl_certificate /etc/ss l /certs/c ert . pem ; ssl_certificate_key /etc/ss l /certs/ key . pem ; location / { proxy_pass http: // app ; proxy_set_header Host $host ; proxy_set_header X - Real - IP $remote_addr ; proxy_set_header X - Forwarded - Proto https ; proxy_set_header X - Forwarded - For $proxy_add_x_forwarded_for ; } } } We will not dig in detail this script in this course section just make sure to replace yourdomain.com with your actual domain name, and update the SSL certificate file paths to match your file names and folder locations.","title":"Create an nginx.conf file"},{"location":"docker/docker_https/#refactor-our-docker-composeyml-file","text":"version : '3' services : db : container_name : db image : postgres:latest environment : POSTGRES_DB : northwind POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres volumes : - postgresql_bin:/usr/lib/postgresql - postgresql_data:/var/lib/postgresql/data - ./northwind.sql:/docker-entrypoint-initdb.d/northwind.sql - ./files:/files ports : - 55432:5432 networks : - db pgadmin : container_name : pgadmin image : dpage/pgadmin4 environment : PGADMIN_DEFAULT_EMAIL : pgadmin4@pgadmin.org PGADMIN_DEFAULT_PASSWORD : postgres PGADMIN_LISTEN_PORT : 5050 PGADMIN_CONFIG_SERVER_MODE : 'False' volumes : - postgresql_bin:/usr/lib/postgresql - pgadmin_root_prefs:/root/.pgadmin - pgadmin_working_dir:/var/lib/pgadmin - ./files:/files ports : - 5050:5050 networks : - db app : build : context : . dockerfile : Dockerfile environment : DB_HOST : db depends_on : - db ports : - \"8000:8000\" networks : - db - mynetwork nginx : image : nginx:latest ports : - \"80:80\" - \"443:443\" volumes : - ./nginx.conf:/etc/nginx/nginx.conf - ./certs:/etc/ssl/certs networks : - mynetwork depends_on : - app restart : always networks : db : driver : bridge mynetwork : driver : bridge volumes : pgadmin_root_prefs : driver : local pgadmin_working_dir : driver : local postgresql_data : driver : local postgresql_bin : driver : local Let's talk about the ppart of the docker-compose file defines a service named nginx that is based on the nginx:latest Docker image. Here is a detailed explanation of each section: image: nginx:latest : This line specifies the Docker image to use for the nginx service, which is nginx:latest . ports : This section maps the container ports to the host ports. It exposes the container ports 80 and 443 to the host machine. volumes : This section maps the host directories or files to the container directories or files. Here, it mounts the nginx.conf file from the current directory into the container's /etc/nginx/nginx.conf path. It also mounts the certs directory from the current directory into the container's /etc/ssl/certs path. networks : This section specifies the networks to which this service is attached. In this case, it is attached to the mynetwork network. depends_on : This section specifies that the nginx service depends on the app service to start. This means that the app service will be started before the nginx service. restart : This line specifies that the container should always be restarted if it stops for any reason. The networks section defines two networks: db and mynetwork . The networks are bridge driver type Then you can run docker-compose up -d and you should see the nginx forwarding our app and all our container like this : fe86842a56e1 nginx:latest \"/docker-entrypoint.\u2026\" 19 seconds ago Up 18 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp d05f1ca65c2f northwind_psql_app \"uvicorn app:app --h\u2026\" 19 seconds ago Up 18 seconds 80/tcp, 0.0.0.0:8000->8000/tcp, :::8000->8000/tcp northwind_psql_app_1 51553cc3b247 postgres:latest \"docker-entrypoint.s\u2026\" 9 hours ago Up 19 seconds 0.0.0.0:55432->5432/tcp, :::55432->5432/tcp db a76abdcbf8da dpage/pgadmin4 \"/entrypoint.sh\" 9 hours ago Up 19 seconds 80/tcp, 443/tcp, 0.0.0.0:5050->5050/tcp, :::5050->5050/tcp pgadmin Congrats our app service is now running in HTTPS \ud83e\udd73","title":"Refactor our docker-compose.yml file"},{"location":"docker/docker_install/","text":"Setting up Docker The process of installing Docker on your local machine will depend on the operating system you are using. Docker provides installation packages for Windows, macOS, and Linux. Installing Docker on MacOS Go to the official docker website and download the appropriate installation package for your operating system. Be aware of your chipset for example if your Apple machine is new install the version apple chip not intel (you can see that chip information if you go on the little \uf8ff on the top left of your screen > about my Mac ) Follow the installation instructions Once Docker is successfully installed, open a terminal or command prompt and run the following command to verify that Docker is running: docker version If Docker is running correctly, you should see information about the Docker version, API version, and other details like this : Client: Cloud integration: 1 .0.14 Version: 20 .10.6 API version: 1 .41 Go version: go1.16.3 Git commit: 370c289 Built: Fri Apr 9 22 :46:57 2021 OS/Arch: darwin/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20 .10.6 API version: 1 .41 ( minimum version 1 .12 ) Go version: go1.13.15 Git commit: 8728dd2 Built: Fri Apr 9 22 :44:56 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1 .4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1 .0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0 .19.0 GitCommit: de40ad0 Technical informations about docker version Just in case if you wondering what the output means \ud83e\udd13 Client : This section shows the version and details of the Docker client that is running on your local machine. The client is responsible for issuing commands to the Docker daemon, which manages containers and images. Cloud integration: This shows the version of the Docker Cloud integration plugin that is installed on your machine. Docker Cloud is a service that provides tools for managing Docker containers in the cloud. Version : This shows the version of the Docker client that is installed on your machine. API version : This shows the version of the Docker API that is supported by the client. Go version: This shows the version of the Go programming language that was used to compile the Docker client. Git commit: This shows the Git commit hash that was used to build the Docker client. Built: This shows the date and time that the Docker client was built. OS/Arch: This shows the operating system and processor architecture that the Docker client is running on. Context: This shows the default Docker context that is currently in use. Experimental: This shows whether experimental features are enabled on the Docker client. Server : This section shows the version and details of the Docker daemon that is running on your local machine. The daemon is responsible for managing containers and images. Engine : This shows the version of the Docker engine that is running on your machine. API version : This shows the version of the Docker API that is supported by the engine. Go version: This shows the version of the Go programming language that was used to compile the Docker engine. Git commit: This shows the Git commit hash that was used to build the Docker engine. Built: This shows the date and time that the Docker engine was built. OS/Arch : This shows the operating system and processor architecture that the Docker engine is running on. Experimental: This shows whether experimental features are enabled on the Docker engine. containerd: This shows the version and Git commit of containerd, which is the container runtime used by Docker. runc: This shows the version and Git commit of runc, which is the command-line tool used to run containers. docker-init: This shows the version and Git commit of docker-init, which is the initialization script used by Docker to start containers. How Docker works on our machine As you can see the output of the docker version command is divided into two sections: Client and Server. In the context of Docker, the Docker client and server are two separate components that work together to manage containers and images. At the top, we have the client component, which runs on the host machine and interacts with the user. The client sends requests to the server component, which is hosted inside a Docker container. The Docker container is built from an image, which contains the application code and its dependencies. The image is created using a Dockerfile, which specifies the steps to build the image. The image is then pushed to a Docker registry, where it can be accessed by other team members or deployed to production. The Docker container runs on a Docker host, which is a machine that has Docker installed. The Docker host abstracts the underlying hardware and provides a consistent interface for running Docker containers. The Docker container is isolated from other containers and the host machine, which provides a secure and predictable environment for running the application. In this architecture, we can easily scale the server component by creating more Docker containers from the same image. We can also deploy the application to different environments, such as development, staging, and production, by using different Docker images and configurations. Docker client The Docker client is a command-line interface (CLI) tool that allows you to interact with the Docker daemon . The client sends commands to the daemon, which then executes those commands and manages the containers and images on your system. The Docker client can be used to build and run containers, manage images, and perform other Docker-related tasks. Docker Daemon The Docker server, also known as the Docker daemon, is a background process that manages the containers and images on your system . The daemon listens for commands from the Docker client, executes those commands, and manages the underlying infrastructure needed to run containers, such as networking and storage. The Docker client and server communicate with each other using the Docker API, which is a RESTful API that provides a standardized way to interact with Docker. When you run a command using the Docker client, such as \"docker run\", the client sends a request to the Docker daemon over the Docker API. The daemon then processes the request and executes the command. In summary, the Docker client is a tool for interacting with the Docker daemon, while the Docker daemon is a background process that manages the containers and images on your system. The client and daemon communicate with each other using the Docker API.","title":"Setting up Docker"},{"location":"docker/docker_install/#setting-up-docker","text":"The process of installing Docker on your local machine will depend on the operating system you are using. Docker provides installation packages for Windows, macOS, and Linux.","title":"Setting up Docker"},{"location":"docker/docker_install/#installing-docker-on-macos","text":"Go to the official docker website and download the appropriate installation package for your operating system. Be aware of your chipset for example if your Apple machine is new install the version apple chip not intel (you can see that chip information if you go on the little \uf8ff on the top left of your screen > about my Mac ) Follow the installation instructions Once Docker is successfully installed, open a terminal or command prompt and run the following command to verify that Docker is running: docker version If Docker is running correctly, you should see information about the Docker version, API version, and other details like this : Client: Cloud integration: 1 .0.14 Version: 20 .10.6 API version: 1 .41 Go version: go1.16.3 Git commit: 370c289 Built: Fri Apr 9 22 :46:57 2021 OS/Arch: darwin/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20 .10.6 API version: 1 .41 ( minimum version 1 .12 ) Go version: go1.13.15 Git commit: 8728dd2 Built: Fri Apr 9 22 :44:56 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1 .4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1 .0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0 .19.0 GitCommit: de40ad0","title":"Installing Docker on MacOS"},{"location":"docker/docker_install/#technical-informations-about-docker-version","text":"Just in case if you wondering what the output means \ud83e\udd13 Client : This section shows the version and details of the Docker client that is running on your local machine. The client is responsible for issuing commands to the Docker daemon, which manages containers and images. Cloud integration: This shows the version of the Docker Cloud integration plugin that is installed on your machine. Docker Cloud is a service that provides tools for managing Docker containers in the cloud. Version : This shows the version of the Docker client that is installed on your machine. API version : This shows the version of the Docker API that is supported by the client. Go version: This shows the version of the Go programming language that was used to compile the Docker client. Git commit: This shows the Git commit hash that was used to build the Docker client. Built: This shows the date and time that the Docker client was built. OS/Arch: This shows the operating system and processor architecture that the Docker client is running on. Context: This shows the default Docker context that is currently in use. Experimental: This shows whether experimental features are enabled on the Docker client. Server : This section shows the version and details of the Docker daemon that is running on your local machine. The daemon is responsible for managing containers and images. Engine : This shows the version of the Docker engine that is running on your machine. API version : This shows the version of the Docker API that is supported by the engine. Go version: This shows the version of the Go programming language that was used to compile the Docker engine. Git commit: This shows the Git commit hash that was used to build the Docker engine. Built: This shows the date and time that the Docker engine was built. OS/Arch : This shows the operating system and processor architecture that the Docker engine is running on. Experimental: This shows whether experimental features are enabled on the Docker engine. containerd: This shows the version and Git commit of containerd, which is the container runtime used by Docker. runc: This shows the version and Git commit of runc, which is the command-line tool used to run containers. docker-init: This shows the version and Git commit of docker-init, which is the initialization script used by Docker to start containers.","title":"Technical informations about docker version"},{"location":"docker/docker_install/#how-docker-works-on-our-machine","text":"As you can see the output of the docker version command is divided into two sections: Client and Server. In the context of Docker, the Docker client and server are two separate components that work together to manage containers and images. At the top, we have the client component, which runs on the host machine and interacts with the user. The client sends requests to the server component, which is hosted inside a Docker container. The Docker container is built from an image, which contains the application code and its dependencies. The image is created using a Dockerfile, which specifies the steps to build the image. The image is then pushed to a Docker registry, where it can be accessed by other team members or deployed to production. The Docker container runs on a Docker host, which is a machine that has Docker installed. The Docker host abstracts the underlying hardware and provides a consistent interface for running Docker containers. The Docker container is isolated from other containers and the host machine, which provides a secure and predictable environment for running the application. In this architecture, we can easily scale the server component by creating more Docker containers from the same image. We can also deploy the application to different environments, such as development, staging, and production, by using different Docker images and configurations.","title":"How Docker works on our machine"},{"location":"docker/docker_install/#docker-client","text":"The Docker client is a command-line interface (CLI) tool that allows you to interact with the Docker daemon . The client sends commands to the daemon, which then executes those commands and manages the containers and images on your system. The Docker client can be used to build and run containers, manage images, and perform other Docker-related tasks.","title":"Docker client"},{"location":"docker/docker_install/#docker-daemon","text":"The Docker server, also known as the Docker daemon, is a background process that manages the containers and images on your system . The daemon listens for commands from the Docker client, executes those commands, and manages the underlying infrastructure needed to run containers, such as networking and storage. The Docker client and server communicate with each other using the Docker API, which is a RESTful API that provides a standardized way to interact with Docker. When you run a command using the Docker client, such as \"docker run\", the client sends a request to the Docker daemon over the Docker API. The daemon then processes the request and executes the command. In summary, the Docker client is a tool for interacting with the Docker daemon, while the Docker daemon is a background process that manages the containers and images on your system. The client and daemon communicate with each other using the Docker API.","title":"Docker Daemon"},{"location":"docker/docker_intro/","text":"Introduction to Docker Docker is an open-source platform for building, shipping, and running applications as containers. Docker solves the problem of \"it works on my machine but not in production\" by providing a consistent environment for running applications. Traditionally, developers would build applications on their local machines, which could have different operating systems, libraries, and dependencies than the production environment. This could lead to compatibility issues and errors when the application was deployed to production. With Docker, developers can package their applications and dependencies into a container, which provides a consistent environment for running the application. This means that the container can be run on any machine that has Docker installed, without worrying about differences in the underlying operating system or environment. By using Docker, developers can ensure that their applications will work the same way in development, testing, and production environments. This can help to reduce the risk of compatibility issues and errors when deploying applications to production. Docker also makes it easier to manage and scale applications. Containers can be quickly and easily deployed, scaled up or down, and updated, which helps to reduce the time and effort required to manage applications. This can help to improve the reliability and performance of applications, while also reducing costs and complexity. In summary, Docker provides a consistent and reliable way to package, deploy, and manage applications, which helps to solve the problem of \"it works on my machine but not in production\". By using Docker, developers can ensure that their applications will work the same way in all environments, which can help to improve the reliability and efficiency of application deployment. What is Docker and Why Use It? Docker provides a number of benefits over traditional methods of deploying applications: Consistency : Docker provides a consistent environment for running your application, regardless of where it is being run. This means that you can build your application once, and then run it in any environment that has Docker installed, without worrying about differences in operating systems, libraries, or dependencies. Portability : Docker containers are lightweight and portable, which means that you can easily move them between machines, or even between different cloud providers. This makes it easy to deploy your application to different environments, such as development, staging, and production. Isolation : Docker containers provide a high degree of isolation between different applications, which reduces the risk of conflicts between different components of your application stack. This means that you can run multiple applications on the same machine, without worrying about interference between them. Resource Efficiency : Docker containers are lightweight and use fewer resources than traditional virtual machines, which means that you can run more containers on the same machine. This can result in significant cost savings for cloud-based applications. Docker VS Virtual Machines Let's talk about the difference between a traditional virtual machine (VM) architecture and a containerized architecture for that let's take a look at this figure : Starting with the VM architecture on the left, you can see that there is a physical server that hosts a hypervisor layer. The hypervisor layer creates multiple virtual machines, each of which has its own operating system (OS) and runs on top of the hypervisor layer. Each VM also has its own set of resources, such as CPU, memory, and storage, which are isolated from the other VMs. In contrast, the containerized architecture on the right does not have a hypervisor layer. Instead, it has a host operating system that runs on top of the physical server. On top of the host operating system, there is a container runtime, which manages the creation and management of containers. Each container shares the host operating system with other containers, but each container has its own isolated file system, network, and process space. Docker is a containerization technology that allows you to create, deploy, and manage containers. It provides a way to package and distribute software applications in a standardized and portable format, making it easy to move them from one environment to another. With Docker, you can create a Dockerfile that describes the dependencies and configuration of your application, and then use the Docker command-line interface to build, run, and manage containers based on that Dockerfile. Overall, Docker provides a lightweight and flexible alternative to traditional VMs, making it easier to develop, deploy, and scale applications. What is a Containers Docker's use of the term \"container\" is inspired by the shipping industry. In the shipping industry, containers are standardized, self-contained units that can be easily transported between ships, trains, and trucks. These containers can hold a variety of goods and products, and they are designed to be easy to load and unload from transport vehicles. Similarly, in the context of software development, a container is a standardized, self-contained unit that can hold an application along with its dependencies and configurations. Like a shipping container, a software container can be easily transported between different environments, such as development, testing, and production. By using the term \"container\", Docker is emphasizing the portability and standardization of its technology, which is similar to the shipping industry's use of containers to transport goods and products between different locations. Advantages of Containerization Containerization provides several advantages over traditional deployment methods: Portability : Containers are self-contained units of software that can be easily moved between different environments. Scalability : Containers can be quickly and easily scaled up or down, depending on demand. Consistency : Containers provide a consistent environment for running applications, which makes it easier to manage and troubleshoot applications. Resource Efficiency : Containers use fewer resources than traditional virtual machines, which means that you can run more containers on the same hardware. Security : Containers provide a high degree of isolation between different applications, which helps to reduce the risk of security breaches. In summary, Docker provides a flexible and efficient way to package and deploy applications as containers. By using Docker, you can create consistent, portable, and scalable environments for running your applications, which can help to reduce costs and improve reliability. 5 reasons why developers should consider using Docker Consistent Development Environments : With Docker, developers can create a consistent environment for developing and testing applications. Docker allows developers to package an application along with all its dependencies, libraries, and configurations into a container. This ensures that the application will run the same way on any machine, regardless of the underlying operating system or environment. Easy Collaboration : Docker containers can be easily shared between developers, which makes it easier to collaborate on projects. Containers can be used to create a development environment that is identical across all team members, which helps to reduce the risk of compatibility issues and errors. Faster Application Development and Deployment : Docker makes it easier to develop and deploy applications by automating the process of packaging and deploying applications. Developers can quickly create and test new versions of an application in a container, and then deploy it to production with minimal effort. Improved Testing : Docker makes it easier to test applications by allowing developers to create multiple containers with different configurations and environments. This makes it easier to test applications in different scenarios, such as different operating systems, libraries, or dependencies. Resource Efficiency : Docker containers are lightweight and use fewer resources than traditional virtual machines, which means that developers can run more containers on the same machine. This can result in significant cost savings for cloud-based applications.","title":"Introduction"},{"location":"docker/docker_intro/#introduction-to-docker","text":"Docker is an open-source platform for building, shipping, and running applications as containers. Docker solves the problem of \"it works on my machine but not in production\" by providing a consistent environment for running applications. Traditionally, developers would build applications on their local machines, which could have different operating systems, libraries, and dependencies than the production environment. This could lead to compatibility issues and errors when the application was deployed to production. With Docker, developers can package their applications and dependencies into a container, which provides a consistent environment for running the application. This means that the container can be run on any machine that has Docker installed, without worrying about differences in the underlying operating system or environment. By using Docker, developers can ensure that their applications will work the same way in development, testing, and production environments. This can help to reduce the risk of compatibility issues and errors when deploying applications to production. Docker also makes it easier to manage and scale applications. Containers can be quickly and easily deployed, scaled up or down, and updated, which helps to reduce the time and effort required to manage applications. This can help to improve the reliability and performance of applications, while also reducing costs and complexity. In summary, Docker provides a consistent and reliable way to package, deploy, and manage applications, which helps to solve the problem of \"it works on my machine but not in production\". By using Docker, developers can ensure that their applications will work the same way in all environments, which can help to improve the reliability and efficiency of application deployment.","title":"Introduction to Docker"},{"location":"docker/docker_intro/#what-is-docker-and-why-use-it","text":"Docker provides a number of benefits over traditional methods of deploying applications: Consistency : Docker provides a consistent environment for running your application, regardless of where it is being run. This means that you can build your application once, and then run it in any environment that has Docker installed, without worrying about differences in operating systems, libraries, or dependencies. Portability : Docker containers are lightweight and portable, which means that you can easily move them between machines, or even between different cloud providers. This makes it easy to deploy your application to different environments, such as development, staging, and production. Isolation : Docker containers provide a high degree of isolation between different applications, which reduces the risk of conflicts between different components of your application stack. This means that you can run multiple applications on the same machine, without worrying about interference between them. Resource Efficiency : Docker containers are lightweight and use fewer resources than traditional virtual machines, which means that you can run more containers on the same machine. This can result in significant cost savings for cloud-based applications.","title":"What is Docker and Why Use It?"},{"location":"docker/docker_intro/#docker-vs-virtual-machines","text":"Let's talk about the difference between a traditional virtual machine (VM) architecture and a containerized architecture for that let's take a look at this figure : Starting with the VM architecture on the left, you can see that there is a physical server that hosts a hypervisor layer. The hypervisor layer creates multiple virtual machines, each of which has its own operating system (OS) and runs on top of the hypervisor layer. Each VM also has its own set of resources, such as CPU, memory, and storage, which are isolated from the other VMs. In contrast, the containerized architecture on the right does not have a hypervisor layer. Instead, it has a host operating system that runs on top of the physical server. On top of the host operating system, there is a container runtime, which manages the creation and management of containers. Each container shares the host operating system with other containers, but each container has its own isolated file system, network, and process space. Docker is a containerization technology that allows you to create, deploy, and manage containers. It provides a way to package and distribute software applications in a standardized and portable format, making it easy to move them from one environment to another. With Docker, you can create a Dockerfile that describes the dependencies and configuration of your application, and then use the Docker command-line interface to build, run, and manage containers based on that Dockerfile. Overall, Docker provides a lightweight and flexible alternative to traditional VMs, making it easier to develop, deploy, and scale applications.","title":"Docker VS Virtual Machines"},{"location":"docker/docker_intro/#what-is-a-containers","text":"Docker's use of the term \"container\" is inspired by the shipping industry. In the shipping industry, containers are standardized, self-contained units that can be easily transported between ships, trains, and trucks. These containers can hold a variety of goods and products, and they are designed to be easy to load and unload from transport vehicles. Similarly, in the context of software development, a container is a standardized, self-contained unit that can hold an application along with its dependencies and configurations. Like a shipping container, a software container can be easily transported between different environments, such as development, testing, and production. By using the term \"container\", Docker is emphasizing the portability and standardization of its technology, which is similar to the shipping industry's use of containers to transport goods and products between different locations.","title":"What is a Containers"},{"location":"docker/docker_intro/#advantages-of-containerization","text":"Containerization provides several advantages over traditional deployment methods: Portability : Containers are self-contained units of software that can be easily moved between different environments. Scalability : Containers can be quickly and easily scaled up or down, depending on demand. Consistency : Containers provide a consistent environment for running applications, which makes it easier to manage and troubleshoot applications. Resource Efficiency : Containers use fewer resources than traditional virtual machines, which means that you can run more containers on the same hardware. Security : Containers provide a high degree of isolation between different applications, which helps to reduce the risk of security breaches. In summary, Docker provides a flexible and efficient way to package and deploy applications as containers. By using Docker, you can create consistent, portable, and scalable environments for running your applications, which can help to reduce costs and improve reliability.","title":"Advantages of Containerization"},{"location":"docker/docker_intro/#5-reasons-why-developers-should-consider-using-docker","text":"Consistent Development Environments : With Docker, developers can create a consistent environment for developing and testing applications. Docker allows developers to package an application along with all its dependencies, libraries, and configurations into a container. This ensures that the application will run the same way on any machine, regardless of the underlying operating system or environment. Easy Collaboration : Docker containers can be easily shared between developers, which makes it easier to collaborate on projects. Containers can be used to create a development environment that is identical across all team members, which helps to reduce the risk of compatibility issues and errors. Faster Application Development and Deployment : Docker makes it easier to develop and deploy applications by automating the process of packaging and deploying applications. Developers can quickly create and test new versions of an application in a container, and then deploy it to production with minimal effort. Improved Testing : Docker makes it easier to test applications by allowing developers to create multiple containers with different configurations and environments. This makes it easier to test applications in different scenarios, such as different operating systems, libraries, or dependencies. Resource Efficiency : Docker containers are lightweight and use fewer resources than traditional virtual machines, which means that developers can run more containers on the same machine. This can result in significant cost savings for cloud-based applications.","title":"5 reasons why developers should consider using Docker"},{"location":"docker/docker_network/","text":"What is Docker Networking? Docker Networking allows you to connect Docker containers together so that they can communicate with each other. This is useful for building complex applications that are made up of multiple containers, each with its own functionality. Docker Networking also allows you to isolate containers from each other, providing an added layer of security. Additionally, Docker Networking makes it easy to connect containers to external networks, such as the internet, and to other Docker hosts. Docker Network Types Docker supports several types of network drivers that provide different ways to connect containers together. Here are some of the most common Docker network types: Bridge Network : The default network type in Docker, a bridge network is a private network that allows containers to communicate with each other using IP addresses. Containers on a bridge network can communicate with each other but are isolated from the host machine and external networks. Host Network : A host network allows containers to use the host machine's network stack, essentially giving them direct access to the host's network interfaces. This can provide better performance but may not be as secure as other network types. Overlay Network : An overlay network allows you to connect containers that are running on different Docker hosts. This is useful for building distributed applications that are made up of multiple Docker hosts. Macvlan Network : A macvlan network allows you to assign a MAC address to a container, essentially making it appear as though it is a physical machine on the network. This can be useful for running containers that require direct access to the physical network. Creating a Docker Network Creating a Docker network is easy. You can use the docker network create command to create a new network: docker network create mynetwork This command creates a new Docker network with the name mynetwork. Attaching Containers to a Network To attach a container to a network, you can use the --network option when you start the container: docker run --name mycontainer --network mynetwork alpine sleep 3000 This command creates a new container with the name mycontainer and attaches it to the mynetwork network. Connecting to External Networks To connect a container to an external network, such as the internet, you can use the --network option to specify the host network: docker run --name mycontainer --network host alpine ping google.com This command creates a new container with the name mycontainer and attaches it to the host network. The container then uses the host machine's network stack to ping google.com. Create containers and attach them to a network Step 1: Create a Docker Network The first step is to create a Docker network that both containers will be attached to. This can be done using the docker network create command: docker network create mynetwork Step 2: Create the First Container Next, we'll create the first container and attach it to the mynetwork network. We'll use the docker run command to create the container: docker run --name container1 --network mynetwork alpine sleep 3000 This command creates a new container with the name container1, attaches it to the mynetwork network, and starts the sleep command to keep the container running for 3000 seconds. Step 3: Create the Second Container Next, we'll create the second container and attach it to the mynetwork network. We'll use the docker run command again: docker run --name container2 --network mynetwork alpine sleep 3000 Same thing, this command creates a new container with the name container2 , attaches it to the mynetwork network, and starts the sleep command to keep the container running for 3000 seconds. Step 4: Create the Third Container Now, let's create a third container that is not attached to the mynetwork network. We'll use the docker run command to create the container: docker run --name container3 alpine sleep 3000 This command creates a new container with the name container3 and starts the sleep command to keep the container running for 3000 seconds. Since we did not specify a network for this container, it will be attached to the default bridge network. Step 5: Ping One Container from the Other What is ping The ping command is commonly used to test the availability and responsiveness of network devices, such as servers or routers. It can help diagnose network connectivity issues, such as packet loss or latency. When you run the ping command, it will send packets of data to the specified destination, and display the results in the terminal. The output will typically include statistics about the packet transmission, such as the number of packets sent and received, the round-trip time (RTT) for each packet, and any errors or packet loss that occurred during the transmission. Here's an example of running the ping command: ping google.com This command sends packets of data to the Google.com domain name, and displays the results in the terminal. The output will show the RTT for each packet, as well as other statistics about the packet transmission. Ping container1 from container2 Now that both containers are running and attached to the same network, we can confirm that they can communicate with each other. We'll do this by pinging container1 from container2 : docker exec container2 ping container1 This command uses the docker exec command to run the ping container1 command inside container2. If the two containers are able to communicate with each other, you should see output similar to the following: PING container1 ( 172 .19.0.2 ) : 56 data bytes 64 bytes from 172 .19.0.2: seq = 0 ttl = 64 time = 0 .091 ms 64 bytes from 172 .19.0.2: seq = 1 ttl = 64 time = 0 .111 ms If you see this output, it means that the two containers are able to communicate with each other over the mynetwork network. Now, let's try to ping container1 from container3 , which is not attached to the mynetwork network: docker exec container3 ping container1 This command uses the docker exec command to run the ping container1 command inside container3 . Since container3 is not attached to the mynetwork network, it should not be able to communicate with container1. You should see output similar to the following: ping: bad address 'container1' This output confirms that container3 is not able to communicate with container1 . Wrap-up Docker Networking is a powerful feature that allows you to connect Docker containers together so that they can communicate with each other. By mastering Docker Networking, you can build complex applications that are made up of multiple containers, each with its own functionality. You can also isolate containers from each other, connect them to external networks, and build distributed applications that are made up of multiple Docker hosts.","title":"Docker Network"},{"location":"docker/docker_network/#what-is-docker-networking","text":"Docker Networking allows you to connect Docker containers together so that they can communicate with each other. This is useful for building complex applications that are made up of multiple containers, each with its own functionality. Docker Networking also allows you to isolate containers from each other, providing an added layer of security. Additionally, Docker Networking makes it easy to connect containers to external networks, such as the internet, and to other Docker hosts.","title":"What is Docker Networking?"},{"location":"docker/docker_network/#docker-network-types","text":"Docker supports several types of network drivers that provide different ways to connect containers together. Here are some of the most common Docker network types: Bridge Network : The default network type in Docker, a bridge network is a private network that allows containers to communicate with each other using IP addresses. Containers on a bridge network can communicate with each other but are isolated from the host machine and external networks. Host Network : A host network allows containers to use the host machine's network stack, essentially giving them direct access to the host's network interfaces. This can provide better performance but may not be as secure as other network types. Overlay Network : An overlay network allows you to connect containers that are running on different Docker hosts. This is useful for building distributed applications that are made up of multiple Docker hosts. Macvlan Network : A macvlan network allows you to assign a MAC address to a container, essentially making it appear as though it is a physical machine on the network. This can be useful for running containers that require direct access to the physical network.","title":"Docker Network Types"},{"location":"docker/docker_network/#creating-a-docker-network","text":"Creating a Docker network is easy. You can use the docker network create command to create a new network: docker network create mynetwork This command creates a new Docker network with the name mynetwork.","title":"Creating a Docker Network"},{"location":"docker/docker_network/#attaching-containers-to-a-network","text":"To attach a container to a network, you can use the --network option when you start the container: docker run --name mycontainer --network mynetwork alpine sleep 3000 This command creates a new container with the name mycontainer and attaches it to the mynetwork network.","title":"Attaching Containers to a Network"},{"location":"docker/docker_network/#connecting-to-external-networks","text":"To connect a container to an external network, such as the internet, you can use the --network option to specify the host network: docker run --name mycontainer --network host alpine ping google.com This command creates a new container with the name mycontainer and attaches it to the host network. The container then uses the host machine's network stack to ping google.com.","title":"Connecting to External Networks"},{"location":"docker/docker_network/#create-containers-and-attach-them-to-a-network","text":"","title":"Create containers and attach them to a network"},{"location":"docker/docker_network/#step-1-create-a-docker-network","text":"The first step is to create a Docker network that both containers will be attached to. This can be done using the docker network create command: docker network create mynetwork","title":"Step 1: Create a Docker Network"},{"location":"docker/docker_network/#step-2-create-the-first-container","text":"Next, we'll create the first container and attach it to the mynetwork network. We'll use the docker run command to create the container: docker run --name container1 --network mynetwork alpine sleep 3000 This command creates a new container with the name container1, attaches it to the mynetwork network, and starts the sleep command to keep the container running for 3000 seconds.","title":"Step 2: Create the First Container"},{"location":"docker/docker_network/#step-3-create-the-second-container","text":"Next, we'll create the second container and attach it to the mynetwork network. We'll use the docker run command again: docker run --name container2 --network mynetwork alpine sleep 3000 Same thing, this command creates a new container with the name container2 , attaches it to the mynetwork network, and starts the sleep command to keep the container running for 3000 seconds.","title":"Step 3: Create the Second Container"},{"location":"docker/docker_network/#step-4-create-the-third-container","text":"Now, let's create a third container that is not attached to the mynetwork network. We'll use the docker run command to create the container: docker run --name container3 alpine sleep 3000 This command creates a new container with the name container3 and starts the sleep command to keep the container running for 3000 seconds. Since we did not specify a network for this container, it will be attached to the default bridge network.","title":"Step 4: Create the Third Container"},{"location":"docker/docker_network/#step-5-ping-one-container-from-the-other","text":"","title":"Step 5: Ping One Container from the Other"},{"location":"docker/docker_network/#what-is-ping","text":"The ping command is commonly used to test the availability and responsiveness of network devices, such as servers or routers. It can help diagnose network connectivity issues, such as packet loss or latency. When you run the ping command, it will send packets of data to the specified destination, and display the results in the terminal. The output will typically include statistics about the packet transmission, such as the number of packets sent and received, the round-trip time (RTT) for each packet, and any errors or packet loss that occurred during the transmission. Here's an example of running the ping command: ping google.com This command sends packets of data to the Google.com domain name, and displays the results in the terminal. The output will show the RTT for each packet, as well as other statistics about the packet transmission.","title":"What is ping"},{"location":"docker/docker_network/#ping-container1-from-container2","text":"Now that both containers are running and attached to the same network, we can confirm that they can communicate with each other. We'll do this by pinging container1 from container2 : docker exec container2 ping container1 This command uses the docker exec command to run the ping container1 command inside container2. If the two containers are able to communicate with each other, you should see output similar to the following: PING container1 ( 172 .19.0.2 ) : 56 data bytes 64 bytes from 172 .19.0.2: seq = 0 ttl = 64 time = 0 .091 ms 64 bytes from 172 .19.0.2: seq = 1 ttl = 64 time = 0 .111 ms If you see this output, it means that the two containers are able to communicate with each other over the mynetwork network. Now, let's try to ping container1 from container3 , which is not attached to the mynetwork network: docker exec container3 ping container1 This command uses the docker exec command to run the ping container1 command inside container3 . Since container3 is not attached to the mynetwork network, it should not be able to communicate with container1. You should see output similar to the following: ping: bad address 'container1' This output confirms that container3 is not able to communicate with container1 .","title":"Ping  container1 from container2"},{"location":"docker/docker_network/#wrap-up","text":"Docker Networking is a powerful feature that allows you to connect Docker containers together so that they can communicate with each other. By mastering Docker Networking, you can build complex applications that are made up of multiple containers, each with its own functionality. You can also isolate containers from each other, connect them to external networks, and build distributed applications that are made up of multiple Docker hosts.","title":"Wrap-up"},{"location":"docker/docker_python_app/","text":"Building a Docker Image for your Python App Docker provides a convenient way to package your Python applications and dependencies into a self-contained image that can be run on any machine. In this tutorial, we will walk you through the process of building a Docker image for a standard Python app. Common problems if you choose to NOT use Docker Here are three common problems you might encounter if you choose not to use Docker for your Python script deployment: Dependency Conflicts : One of the biggest challenges with Python application deployment is managing dependencies. Without Docker, it can be difficult to ensure that your Python application and its dependencies will work correctly on different machines and environments. This can result in dependency conflicts, broken code, and lost productivity. Inconsistency : Another issue with deploying Python applications without Docker is inconsistency. Different machines and environments can have different versions of Python, different system libraries, and different configurations. This can make it difficult to reproduce and debug issues, and can result in code that works on some machines but not on others. Limited Portability : Without Docker, it can be difficult to move your Python application between different machines and environments. This can limit your ability to scale and deploy your application effectively, and can result in lost opportunities and increased costs. Overall, while it is possible to deploy Python applications without Docker, doing so can lead to dependency conflicts, inconsistency, and limited portability. By using Docker to package your Python application and its dependencies into a self-contained container, you can ensure that your application runs consistently and reliably across different machines and environments. Why we use Docker to run Python scripts in containers: Isolation : Running a Python script in a Docker container provides a degree of isolation between the script and the host machine, which helps to minimize conflicts and dependencies with other applications or processes. Consistency : By running a Python script in a Docker container, you can ensure that the environment in which the script runs is consistent across different machines and environments. This helps to avoid the \"it works on my machine\" problem and makes it easier to reproduce and debug issues. Portability : Docker containers are self-contained units that can be easily moved between different machines and environments. This makes it easy to deploy and scale Python scripts in a variety of settings, from local development to production servers. Efficiency : Docker containers are lightweight and efficient, which means that they can be deployed quickly and consume minimal resources. This makes them an ideal choice for running Python scripts that need to be deployed quickly or scaled up or down rapidly. Creating a Dockerfile for our python app The first step in building a Docker image for your Python app is to create a Dockerfile. The Dockerfile is a text file that contains a set of instructions for building the Docker image. Here's an example Dockerfile for a Python app: # Use an official Python runtime as a parent image FROM python:3.9 # Set the working directory to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir -r requirements.txt # Make port 80 available to the world outside this container EXPOSE 80 # Define environment variable ENV NAME World # Run app.py when the container launches CMD [ \"python\" , \"app.py\" ] Let's see how this file works line by line Defining the base image and environment The first line in the Dockerfile specifies the base image to use for the container. In this case, we are using the official Python 3.9 image. FROM python:3.9 Next, we set the working directory to \"/app\" inside the container. WORKDIR /app This is the directory where we will copy our application code and dependencies. Installing dependencies and copying source code Next, we copy our application code and dependencies into the container. This is done using the COPY command. COPY . /app This command copies the contents of the current directory into the \"/app\" directory inside the container. We then install any dependencies that are needed for our application. This is done using the \"RUN\" command. RUN pip install --no-cache-dir -r requirements.txt This command reads the requirements.txt file and installs any dependencies listed in the file. Configuring the app and exposing ports Next, we configure our application by setting any environment variables and exposing any necessary ports. EXPOSE 80 ENV NAME World In this example, we are exposing port 80 and setting an environment variable named \"NAME\" to \"World\". Finally, we specify the command to run when the container is launched. CMD [ \"python\" , \"app.py\" ] This command specifies that the \"app.py\" file should be executed when the container is launched. Building the Docker image Now that we have created our Dockerfile, we can build our Docker image. To do this, we use the docker build command like this : docker build -t my-python-app . This command tells Docker to build an image with the name my-python-app using the Dockerfile in the current directory (.) Running the Docker container Once we have built our Docker image, we can run it using the docker run command : docker run -p 4000 :80 my-python-app This command tells Docker to run the my-python-app image and map port 4000 on the host machine to port 80 inside the container. Summary of the most common Dockerfile commands These commands can be combined in various ways to create a Dockerfile for your specific application. By using Dockerfile commands, you can define the steps needed to build a Docker image and run a Docker container for your application. Most common Dockerfile commands: FROM : Specifies the base image for the Docker image. RUN : Executes a command during the build process, such as installing dependencies or running tests. COPY or ADD : Copies files or directories from the host machine into the Docker image. WORKDIR : Sets the working directory inside the Docker image. EXPOSE : Exposes a port for the Docker container. ENV : Sets an environment variable inside the Docker image. CMD or ENTRYPOINT : Specifies the command to run when the Docker container starts. These commands can be combined in various ways to create a Dockerfile for your specific application. By using Dockerfile commands, you can define the steps needed to build a Docker image and run a Docker container for your application. Run a FastAPI \"Hello World\" Python app into a container Create a new Python file named main.py with the following code : from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) async def root (): return { \"message\" : \"Hello World!\" } Create a new file named Dockerfile in the same directory with the following contents: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim COPY ./app /app EXPOSE 80 CMD [ \"uvicorn\" , \"main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] This Dockerfile uses the tiangolo/uvicorn-gunicorn-fastapi base image, copies the app directory (which contains main.py) into the container, exposes port 80 , and sets the CMD to run the uvicorn server with the main:app parameter. Build the Docker image using the following command: docker build -t fastapi-demo . This command builds the Docker image and tags it with the name fastapi-demo. Run the Docker container using the following command: docker run -p 80 :80 fastapi-demo This command starts the Docker container and maps port 80 on the host machine to port 80 inside the container. Make sure the container is up and the API is running with the command : docker ps If you see the container UP and running, then you can open your web browser and navigate to http://localhost:80/ . You should see the message \"Hello World!\" displayed in your browser \ud83e\udd73 Why 0.0.0.0 and not localhost When setting up a Docker container, it's common to bind the container's internal port to a port on the host machine so that the container's services can be accessed from the outside. When specifying the IP address for the --host parameter in the uvicorn command, you have a choice between using localhost and 0.0.0.0 . Using localhost as the IP address for the --host parameter means that the server will only accept requests coming from within the container itself. This can be useful if you want to restrict access to the server to only the container itself. However, if you want to allow external access to the server (i.e., from the host machine or other machines on the same network), you should use 0.0.0.0 as the IP address for the --host parameter. This tells the server to accept requests from any IP address. So, in a FastAPI application context for a Docker container, using 0.0.0.0 as the IP address for the --host parameter allows the container's services to be accessed from the host machine or other machines on the same network, while using localhost would restrict access to only the container itself . Wrap-up In summary, building a Docker image for your Python app involves creating a Dockerfile, defining the base image and environment, installing dependencies and copying source code, configuring the app and exposing ports, and finally building and running the Docker image.","title":"Building a Docker Image for Python App"},{"location":"docker/docker_python_app/#building-a-docker-image-for-your-python-app","text":"Docker provides a convenient way to package your Python applications and dependencies into a self-contained image that can be run on any machine. In this tutorial, we will walk you through the process of building a Docker image for a standard Python app.","title":"Building a Docker Image for your Python App"},{"location":"docker/docker_python_app/#common-problems-if-you-choose-to-not-use-docker","text":"Here are three common problems you might encounter if you choose not to use Docker for your Python script deployment: Dependency Conflicts : One of the biggest challenges with Python application deployment is managing dependencies. Without Docker, it can be difficult to ensure that your Python application and its dependencies will work correctly on different machines and environments. This can result in dependency conflicts, broken code, and lost productivity. Inconsistency : Another issue with deploying Python applications without Docker is inconsistency. Different machines and environments can have different versions of Python, different system libraries, and different configurations. This can make it difficult to reproduce and debug issues, and can result in code that works on some machines but not on others. Limited Portability : Without Docker, it can be difficult to move your Python application between different machines and environments. This can limit your ability to scale and deploy your application effectively, and can result in lost opportunities and increased costs. Overall, while it is possible to deploy Python applications without Docker, doing so can lead to dependency conflicts, inconsistency, and limited portability. By using Docker to package your Python application and its dependencies into a self-contained container, you can ensure that your application runs consistently and reliably across different machines and environments.","title":"Common problems if you choose to NOT use Docker"},{"location":"docker/docker_python_app/#why-we-use-docker-to-run-python-scripts-in-containers","text":"Isolation : Running a Python script in a Docker container provides a degree of isolation between the script and the host machine, which helps to minimize conflicts and dependencies with other applications or processes. Consistency : By running a Python script in a Docker container, you can ensure that the environment in which the script runs is consistent across different machines and environments. This helps to avoid the \"it works on my machine\" problem and makes it easier to reproduce and debug issues. Portability : Docker containers are self-contained units that can be easily moved between different machines and environments. This makes it easy to deploy and scale Python scripts in a variety of settings, from local development to production servers. Efficiency : Docker containers are lightweight and efficient, which means that they can be deployed quickly and consume minimal resources. This makes them an ideal choice for running Python scripts that need to be deployed quickly or scaled up or down rapidly.","title":"Why we use Docker to run Python scripts in containers:"},{"location":"docker/docker_python_app/#creating-a-dockerfile-for-our-python-app","text":"The first step in building a Docker image for your Python app is to create a Dockerfile. The Dockerfile is a text file that contains a set of instructions for building the Docker image. Here's an example Dockerfile for a Python app: # Use an official Python runtime as a parent image FROM python:3.9 # Set the working directory to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --no-cache-dir -r requirements.txt # Make port 80 available to the world outside this container EXPOSE 80 # Define environment variable ENV NAME World # Run app.py when the container launches CMD [ \"python\" , \"app.py\" ]","title":"Creating a Dockerfile for our python app"},{"location":"docker/docker_python_app/#lets-see-how-this-file-works-line-by-line","text":"","title":"Let's see how this file works line by line"},{"location":"docker/docker_python_app/#defining-the-base-image-and-environment","text":"The first line in the Dockerfile specifies the base image to use for the container. In this case, we are using the official Python 3.9 image. FROM python:3.9 Next, we set the working directory to \"/app\" inside the container. WORKDIR /app This is the directory where we will copy our application code and dependencies.","title":"Defining the base image and environment"},{"location":"docker/docker_python_app/#installing-dependencies-and-copying-source-code","text":"Next, we copy our application code and dependencies into the container. This is done using the COPY command. COPY . /app This command copies the contents of the current directory into the \"/app\" directory inside the container. We then install any dependencies that are needed for our application. This is done using the \"RUN\" command. RUN pip install --no-cache-dir -r requirements.txt This command reads the requirements.txt file and installs any dependencies listed in the file.","title":"Installing dependencies and copying source code"},{"location":"docker/docker_python_app/#configuring-the-app-and-exposing-ports","text":"Next, we configure our application by setting any environment variables and exposing any necessary ports. EXPOSE 80 ENV NAME World In this example, we are exposing port 80 and setting an environment variable named \"NAME\" to \"World\". Finally, we specify the command to run when the container is launched. CMD [ \"python\" , \"app.py\" ] This command specifies that the \"app.py\" file should be executed when the container is launched.","title":"Configuring the app and exposing ports"},{"location":"docker/docker_python_app/#building-the-docker-image","text":"Now that we have created our Dockerfile, we can build our Docker image. To do this, we use the docker build command like this : docker build -t my-python-app . This command tells Docker to build an image with the name my-python-app using the Dockerfile in the current directory (.)","title":"Building the Docker image"},{"location":"docker/docker_python_app/#running-the-docker-container","text":"Once we have built our Docker image, we can run it using the docker run command : docker run -p 4000 :80 my-python-app This command tells Docker to run the my-python-app image and map port 4000 on the host machine to port 80 inside the container.","title":"Running the Docker container"},{"location":"docker/docker_python_app/#summary-of-the-most-common-dockerfile-commands","text":"These commands can be combined in various ways to create a Dockerfile for your specific application. By using Dockerfile commands, you can define the steps needed to build a Docker image and run a Docker container for your application. Most common Dockerfile commands: FROM : Specifies the base image for the Docker image. RUN : Executes a command during the build process, such as installing dependencies or running tests. COPY or ADD : Copies files or directories from the host machine into the Docker image. WORKDIR : Sets the working directory inside the Docker image. EXPOSE : Exposes a port for the Docker container. ENV : Sets an environment variable inside the Docker image. CMD or ENTRYPOINT : Specifies the command to run when the Docker container starts. These commands can be combined in various ways to create a Dockerfile for your specific application. By using Dockerfile commands, you can define the steps needed to build a Docker image and run a Docker container for your application.","title":"Summary of the most common Dockerfile commands"},{"location":"docker/docker_python_app/#run-a-fastapi-hello-world-python-app-into-a-container","text":"Create a new Python file named main.py with the following code : from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) async def root (): return { \"message\" : \"Hello World!\" } Create a new file named Dockerfile in the same directory with the following contents: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim COPY ./app /app EXPOSE 80 CMD [ \"uvicorn\" , \"main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] This Dockerfile uses the tiangolo/uvicorn-gunicorn-fastapi base image, copies the app directory (which contains main.py) into the container, exposes port 80 , and sets the CMD to run the uvicorn server with the main:app parameter. Build the Docker image using the following command: docker build -t fastapi-demo . This command builds the Docker image and tags it with the name fastapi-demo. Run the Docker container using the following command: docker run -p 80 :80 fastapi-demo This command starts the Docker container and maps port 80 on the host machine to port 80 inside the container. Make sure the container is up and the API is running with the command : docker ps If you see the container UP and running, then you can open your web browser and navigate to http://localhost:80/ . You should see the message \"Hello World!\" displayed in your browser \ud83e\udd73","title":"Run a FastAPI \"Hello World\" Python app into a container"},{"location":"docker/docker_python_app/#why-0000-and-not-localhost","text":"When setting up a Docker container, it's common to bind the container's internal port to a port on the host machine so that the container's services can be accessed from the outside. When specifying the IP address for the --host parameter in the uvicorn command, you have a choice between using localhost and 0.0.0.0 . Using localhost as the IP address for the --host parameter means that the server will only accept requests coming from within the container itself. This can be useful if you want to restrict access to the server to only the container itself. However, if you want to allow external access to the server (i.e., from the host machine or other machines on the same network), you should use 0.0.0.0 as the IP address for the --host parameter. This tells the server to accept requests from any IP address. So, in a FastAPI application context for a Docker container, using 0.0.0.0 as the IP address for the --host parameter allows the container's services to be accessed from the host machine or other machines on the same network, while using localhost would restrict access to only the container itself .","title":"Why 0.0.0.0 and not localhost"},{"location":"docker/docker_python_app/#wrap-up","text":"In summary, building a Docker image for your Python app involves creating a Dockerfile, defining the base image and environment, installing dependencies and copying source code, configuring the app and exposing ports, and finally building and running the Docker image.","title":"Wrap-up"},{"location":"docker/docker_volume/","text":"Docker Volumes What are Docker Volumes? Docker volumes are a way to persist data outside of a container's file system. When you create a Docker volume, you create a new volume object that can be attached to one or more containers. Data can be written to or read from the volume, and the data will persist even if the container is removed or recreated. Using a Docker Volume for a Hello World FastAPI App Let's say you have a simple Hello World FastAPI app that you want to run in a Docker container. Here's an example of what the Dockerfile might look like: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8 WORKDIR /app COPY ./app /app RUN pip install --no-cache-dir -r requirements.txt CMD [ \"uvicorn\" , \"app.main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] This Dockerfile uses the tiangolo/uvicorn-gunicorn-fastapi base image and copies the app directory into the /app directory in the container. Next, it runs the pip command to install the packages listed in the requirements.txt file. The --no-cache-dir flag is used to ensure that the packages are installed from scratch, rather than using any cached packages. Finally, it sets the command to start the app using the Uvicorn server on port 80 . Here's the code for the app.py file: from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } and the requirements.txt file : fastapi==0.68.1 uvicorn==0.15.0 Why using volume When you create a Docker volume and attach it to a container, it's like putting a bookmark from the container to a local folder on your host machine. Just like a bookmark in a web browser, a volume allows you to quickly access a specific location in the container's file system, without having to navigate through all of the directories manually. With a volume, you can also persist data outside of the container's file system. This can be useful if you need to share data between multiple containers, or if you need to keep data separate from the container image itself. Overall, volumes are a powerful tool in Docker that allow you to manage and persist data in a flexible and efficient way. Add volume to docker run With this Dockerfile, you can build and run the container using the following commands: docker build -t myimage . docker run -d --name mycontainer -p 8000 :80 -v $( pwd ) :/app myimage When you run this app in a Docker container, you can use a Docker volume to mount the app.py file into the container at runtime, rather than copying it into the container at build time. This has a few advantages: You can make changes to the app.py file without having to rebuild the entire Docker image. You can keep the app code and the container image separate, which can make it easier to manage and update the app over time.","title":"Docker Volumes"},{"location":"docker/docker_volume/#docker-volumes","text":"","title":"Docker Volumes"},{"location":"docker/docker_volume/#what-are-docker-volumes","text":"Docker volumes are a way to persist data outside of a container's file system. When you create a Docker volume, you create a new volume object that can be attached to one or more containers. Data can be written to or read from the volume, and the data will persist even if the container is removed or recreated.","title":"What are Docker Volumes?"},{"location":"docker/docker_volume/#using-a-docker-volume-for-a-hello-world-fastapi-app","text":"Let's say you have a simple Hello World FastAPI app that you want to run in a Docker container. Here's an example of what the Dockerfile might look like: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8 WORKDIR /app COPY ./app /app RUN pip install --no-cache-dir -r requirements.txt CMD [ \"uvicorn\" , \"app.main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] This Dockerfile uses the tiangolo/uvicorn-gunicorn-fastapi base image and copies the app directory into the /app directory in the container. Next, it runs the pip command to install the packages listed in the requirements.txt file. The --no-cache-dir flag is used to ensure that the packages are installed from scratch, rather than using any cached packages. Finally, it sets the command to start the app using the Uvicorn server on port 80 . Here's the code for the app.py file: from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } and the requirements.txt file : fastapi==0.68.1 uvicorn==0.15.0","title":"Using a Docker Volume for a Hello World FastAPI App"},{"location":"docker/docker_volume/#why-using-volume","text":"When you create a Docker volume and attach it to a container, it's like putting a bookmark from the container to a local folder on your host machine. Just like a bookmark in a web browser, a volume allows you to quickly access a specific location in the container's file system, without having to navigate through all of the directories manually. With a volume, you can also persist data outside of the container's file system. This can be useful if you need to share data between multiple containers, or if you need to keep data separate from the container image itself. Overall, volumes are a powerful tool in Docker that allow you to manage and persist data in a flexible and efficient way.","title":"Why using volume"},{"location":"docker/docker_volume/#add-volume-to-docker-run","text":"With this Dockerfile, you can build and run the container using the following commands: docker build -t myimage . docker run -d --name mycontainer -p 8000 :80 -v $( pwd ) :/app myimage When you run this app in a Docker container, you can use a Docker volume to mount the app.py file into the container at runtime, rather than copying it into the container at build time. This has a few advantages: You can make changes to the app.py file without having to rebuild the entire Docker image. You can keep the app code and the container image separate, which can make it easier to manage and update the app over time.","title":"Add volume to docker run"},{"location":"nosql/elastic/","text":"Introduction to Elasticsearch Elasticsearch is an open-source, document-based NoSQL database that is designed for full-text search and analytics. Elasticsearch is built on top of the Lucene search engine and is optimized for fast search and retrieval of unstructured data . Elastic architecture introduction Elasticsearch is a distributed, document-oriented NoSQL database that is optimized for search and analytics. It is built on top of the Lucene search engine and is designed to scale horizontally across multiple nodes and clusters. Cluster, nodes and replicas Nodes are individual instances of Elasticsearch that are part of a cluster. Each node stores a subset of the data in the cluster and is responsible for processing search and indexing requests for that data. Nodes can be added or removed from a cluster dynamically, allowing for easy scalability and fault tolerance. A cluster is a collection of nodes that work together to store and process data. Elasticsearch clusters are designed to be highly available and fault tolerant, with built-in features for data replication and failover. Clusters can scale horizontally by adding more nodes to the cluster, allowing for increased processing power and storage capacity. Sharding is the process of partitioning data across multiple nodes in a cluster. When a document is indexed in Elasticsearch, it is assigned to a specific shard based on a hashing algorithm that takes into account the document's ID. By default, each index in Elasticsearch is divided into five primary shards, with each shard having one or more replicas. This allows Elasticsearch to distribute the workload across multiple nodes, improving search and indexing performance. The number of shards and replicas can be configured for each index based on the size and search requirements of the data. For example, a large index with a high write throughput might require more primary shards to distribute the data more evenly across nodes, while a smaller index with a lower write throughput might require fewer primary shards to reduce the overhead of managing multiple shards. Index An index is like a database in a traditional relational database system. It is a logical container for one or more documents that share a similar structure and are stored together. When you create an index in Elasticsearch, you can specify the mapping for the fields that the documents in the index will contain. The mapping defines the data types and formats for the fields, which allows Elasticsearch to index and search the data more efficiently. You can also configure the number of primary shards that the index should have, which determines how the data in the index is distributed across nodes in the Elasticsearch cluster. This allows Elasticsearch to scale horizontally as the size of the index grows. In addition, you can configure the number of replica shards for the index, which provide redundancy and allow for failover in case a primary shard becomes unavailable. Overall, an index in Elasticsearch is a distributed, document-oriented NoSQL database that uses nodes, clusters, and sharding to provide scalability, fault tolerance, high availability and provides a way to organize and search data efficiently. Overall architecture The diagram shows a simplified architecture for an Elasticsearch cluster with three nodes. Each node is represented by a rectangular box and is labeled with its unique node name, IP address, and port number. The nodes are connected to each other through a network, represented by the gray lines connecting the boxes. This network allows the nodes to communicate with each other and share data. The Elasticsearch cluster is managed by a master node, which is responsible for coordinating the cluster and maintaining its state. In the diagram, the master node is indicated by the green box labeled \"Master-eligible node.\" The other nodes in the cluster are known as data nodes, and they are responsible for storing and indexing the data. In the diagram, the data nodes are indicated by the yellow boxes labeled \"Data node.\" Each data node stores a subset of the data in the cluster, and the data is distributed across the nodes using a technique called sharding. Each shard represents a portion of the data and is stored on a separate data node. The client nodes, represented by the blue boxes labeled \"Client node,\" are used to interact with the cluster and submit search and indexing requests. Client nodes do not store any data themselves, but they communicate with the data nodes to retrieve and manipulate the data. Finally, the external world is represented by the orange box labeled \"External clients,\" which can be any application or user that needs to interact with the Elasticsearch cluster. Overall, the architecture diagram shows how an Elasticsearch cluster is composed of multiple nodes working together to store, index, and search data efficiently. Installation with Docker To install Elasticsearch, we can use Docker, a containerization platform that simplifies the process of installing and running software applications. Install Docker on your machine if you haven't already. Open a command prompt and run the following command to download and run the Elasticsearch Docker image: docker run -p 9200 :9200 -p 9300 :9300 -d -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.14.0 docker run : This command tells Docker to run a container from an image. -p 9200:9200: This option maps port 9200 in the container to port 9200 on the host machine, allowing us to access Elasticsearch on port - 9200 from outside the container. -p 9300:9300: This option maps port 9300 in the container to port 9300 on the host machine, allowing nodes in the Elasticsearch cluster to communicate with each other on port 9300. -d : This option runs the container in detached mode, which means it runs in the background and doesn't attach to the terminal. This allows us to continue using the terminal while the container is running. -e \"discovery.type=single-node\": This option sets an environment variable in the container called discovery.type to single-node. This tells Elasticsearch to start as a single-node cluster, which is useful for testing or development purposes. docker.elastic.co/elasticsearch/elasticsearch:7.14.0 : This is the name and version of the Elasticsearch image we want to run. When you run this command, Docker will download the Elasticsearch image from Docker Hub (unless it's already downloaded), create a container from the image, and start Elasticsearch running inside the container. The container will be accessible on port 9200 and 9300, and Elasticsearch will be running as a single-node cluster \ud83e\udd73 For more information about docker installation you can check the official documention here Test your installation Cluster info You can test if the elasticsearch container is running by run this command into your terminal (the jq command is here for a better return format, if you don't have it insall it here ) : curl 0 .0.0.0:9200/_cluster/health | jq You should see this : % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 390 100 390 0 0 308 0 0 :00:01 0 :00:01 --:--:-- 308 { \"cluster_name\" : \"docker-cluster\" , \"status\" : \"green\" , \"timed_out\" : false, \"number_of_nodes\" : 1 , \"number_of_data_nodes\" : 1 , \"active_primary_shards\" : 1 , \"active_shards\" : 1 , \"relocating_shards\" : 0 , \"initializing_shards\" : 0 , \"unassigned_shards\" : 0 , \"delayed_unassigned_shards\" : 0 , \"number_of_pending_tasks\" : 0 , \"number_of_in_flight_fetch\" : 0 , \"task_max_waiting_in_queue_millis\" : 0 , \"active_shards_percent_as_number\" : 100 } This command returns information about the overall health of the Elasticsearch cluster. It provides information about the number of nodes in the cluster, the status of each node, the number of shards and replicas, and the overall status of the cluster. The response includes a variety of metrics such as the number of unassigned shards, the number of active and inactive primary shards, and the status of the cluster's overall health. Nodes info curl -X GET \"http://0.0.0.0:9200/_cat/nodes?v\" This command returns information about the nodes in the Elasticsearch cluster. It provides a summary of each node, including its IP address, node ID, and whether it is currently active or not. The response also includes a variety of metrics such as the number of open file descriptors, the amount of disk space used, and the amount of heap memory used. Data Modeling in Elasticsearch Elasticsearch stores data as JSON-like documents, which can be nested and hierarchical. The data model in Elasticsearch is flexible, allowing for easy changes to the schema. Documents are the primary storage structure in Elasticsearch. Each document contains reserved fields (the document's metadata), such as : _index : where the document resides _type : the type of document it represents (database) _id : a unique identifier for the document _source : the data in the form of a dictionary json format overview JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate. It consists of key-value pairs, with each pair separated by a comma and enclosed in curly braces. Here's an example of a simple JSON document: { \"name\" : \"John Smith\" , \"age\" : 35 , \"city\" : \"New York\" } In Elasticsearch, JSON is used as the primary format for documents that are stored and indexed in the database. Each document is represented as a JSON object, with fields that describe the properties of the document. Create an index and insert data Remember, an index is like a database in a traditional relational database system. We can create a specific index, let's say cities and give it a certain settings like 2 shards and 2 replicas per shards with the following command : curl -XPUT 'http://localhost:9200/cities' -H 'Content-Type: application/json' -d ' { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 } }' you should see this response : {\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"my_index\"} To retrieve the index and verify its settings: curl -XGET 'http://localhost:9200/my_index/_settings' | jq you should see this output : { \"cities\" : { \"settings\" : { \"index\" : { \"routing\" : { \"allocation\" : { \"include\" : { \"_tier_preference\" : \"data_content\" } } } , \"number_of_shards\" : \"2\" , \"provided_name\" : \"cities\" , \"creation_date\" : \"1678636556321\" , \"number_of_replicas\" : \"2\" , \"uuid\" : \"vsqBEmHWSBaki2AL-oClsA\" , \"version\" : { \"created\" : \"7110199\" } } } } } Now let's populate this index by creating our first document by running into our terminal : curl -XPOST 'http://localhost:9200/cities/_doc' -H 'Content-Type: application/json' -d ' { \"city\": \"London\", \"country\": \"England\" }' You should see the following response confirmation : { \"_index\" : \"cities\" , \"_type\" : \"_doc\" , \"_id\" : \"pquR1oYBQIvdICuNRLuD\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 3 , \"successful\" : 1 , \"failed\" : 0 } , \"_seq_no\" : 1 , \"_primary_term\" : 1 } That's indicate the document is been created and inserted into our index with an unique id pquR1oYBQIvdICuNRLuD \ud83e\udd13 You can verify this by running this command : curl -XGET 'http://localhost:9200/cities/_doc/{document_id}' You should see : { \"_index\" : \"cities\" , \"_type\" : \"_doc\" , \"_id\" : \"pquR1oYBQIvdICuNRLuD\" , \"_version\" :1, \"_seq_no\" :1, \"_primary_term\" :1, \"found\" :true \"_source\" : { \"city\" : \"London\" , \"country\" : \"England\" } } As you can see, our document content is in the _source field, we will explain more the particular return format of elasticsearch. Indexing data in Elasticsearch Create index from json file Let's create a bash script in order to insert few indices from json files and play with them. First download the json files here and run the following bash script into your terminal : insert_data.sh curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/receipe/_bulk --data-binary \"@receipe.json\" && \\ printf \"\\n\u2705 Insertion receipe index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/accounts/docs/_bulk --data-binary \"@accounts.json\" printf \"\\n\u2705 Insertion accounts index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/movies/_bulk --data-binary \"@movies.json\" printf \"\\n\u2705 Insertion movies index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/products/_bulk --data-binary \"@products.json\" printf \"\\n\u2705 Insertion products index to elastic node OK \u2705 \" curl : a command-line tool for sending HTTP requests and receiving HTTP responses from a server. -s : a flag that tells curl to operate silently, i.e., not to show the progress meter or any error messages. -H \"Content-Type: application/x-ndjson\": a header that sets the content type of the request to application/x-ndjson. This tells - Elasticsearch that the data being sent in the request body is in NDJSON format. -XPOST : a flag that tells curl to send a POST request to the specified URL. localhost:9200/receipe/_bulk : the URL of the Elasticsearch endpoint to which the request is being sent. In this case, it's the _bulk API for the receipe index on the local Elasticsearch instance running on port 9200. --data-binary \"@receipe.json\" : the request body, which is specified as a binary data file (@ symbol followed by the filename in this case receipe) in NDJSON format. The --data-binary flag tells curl to send the data as is, without any special interpretation. Overall, this command is sending a bulk request to Elasticsearch to index data contained in the receipe.json file into the receipe index. The data is in NDJSON format, which is a format that Elasticsearch can parse and process efficiently. Working with Kibana interface For the rest of this section we will be working with kibana dev tools graphic user interface (GUI). You can run an elastic single node cluster and kibana GUI with the following docker-compose file : docker-compose.yml version : '2.2' services : elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.11.1 container_name : elasticsearch restart : always environment : - xpack.security.enabled=false - discovery.type=single-node ulimits : memlock : soft : -1 hard : -1 nofile : soft : 65536 hard : 65536 cap_add : - IPC_LOCK volumes : - ./elas1:/usr/share/elasticsearch/data ports : - 9200:9200 - 9300:9300 networks : - esnet kibana : container_name : kibana image : docker.elastic.co/kibana/kibana:7.11.1 restart : always ports : - 5601:5601 depends_on : - elasticsearch networks : - esnet networks : esnet : driver : bridge This is a Docker Compose file that defines two services: Elasticsearch and Kibana. The elasticsearch service uses the official Elasticsearch Docker image version 7.11.1. It sets the container name to elasticsearch and ensures that the container is always restarted if it stops or crashes. It disables X-Pack security and configures Elasticsearch to run as a single-node cluster. It also sets ulimit and cap_add settings to ensure that Elasticsearch has sufficient resources. Finally, it maps the ./elas1 directory on the host to /usr/share/elasticsearch/data inside the container, and exposes ports 9200 and 9300 to allow external access to Elasticsearch. The kibana service uses the official Kibana Docker image version 7.11.1. It sets the container name to kibana and ensures that the container is always restarted if it stops or crashes. It exposes port 5601 to allow external access to Kibana. It also depends on the Elasticsearch service, so it won't start until Elasticsearch is up and running. Overall, Kibana is a web-based user interface for Elasticsearch that allows you to interact with Elasticsearch data, run queries, and visualize data in various ways. One of the main advantages of using Kibana is that it provides a user-friendly interface for Elasticsearch, making it easier to explore and analyze data without needing to write complex queries. It should take a few seconds to launch the installation and then you can go to : http://localhost:5601/app/dev_tools#/console in your browser to see the dev tool console \ud83e\udd13 You can know verify all our indicies by running the following command : GET /_cat/in dices?v Mapping An index mapping in Elasticsearch is a way to define the structure of the documents that will be stored in an index. It specifies the fields that will be part of each document, along with their data types, properties, and settings. When you create an index in Elasticsearch, you can either provide an explicit mapping or let Elasticsearch infer the mapping automatically based on the first document that is indexed. However, it is generally recommended to define an explicit mapping to ensure that the index has a consistent structure and to avoid unexpected field types or mappings. An index mapping consists of two main components: field mappings and index settings. Field mappings define the fields that will be part of each document in the index, along with their data types and properties. For example, a field mapping can define a string field that will store text data, or a numeric field that will store integer or float values. Field mappings can also specify additional settings such as the analyzer to use for text fields, or the format to use for date fields. Index settings define various aspects of the index, such as the number of shards and replicas, the analysis settings, and the index lifecycle policies. For example, you can specify the number of primary and replica shards that the index will use, or the analyzer to use for text fields in the index. Overall, an index mapping is an essential component of an Elasticsearch index that allows you to define the structure of the documents that will be stored in the index, along with various settings and configurations that affect the index behavior. Get index mapping Let's take a look of our receipe index by tapping this into our kibana dev tool console : GET /receipe/_mapping You should see this output : { \"receipe\" : { \"mappings\" : { \"properties\" : { \"created\" : { \"type\" : \"date\" , \"format\" : \"yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis\" } , \"description\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"ingredients\" : { \"properties\" : { \"name\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"quantity\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } , \"preparation_time_minutes\" : { \"type\" : \"long\" } , \"ratings\" : { \"type\" : \"float\" } , \"servings\" : { \"properties\" : { \"max\" : { \"type\" : \"long\" } , \"min\" : { \"type\" : \"long\" } } } , \"steps\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"title\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } } } I know it is long but it is very explicit, it shows the structure of the receipe.json document, which has been indexed in Elasticsearch. Here's a breakdown of what each section of the mapping represents: The mapping is defined for the receipe index, which contains a single document type. The properties section contains all the fields that are part of each document in the index. In this case, the receipe.json document has 7 fields: created, description, ingredients, preparation_time_minutes, ratings, servings, and steps. Each field in the properties section has a specific data type and additional properties that define how the data is indexed and stored. Here are the properties for each field : created : a date field that can be parsed in the formats \"yyyy/MM/dd HH:mm:ss\", \"yyyy/MM/dd\", or as epoch milliseconds. description : a text field that allows full-text search and has an additional keyword sub-field that can be used for exact matching. ingredients : an object field that contains two sub-fields: name and quantity. Both sub-fields are text fields with an additional keyword sub-field. preparation_time_minutes : a long field that stores the preparation time for the recipe in minutes. ratings : a float field that stores the average rating for the recipe. servings : an object field that contains two sub-fields: min and max. Both sub-fields are long fields that represent the minimum and maximum servings for the recipe. steps : a text field that allows full-text search and has an additional keyword sub-field that can be used for exact matching. Overall, this mapping provides a detailed description of the structure of the receipe.json document, which allows Elasticsearch to index and search the data efficiently. It also provides additional settings and properties that can be used to customize the behavior of the index and optimize its performance. You can try to print the mapping for the other json documents \ud83e\udd13 First query Let's do our first query on the movies index, go to the kibana dev tool interface and copy this query : GET movies / _search { \"query\" : { \"match_all\" : {} } } This query retrieves all documents from the movies index in Elasticsearch. It uses the match_all query to match all documents in the index, which is equivalent to a SELECT * statement in SQL. GET movies/_search indicates that we want to execute a search request on the movies index. The _search endpoint is used to search for documents in Elasticsearch we will see this later. \"query\": { \"match_all\": {} } is the query definition. In this case, we're using the match_all query, which matches all documents in the index. The empty object {} inside the match_all query means that we're not applying any filters or constraints to the search results. As you can notice elatic has a strange return format. Let's take a look at this return : CRUD Operations in Elasticsearch Elasticsearch supports CRUD (Create, Read, Update, and Delete) operations for manipulating data. Here are some examples of how to perform CRUD operations in Elasticsearch with our json documents : Create documents To create a new document in a given index, let's say the receipe index for example, use the following query : POST receipe / _doc { \"created\" : \"2022/03/12 12:00:00\" , \"title\" : \"Chocolate Cake\" , \"description\" : \"A rich and decadent chocolate cake recipe\" , \"preparation_time_minutes\" : 60 , \"servings\" : { \"min\" : 8 , \"max\" : 10 }, \"ingredients\" : [ { \"name\" : \"flour\" , \"quantity\" : \"2 cups\" }, { \"name\" : \"sugar\" , \"quantity\" : \"2 cups\" }, { \"name\" : \"cocoa powder\" , \"quantity\" : \"3/4 cup\" }, { \"name\" : \"baking powder\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"baking soda\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"salt\" , \"quantity\" : \"1 teaspoon\" }, { \"name\" : \"buttermilk\" , \"quantity\" : \"1 cup\" }, { \"name\" : \"vegetable oil\" , \"quantity\" : \"1/2 cup\" }, { \"name\" : \"eggs\" , \"quantity\" : \"2\" }, { \"name\" : \"vanilla extract\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"boiling water\" , \"quantity\" : \"1 cup\" } ] , \"steps\" : \"1. Preheat oven to 350 degrees F (175 degrees C). Grease and flour two 9-inch round cake pans. \\n 2. In a large mixing bowl, combine the flour, sugar, cocoa powder, baking powder, baking soda, and salt. Mix well. \\n 3. Add the buttermilk, vegetable oil, eggs, and vanilla extract. Beat with an electric mixer on medium speed for 2 minutes. \\n 4. Stir in the boiling water (the batter will be thin). Pour the batter into the prepared pans. \\n 5. Bake for 30 to 35 minutes, or until a toothpick inserted into the center of the cakes comes out clean. \\n 6. Allow the cakes to cool in the pans for 10 minutes, then remove them from the pans and cool completely on wire racks. \\n 7. Frost and decorate the cakes as desired.\" } You should see this return : { \"_index\" : \"receipe\" , \"_type\" : \"_doc\" , \"_id\" : \"p6va1oYBQIvdICuNjLvj\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 2 , \"successful\" : 1 , \"failed\" : 0 } , \"_seq_no\" : 42 , \"_primary_term\" : 13 } With details, it means : _index : The name of the index where the document was created, which in this case is \"receipe\". _type : The document type, which is \"_doc\" by default in Elasticsearch 7.x and later. _id : The unique ID of the newly created document. Elasticsearch automatically generates an ID if you don't provide one explicitly for our case \"_id\" : \"p6va1oYBQIvdICuNjLvj\" _version : The version of the document after the create operation. The initial version is always 1. result: The result of the create operation, which is \"created\" in this case. This indicates that a new document was created. _shards : The number of shards involved in the create operation and the number of successful and failed shards. In this case, the create operation involved two shards and was successful on one shard. _seq_no and _primary_term : These values are used internally by Elasticsearch to manage replication and consistency. They are not relevant for most users. Overall, this response confirms that the document was created successfully in the \"receipe\" index with a unique ID, version 1, and one successful shard. If we want to create a document with a particular ID just modify the first line of the request by : POST receipe / _doc / 9999 It will create a \"receipe\" document with an ID of 9999 . Read documents Like we have seen before you can run : GET index / _doc / { document_id } Update documents To update a document in the index, use the following query: POST receipe / _update / { document_id } { \"doc\" : { \"description\" : \"A rich and decadent chocolate cake recipe with layers of buttercream frosting\" } } Replace {document_id} with the ID of the document you want to update. This query updates the description field. Delete documents To delete a document from the index, use the following query: DELETE receipe / _doc / { document_id } Replace {document_id} with the ID of the document you want to delete. This query deletes the specified document from the index. That's it for the CRUD operations! These are the basic operations you can perform on documents in Elasticsearch. There are many other advanced features and queries you can use to search, analyze, and visualize your data ! Querying Elasticsearch data Querying in Elasticsearch can seem more complex than a traditional approach like SQL because Elasticsearch is designed to handle unstructured and semi-structured data, whereas traditional databases like SQL are designed to handle structured data. In Elasticsearch, documents are stored as JSON objects, and each document can have different fields with different data types. This means that searching for information in Elasticsearch requires a different approach than searching for information in a traditional relational database, where the schema is predefined and all data is structured in tables with rows and columns. Elasticsearch provides a wide range of powerful query options that allow you to search for information in your data in ways that would be difficult or impossible with a traditional relational database. For example, you can use full-text search, fuzzy matching, phrase matching, and regular expressions to search for text data. You can use range queries and geo-queries to search for numerical and geographic data. You can use aggregations to perform statistical analysis on your data. And you can use highlighting and suggesters to provide more user-friendly search results. In addition, Elasticsearch is designed to be highly scalable and performant, which makes it an excellent choice for applications that require fast and efficient searching of large volumes of data. Elasticsearch can handle massive amounts of data and can be distributed across multiple nodes for even greater scalability and resilience. Overall, while querying in Elasticsearch may require a different approach than querying in a traditional relational database, the powerful query options and scalability that Elasticsearch provides make it a better choice for information searching and retrieval in many cases. _search endpoint The _search endpoint is Elasticsearch's primary endpoint for querying data. It allows you to search one or more indices and retrieve matching documents. The response includes a list of documents that match the query, along with metadata such as the relevance score and any aggregations that were requested. The main query types available in Elasticsearch are: Match query A match query retrieves documents that match a specific keyword or phrase. Here's an example: GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } } } Term query A term query retrieves documents that contain an exact term or phrase. Here's an example: GET receipe/_search { \"query\": { \"term\": { \"title.keyword\": \"Chocolate Cake\" } } } Range query A range query retrieves documents that contain a value within a specific range. Here's an example: GET receipe/_search { \"query\": { \"range\": { \"preparation_time_minutes\": { \"gte\": 60, \"lte\": 120 } } } } Boolean query A query that combines multiple sub-queries using Boolean operators such as AND, OR, and NOT. GET receipe/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"title\": \"chocolate cake\" } }, { \"range\": { \"preparation_time_minutes\": { \"gte\": 60, \"lte\": 120 } } } ] } } } Exists query An exists query retrieves documents that contain a specific field. Here's an example: GET receipe/_search { \"query\": { \"exists\": { \"field\": \"servings\" } } } Prefix query A prefix query retrieves documents that contain a specific prefix in a field. Here's an example: GET receipe/_search { \"query\": { \"prefix\": { \"title.keyword\": \"choc\" } } } Wildcard query A query that retrieves documents that match a specified wildcard pattern in a specified field. GET receipe/_search { \"query\": { \"wildcard\": { \"title.keyword\": \"Choc*\" } } } Regexp query: A query that retrieves documents that match a specified regular expression pattern in a specified field. Fuzzy query A query that retrieves documents that are similar to a specified search term, accounting for minor spelling errors and variations. GET receipe/_search { \"query\": { \"fuzzy\": { \"title\": { \"value\": \"choclete\", \"fuzziness\": 2 } } } } Match Phrase Prefix query A query that retrieves documents that contain a prefix of a specified phrase in a specified field GET receipe/_search { \"query\": { \"match_phrase_prefix\": { \"title\": \"chocolate c\" } } } Common Terms query A query that retrieves documents that contain common terms in a specified field, while filtering out terms that are too common. GET receipe/_search { \"query\": { \"match_phrase_prefix\": { \"title\": \"chocolate c\" } } } Query String query A query that allows you to use advanced search syntax to search for documents in one or more fields. GET receipe/_search { \"query\": { \"query_string\": { \"default_field\": \"title\", \"query\": \"chocolate AND cake\" } } } Match Phrase query A query that retrieves documents that contain a specific phrase in a specified field. GET receipe/_search { \"query\": { \"match_phrase\": { \"title\": \"chocolate cake\" } } } Match Boolean Prefix query A query that retrieves documents that contain a prefix of a specific phrase, using boolean logic to filter out unwanted results. GET receipe/_search { \"query\": { \"match_bool_prefix\": { \"description\": \"chocolate ca\", \"operator\": \"and\" } } } Terms query A query that retrieves documents that contain any of a set of specified terms in a specified field. GET receipe/_search { \"query\": { \"terms\": { \"ingredients.name.keyword\": [\"chocolate\", \"sugar\"] } } } Nested query A query that allows you to search within arrays of objects in a specific field. GET receipe/_search { \"query\": { \"nested\": { \"path\": \"ingredients\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"ingredients.name\": \"chocolate\" } }, { \"match\": { \"ingredients.quantity\": \"1 cup\" } } ] } } } } } Geo Distance query A query that retrieves documents that fall within a specified distance of a geographic location. GET some_geo_data_index/_search { \"query\": { \"bool\": { \"filter\": { \"geo_distance\": { \"distance\": \"50km\", \"location\": { \"lat\": 40.715, \"lon\": -74.011 } } } } } } More Like This query A query that retrieves documents that are similar to a specified document. GET receipe/_search { \"query\": { \"more_like_this\": { \"fields\": [\"title\", \"description\"], \"like\": [ { \"_index\": \"receipe\", \"_id\": \"9999\" } ], \"min_term_freq\": 1, \"min_doc_freq\": 1 } } } Script query A query that allows you to write custom scripts to search for documents. GET receipe/_search { \"query\": { \"script\": { \"script\": { \"source\": \"doc['preparation_time_minutes'].value > params.time\", \"params\": { \"time\": 60 } } } } } Highlighting A feature that allows you to highlight matching terms in your search results. GET receipe/_search { \"query\": { \"match\": { \"description\": \"chocolate\" } }, \"highlight\": { \"fields\": { \"description\": {} } } } Aggregations A feature that allows you to perform statistical analysis and grouping of your data. GET receipe/_search { \"aggs\": { \"group_by_ratings\": { \"terms\": { \"field\": \"ratings\" } } } } Sorting A feature that allows you to sort your search results by one or more fields. GET receipe/_search { \"sort\": [ { \"preparation_time_minutes\": \"asc\" }, { \"ratings\": \"desc\" } ], \"query\": { \"match\": { \"title\": \"cake\" } } } Relevance Score A score that indicates how well a document matches a query. GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } }, \"explain\": true } Suggesters A feature that allows you to provide suggestions for misspelled or incomplete search terms. GET receipe/_search { \"suggest\": { \"title-suggestion\": { \"text\": \"choclate cake\", \"term\": { \"field\": \"title\" } } } } Rescoring A feature that allows you to re-rank your search results using a different algorithm or set of parameters. GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } }, \"rescore\": { \"window_size\": 50, \"query\": { \"rescore_query\": { \"match\": { \"description\": \"chocolate\" } }, \"query_weight\": 0.7, \"rescore_query_weight\": 1.2 } } } These are the main query types available in Elasticsearch. It's important to consult the Elasticsearch documentation for more information and advanced query options. Aggregation in Elasticsearch Aggregations are a powerful feature of Elasticsearch that allow you to perform statistical analysis on your data. Aggregations can be used to compute metrics like counts, sums, averages, and histograms, as well as to group data into buckets based on a specified field or set of fields. Terms Aggregation A terms aggregation allows you to group your data into buckets based on a specified field. Here's an example of a terms aggregation that groups documents in the \"receipe\" index by their ratings field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_ratings\": { \"terms\": { \"field\": \"ratings\" } } } } This query will return a response that includes the number of documents that fall into each bucket like this : \"aggregations\" : { \"group_by_ratings\" : { \"buckets\" : [ { \"key\" : 3.5 , \"doc_count\" : 3 }, { \"key\" : 4.0 , \"doc_count\" : 2 }, { \"key\" : 5.0 , \"doc_count\" : 2 } ] } } Count Aggregation A count aggregation allows you to count the number of documents that match a specified query. Here's an example of a count aggregation that counts the number of documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"count\": { \"value_count\": { \"field\": \"_id\" } } } } This query will return a response that includes the total number of documents in the \"receipe\" index: \"aggregations\" : { \"count\" : { \"value\" : 7 } } Average Aggregation An average aggregation allows you to compute the average value of a specified field. Here's an example of an average aggregation that computes the average preparation_time_minutes for all documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"average_preparation_time\": { \"avg\": { \"field\": \"preparation_time_minutes\" } } } } This query will return a response that includes the average preparation_time_minutes for all documents in the \"receipe\" index : \"aggregations\" : { \"average_preparation_time\" : { \"value\" : 34.285714285714285 } } Max and Min Aggregation A max or min aggregation allows you to compute the maximum or minimum value of a specified field. Here's an example of a max aggregation that computes the maximum servings.max value for all documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"max_servings\": { \"max\": { \"field\": \"servings.max\" } } } } This query will return a response that includes the maximum servings.max value for all documents in the \"receipe\" index: \"aggregations\" : { \"max_servings\" : { \"value\" : 10 } } Similarly, you can use a min aggregation to compute the minimum value of a field. Date Histogram Aggregation A date_histogram aggregation allows you to group your data into buckets based on a specified date field. Here's an example of a date_histogram aggregation that groups documents in the \"receipe\" index by their created field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_created\": { \"date_histogram\": { \"field\": \"created\", \"interval\": \"month\" } } } } This query will return a response that includes the number of documents that fall into each bucket: \"aggregations\" : { \"group_by_created\" : { \"buckets\" : [ { \"key_as_string\" : \"2022-03-01T00:00:00.000Z\" , \"key\" : 1646064000000 , \"doc_count\" : 3 }, { \"key_as_string\" : \"2022-04-01T00:00:00.000Z\" , \"key\" : 1648742400000 , \"doc_count\" : 1 }, { \"key_as_string\" : \"2022-06-01T00:00:00.000Z\" , \"key\" : 1654022400000 , \"doc_count\" : 2 }, { \"key_as_string\" : \"2022-07-01T00:00:00.000Z\" , \"key\" : 1656691200000 , \"doc_count\" : 1 } ] } } This example uses a month interval to group documents by month . You can also use other intervals like day , hour , minute , second , and so on. Filter Aggregation A filter aggregation allows you to filter your data by a specified query before applying any other aggregations. Here's an example of a filter aggregation that only includes documents in the \"receipe\" index that have a ratings field greater than or equal to 4.0: GET receipe/_search { \"size\": 0, \"aggs\": { \"highly_rated\": { \"filter\": { \"range\": { \"ratings\": { \"gte\": 4.0 } } }, \"aggs\": { \"group_by_servings\": { \"terms\": { \"field\": \"servings.max\" } } } } } } This example uses a range filter to only include documents with a ratings field greater than or equal to 4.0. You can use other filters like match , bool , term , and so on. Nested Aggregation A nested aggregation allows you to perform aggregations on nested fields in your documents. Here's an example of a nested aggregation that groups documents in the \"receipe\" index by their ingredients.name field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_ingredient_name\": { \"nested\": { \"path\": \"ingredients\" }, \"aggs\": { \"group_by_name\": { \"terms\": { \"field\": \"ingredients.name\" } } } } } } Wrap-up Elasticsearch is a powerful search and analytics engine that allows you to store, search, and analyze large amounts of data quickly and in near real-time. Elasticsearch provides a rich set of querying and aggregating capabilities that allow you to search and analyze your data in many different ways. Queries in Elasticsearch are used to retrieve specific documents or sets of documents that match certain criteria. Elasticsearch supports a wide variety of queries, including term, match, range, wildcard, and fuzzy queries. You can also combine multiple queries using boolean logic and use filters to narrow down your search results. Aggregations in Elasticsearch are used to compute and summarize statistics about your data. Elasticsearch supports a wide variety of aggregations, including sum, avg, min, max, date histogram, terms, and nested aggregations. Aggregations allow you to group your data into buckets based on one or more fields and compute statistics like counts, averages, sums, and more. Together, queries and aggregations in Elasticsearch allow you to search and analyze your data in many different ways, giving you valuable insights into your data and helping you make better business decisions. Exercices Movies database Here some queries to practice on the movies database : Retrieve all Films titled \"Star Wars\" directed by \"George Lucas\" using boolean query. Retrieve all Films in which \"Harrison Ford\" played. Retrieve all Films in which \"Harrison Ford\" played and the plot contains \"Jones\". Retrieve all Films in which \"Harrison Ford\" played, the plot contains \"Jones\", but not the word \"Nazis\". Retrieve all Films directed by \"James Cameron\" with a rank below 1000 using boolean and range query. Retrieve all Films directed by \"James Cameron\" with a rank below 400. (Exact response: 2) Retrieve all Films directed by \"Quentin Tarantino\" with a rating above 5, but not categorized as an action or drama. Retrieve all Films directed by \"J.J. Abrams\" released between 2010 and 2015. Retrieve all Films with the word \"Star\" in the title and a rating above 7. Retrieve all Films with the word \"Trek\" in the title and a rating above 8 released after the year 2000. Receipe database Retrieve all documents in the index. Retrieve all documents in the index that have a preparation_time_minutes field greater than or equal to 60. Retrieve all documents in the index that have an ingredient with the name \"sugar\". Retrieve all documents in the index that have a servings.min field less than or equal to 4. Retrieve all documents in the index that have a ratings field greater than or equal to 4.5. Retrieve all documents in the index that have the word \"chicken\" in the title field. Retrieve all documents in the index that have the word \"vegetarian\" in the description field. Retrieve all documents in the index that have the word \"bake\" in the steps field. Retrieve all documents in the index that have a created field after January 1st, 2000. Retrieve all documents in the index that have an ingredient with the name \"flour\" and a servings.max field greater than or equal to 8. Compute the average preparation_time_minutes across all documents in the index. Group all documents in the index by the number of servings.min and compute the average preparation_time_minutes for each group. Compute the sum of preparation_time_minutes for all documents in the index that have the word \"chicken\" in the title field. Group all documents in the index by the servings.max field and compute the average ratings for each group. Compute the minimum and maximum preparation_time_minutes for all documents in the index that have an ingredient with the name \"sugar\". Account database Retrieve all documents in the index with a balance field greater than or equal to 1000. Retrieve all documents in the index with a gender field equal to \"female\". Retrieve all documents in the index with an age field between 30 and 40. Retrieve all documents in the index with a state field equal to \"California\". Retrieve all documents in the index with an email field containing the word \"gmail\". Retrieve all documents in the index with a registered field after January 1st, 2022. Retrieve all documents in the index with a tags field containing the value \"neque\". Retrieve all documents in the index with a phone field starting with the area code \"510\". Retrieve all documents in the index with a isActive field set to true. Compute the average balance across all documents in the index. Group all documents in the index by the gender field and compute the average balance for each group. Compute the sum of balance for all documents in the index with a state field equal to \"California\". Group all documents in the index by the age field and compute the average balance for each group. Compute the minimum and maximum balance for all documents in the index with an email field containing the word \"quility\". Orders database Retrieve all documents in the index. Retrieve all documents in the index with a total_amount field greater than or equal to 100. Retrieve all documents in the index with a status field equal to \"processed\". Retrieve all documents in the index with a salesman.name field containing the word \"Woodruff\". Retrieve all documents in the index with a sales_channel field equal to \"store\" and a total_amount field greater than 50. Compute the average total_amount across all documents in the index. Group all documents in the index by the sales_channel field and compute the sum of total_amount for each group. Compute the count of documents in the index with a status field equal to \"completed\". Group all documents in the index by the salesman.name field and compute the average total_amount for each group. Compute the minimum and maximum total_amount for all documents in the index with a purchased_at field in the year 2016.","title":"Elasticseach"},{"location":"nosql/elastic/#introduction-to-elasticsearch","text":"Elasticsearch is an open-source, document-based NoSQL database that is designed for full-text search and analytics. Elasticsearch is built on top of the Lucene search engine and is optimized for fast search and retrieval of unstructured data .","title":"Introduction to Elasticsearch"},{"location":"nosql/elastic/#elastic-architecture-introduction","text":"Elasticsearch is a distributed, document-oriented NoSQL database that is optimized for search and analytics. It is built on top of the Lucene search engine and is designed to scale horizontally across multiple nodes and clusters.","title":"Elastic architecture introduction"},{"location":"nosql/elastic/#cluster-nodes-and-replicas","text":"Nodes are individual instances of Elasticsearch that are part of a cluster. Each node stores a subset of the data in the cluster and is responsible for processing search and indexing requests for that data. Nodes can be added or removed from a cluster dynamically, allowing for easy scalability and fault tolerance. A cluster is a collection of nodes that work together to store and process data. Elasticsearch clusters are designed to be highly available and fault tolerant, with built-in features for data replication and failover. Clusters can scale horizontally by adding more nodes to the cluster, allowing for increased processing power and storage capacity. Sharding is the process of partitioning data across multiple nodes in a cluster. When a document is indexed in Elasticsearch, it is assigned to a specific shard based on a hashing algorithm that takes into account the document's ID. By default, each index in Elasticsearch is divided into five primary shards, with each shard having one or more replicas. This allows Elasticsearch to distribute the workload across multiple nodes, improving search and indexing performance. The number of shards and replicas can be configured for each index based on the size and search requirements of the data. For example, a large index with a high write throughput might require more primary shards to distribute the data more evenly across nodes, while a smaller index with a lower write throughput might require fewer primary shards to reduce the overhead of managing multiple shards.","title":"Cluster, nodes and replicas"},{"location":"nosql/elastic/#index","text":"An index is like a database in a traditional relational database system. It is a logical container for one or more documents that share a similar structure and are stored together. When you create an index in Elasticsearch, you can specify the mapping for the fields that the documents in the index will contain. The mapping defines the data types and formats for the fields, which allows Elasticsearch to index and search the data more efficiently. You can also configure the number of primary shards that the index should have, which determines how the data in the index is distributed across nodes in the Elasticsearch cluster. This allows Elasticsearch to scale horizontally as the size of the index grows. In addition, you can configure the number of replica shards for the index, which provide redundancy and allow for failover in case a primary shard becomes unavailable. Overall, an index in Elasticsearch is a distributed, document-oriented NoSQL database that uses nodes, clusters, and sharding to provide scalability, fault tolerance, high availability and provides a way to organize and search data efficiently.","title":"Index"},{"location":"nosql/elastic/#overall-architecture","text":"The diagram shows a simplified architecture for an Elasticsearch cluster with three nodes. Each node is represented by a rectangular box and is labeled with its unique node name, IP address, and port number. The nodes are connected to each other through a network, represented by the gray lines connecting the boxes. This network allows the nodes to communicate with each other and share data. The Elasticsearch cluster is managed by a master node, which is responsible for coordinating the cluster and maintaining its state. In the diagram, the master node is indicated by the green box labeled \"Master-eligible node.\" The other nodes in the cluster are known as data nodes, and they are responsible for storing and indexing the data. In the diagram, the data nodes are indicated by the yellow boxes labeled \"Data node.\" Each data node stores a subset of the data in the cluster, and the data is distributed across the nodes using a technique called sharding. Each shard represents a portion of the data and is stored on a separate data node. The client nodes, represented by the blue boxes labeled \"Client node,\" are used to interact with the cluster and submit search and indexing requests. Client nodes do not store any data themselves, but they communicate with the data nodes to retrieve and manipulate the data. Finally, the external world is represented by the orange box labeled \"External clients,\" which can be any application or user that needs to interact with the Elasticsearch cluster. Overall, the architecture diagram shows how an Elasticsearch cluster is composed of multiple nodes working together to store, index, and search data efficiently.","title":"Overall architecture"},{"location":"nosql/elastic/#installation-with-docker","text":"To install Elasticsearch, we can use Docker, a containerization platform that simplifies the process of installing and running software applications. Install Docker on your machine if you haven't already. Open a command prompt and run the following command to download and run the Elasticsearch Docker image: docker run -p 9200 :9200 -p 9300 :9300 -d -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.14.0 docker run : This command tells Docker to run a container from an image. -p 9200:9200: This option maps port 9200 in the container to port 9200 on the host machine, allowing us to access Elasticsearch on port - 9200 from outside the container. -p 9300:9300: This option maps port 9300 in the container to port 9300 on the host machine, allowing nodes in the Elasticsearch cluster to communicate with each other on port 9300. -d : This option runs the container in detached mode, which means it runs in the background and doesn't attach to the terminal. This allows us to continue using the terminal while the container is running. -e \"discovery.type=single-node\": This option sets an environment variable in the container called discovery.type to single-node. This tells Elasticsearch to start as a single-node cluster, which is useful for testing or development purposes. docker.elastic.co/elasticsearch/elasticsearch:7.14.0 : This is the name and version of the Elasticsearch image we want to run. When you run this command, Docker will download the Elasticsearch image from Docker Hub (unless it's already downloaded), create a container from the image, and start Elasticsearch running inside the container. The container will be accessible on port 9200 and 9300, and Elasticsearch will be running as a single-node cluster \ud83e\udd73 For more information about docker installation you can check the official documention here","title":"Installation with Docker"},{"location":"nosql/elastic/#test-your-installation","text":"","title":"Test your installation"},{"location":"nosql/elastic/#cluster-info","text":"You can test if the elasticsearch container is running by run this command into your terminal (the jq command is here for a better return format, if you don't have it insall it here ) : curl 0 .0.0.0:9200/_cluster/health | jq You should see this : % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 390 100 390 0 0 308 0 0 :00:01 0 :00:01 --:--:-- 308 { \"cluster_name\" : \"docker-cluster\" , \"status\" : \"green\" , \"timed_out\" : false, \"number_of_nodes\" : 1 , \"number_of_data_nodes\" : 1 , \"active_primary_shards\" : 1 , \"active_shards\" : 1 , \"relocating_shards\" : 0 , \"initializing_shards\" : 0 , \"unassigned_shards\" : 0 , \"delayed_unassigned_shards\" : 0 , \"number_of_pending_tasks\" : 0 , \"number_of_in_flight_fetch\" : 0 , \"task_max_waiting_in_queue_millis\" : 0 , \"active_shards_percent_as_number\" : 100 } This command returns information about the overall health of the Elasticsearch cluster. It provides information about the number of nodes in the cluster, the status of each node, the number of shards and replicas, and the overall status of the cluster. The response includes a variety of metrics such as the number of unassigned shards, the number of active and inactive primary shards, and the status of the cluster's overall health.","title":"Cluster info"},{"location":"nosql/elastic/#nodes-info","text":"curl -X GET \"http://0.0.0.0:9200/_cat/nodes?v\" This command returns information about the nodes in the Elasticsearch cluster. It provides a summary of each node, including its IP address, node ID, and whether it is currently active or not. The response also includes a variety of metrics such as the number of open file descriptors, the amount of disk space used, and the amount of heap memory used.","title":"Nodes info"},{"location":"nosql/elastic/#data-modeling-in-elasticsearch","text":"Elasticsearch stores data as JSON-like documents, which can be nested and hierarchical. The data model in Elasticsearch is flexible, allowing for easy changes to the schema. Documents are the primary storage structure in Elasticsearch. Each document contains reserved fields (the document's metadata), such as : _index : where the document resides _type : the type of document it represents (database) _id : a unique identifier for the document _source : the data in the form of a dictionary","title":"Data Modeling in Elasticsearch"},{"location":"nosql/elastic/#json-format-overview","text":"JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate. It consists of key-value pairs, with each pair separated by a comma and enclosed in curly braces. Here's an example of a simple JSON document: { \"name\" : \"John Smith\" , \"age\" : 35 , \"city\" : \"New York\" } In Elasticsearch, JSON is used as the primary format for documents that are stored and indexed in the database. Each document is represented as a JSON object, with fields that describe the properties of the document.","title":"json format overview"},{"location":"nosql/elastic/#create-an-index-and-insert-data","text":"Remember, an index is like a database in a traditional relational database system. We can create a specific index, let's say cities and give it a certain settings like 2 shards and 2 replicas per shards with the following command : curl -XPUT 'http://localhost:9200/cities' -H 'Content-Type: application/json' -d ' { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 } }' you should see this response : {\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"my_index\"} To retrieve the index and verify its settings: curl -XGET 'http://localhost:9200/my_index/_settings' | jq you should see this output : { \"cities\" : { \"settings\" : { \"index\" : { \"routing\" : { \"allocation\" : { \"include\" : { \"_tier_preference\" : \"data_content\" } } } , \"number_of_shards\" : \"2\" , \"provided_name\" : \"cities\" , \"creation_date\" : \"1678636556321\" , \"number_of_replicas\" : \"2\" , \"uuid\" : \"vsqBEmHWSBaki2AL-oClsA\" , \"version\" : { \"created\" : \"7110199\" } } } } } Now let's populate this index by creating our first document by running into our terminal : curl -XPOST 'http://localhost:9200/cities/_doc' -H 'Content-Type: application/json' -d ' { \"city\": \"London\", \"country\": \"England\" }' You should see the following response confirmation : { \"_index\" : \"cities\" , \"_type\" : \"_doc\" , \"_id\" : \"pquR1oYBQIvdICuNRLuD\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 3 , \"successful\" : 1 , \"failed\" : 0 } , \"_seq_no\" : 1 , \"_primary_term\" : 1 } That's indicate the document is been created and inserted into our index with an unique id pquR1oYBQIvdICuNRLuD \ud83e\udd13 You can verify this by running this command : curl -XGET 'http://localhost:9200/cities/_doc/{document_id}' You should see : { \"_index\" : \"cities\" , \"_type\" : \"_doc\" , \"_id\" : \"pquR1oYBQIvdICuNRLuD\" , \"_version\" :1, \"_seq_no\" :1, \"_primary_term\" :1, \"found\" :true \"_source\" : { \"city\" : \"London\" , \"country\" : \"England\" } } As you can see, our document content is in the _source field, we will explain more the particular return format of elasticsearch.","title":"Create an index and insert data"},{"location":"nosql/elastic/#indexing-data-in-elasticsearch","text":"","title":"Indexing data in Elasticsearch"},{"location":"nosql/elastic/#create-index-from-json-file","text":"Let's create a bash script in order to insert few indices from json files and play with them. First download the json files here and run the following bash script into your terminal : insert_data.sh curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/receipe/_bulk --data-binary \"@receipe.json\" && \\ printf \"\\n\u2705 Insertion receipe index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/accounts/docs/_bulk --data-binary \"@accounts.json\" printf \"\\n\u2705 Insertion accounts index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/movies/_bulk --data-binary \"@movies.json\" printf \"\\n\u2705 Insertion movies index to elastic node OK \u2705 \" curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/products/_bulk --data-binary \"@products.json\" printf \"\\n\u2705 Insertion products index to elastic node OK \u2705 \" curl : a command-line tool for sending HTTP requests and receiving HTTP responses from a server. -s : a flag that tells curl to operate silently, i.e., not to show the progress meter or any error messages. -H \"Content-Type: application/x-ndjson\": a header that sets the content type of the request to application/x-ndjson. This tells - Elasticsearch that the data being sent in the request body is in NDJSON format. -XPOST : a flag that tells curl to send a POST request to the specified URL. localhost:9200/receipe/_bulk : the URL of the Elasticsearch endpoint to which the request is being sent. In this case, it's the _bulk API for the receipe index on the local Elasticsearch instance running on port 9200. --data-binary \"@receipe.json\" : the request body, which is specified as a binary data file (@ symbol followed by the filename in this case receipe) in NDJSON format. The --data-binary flag tells curl to send the data as is, without any special interpretation. Overall, this command is sending a bulk request to Elasticsearch to index data contained in the receipe.json file into the receipe index. The data is in NDJSON format, which is a format that Elasticsearch can parse and process efficiently.","title":"Create index from json file"},{"location":"nosql/elastic/#working-with-kibana-interface","text":"For the rest of this section we will be working with kibana dev tools graphic user interface (GUI). You can run an elastic single node cluster and kibana GUI with the following docker-compose file : docker-compose.yml version : '2.2' services : elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.11.1 container_name : elasticsearch restart : always environment : - xpack.security.enabled=false - discovery.type=single-node ulimits : memlock : soft : -1 hard : -1 nofile : soft : 65536 hard : 65536 cap_add : - IPC_LOCK volumes : - ./elas1:/usr/share/elasticsearch/data ports : - 9200:9200 - 9300:9300 networks : - esnet kibana : container_name : kibana image : docker.elastic.co/kibana/kibana:7.11.1 restart : always ports : - 5601:5601 depends_on : - elasticsearch networks : - esnet networks : esnet : driver : bridge This is a Docker Compose file that defines two services: Elasticsearch and Kibana. The elasticsearch service uses the official Elasticsearch Docker image version 7.11.1. It sets the container name to elasticsearch and ensures that the container is always restarted if it stops or crashes. It disables X-Pack security and configures Elasticsearch to run as a single-node cluster. It also sets ulimit and cap_add settings to ensure that Elasticsearch has sufficient resources. Finally, it maps the ./elas1 directory on the host to /usr/share/elasticsearch/data inside the container, and exposes ports 9200 and 9300 to allow external access to Elasticsearch. The kibana service uses the official Kibana Docker image version 7.11.1. It sets the container name to kibana and ensures that the container is always restarted if it stops or crashes. It exposes port 5601 to allow external access to Kibana. It also depends on the Elasticsearch service, so it won't start until Elasticsearch is up and running. Overall, Kibana is a web-based user interface for Elasticsearch that allows you to interact with Elasticsearch data, run queries, and visualize data in various ways. One of the main advantages of using Kibana is that it provides a user-friendly interface for Elasticsearch, making it easier to explore and analyze data without needing to write complex queries. It should take a few seconds to launch the installation and then you can go to : http://localhost:5601/app/dev_tools#/console in your browser to see the dev tool console \ud83e\udd13 You can know verify all our indicies by running the following command : GET /_cat/in dices?v","title":"Working with Kibana interface"},{"location":"nosql/elastic/#mapping","text":"An index mapping in Elasticsearch is a way to define the structure of the documents that will be stored in an index. It specifies the fields that will be part of each document, along with their data types, properties, and settings. When you create an index in Elasticsearch, you can either provide an explicit mapping or let Elasticsearch infer the mapping automatically based on the first document that is indexed. However, it is generally recommended to define an explicit mapping to ensure that the index has a consistent structure and to avoid unexpected field types or mappings. An index mapping consists of two main components: field mappings and index settings. Field mappings define the fields that will be part of each document in the index, along with their data types and properties. For example, a field mapping can define a string field that will store text data, or a numeric field that will store integer or float values. Field mappings can also specify additional settings such as the analyzer to use for text fields, or the format to use for date fields. Index settings define various aspects of the index, such as the number of shards and replicas, the analysis settings, and the index lifecycle policies. For example, you can specify the number of primary and replica shards that the index will use, or the analyzer to use for text fields in the index. Overall, an index mapping is an essential component of an Elasticsearch index that allows you to define the structure of the documents that will be stored in the index, along with various settings and configurations that affect the index behavior.","title":"Mapping"},{"location":"nosql/elastic/#get-index-mapping","text":"Let's take a look of our receipe index by tapping this into our kibana dev tool console : GET /receipe/_mapping You should see this output : { \"receipe\" : { \"mappings\" : { \"properties\" : { \"created\" : { \"type\" : \"date\" , \"format\" : \"yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis\" } , \"description\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"ingredients\" : { \"properties\" : { \"name\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"quantity\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } , \"preparation_time_minutes\" : { \"type\" : \"long\" } , \"ratings\" : { \"type\" : \"float\" } , \"servings\" : { \"properties\" : { \"max\" : { \"type\" : \"long\" } , \"min\" : { \"type\" : \"long\" } } } , \"steps\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } , \"title\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } } } I know it is long but it is very explicit, it shows the structure of the receipe.json document, which has been indexed in Elasticsearch. Here's a breakdown of what each section of the mapping represents: The mapping is defined for the receipe index, which contains a single document type. The properties section contains all the fields that are part of each document in the index. In this case, the receipe.json document has 7 fields: created, description, ingredients, preparation_time_minutes, ratings, servings, and steps. Each field in the properties section has a specific data type and additional properties that define how the data is indexed and stored. Here are the properties for each field : created : a date field that can be parsed in the formats \"yyyy/MM/dd HH:mm:ss\", \"yyyy/MM/dd\", or as epoch milliseconds. description : a text field that allows full-text search and has an additional keyword sub-field that can be used for exact matching. ingredients : an object field that contains two sub-fields: name and quantity. Both sub-fields are text fields with an additional keyword sub-field. preparation_time_minutes : a long field that stores the preparation time for the recipe in minutes. ratings : a float field that stores the average rating for the recipe. servings : an object field that contains two sub-fields: min and max. Both sub-fields are long fields that represent the minimum and maximum servings for the recipe. steps : a text field that allows full-text search and has an additional keyword sub-field that can be used for exact matching. Overall, this mapping provides a detailed description of the structure of the receipe.json document, which allows Elasticsearch to index and search the data efficiently. It also provides additional settings and properties that can be used to customize the behavior of the index and optimize its performance. You can try to print the mapping for the other json documents \ud83e\udd13","title":"Get index mapping"},{"location":"nosql/elastic/#first-query","text":"Let's do our first query on the movies index, go to the kibana dev tool interface and copy this query : GET movies / _search { \"query\" : { \"match_all\" : {} } } This query retrieves all documents from the movies index in Elasticsearch. It uses the match_all query to match all documents in the index, which is equivalent to a SELECT * statement in SQL. GET movies/_search indicates that we want to execute a search request on the movies index. The _search endpoint is used to search for documents in Elasticsearch we will see this later. \"query\": { \"match_all\": {} } is the query definition. In this case, we're using the match_all query, which matches all documents in the index. The empty object {} inside the match_all query means that we're not applying any filters or constraints to the search results. As you can notice elatic has a strange return format. Let's take a look at this return :","title":"First query"},{"location":"nosql/elastic/#crud-operations-in-elasticsearch","text":"Elasticsearch supports CRUD (Create, Read, Update, and Delete) operations for manipulating data. Here are some examples of how to perform CRUD operations in Elasticsearch with our json documents :","title":"CRUD Operations in Elasticsearch"},{"location":"nosql/elastic/#create-documents","text":"To create a new document in a given index, let's say the receipe index for example, use the following query : POST receipe / _doc { \"created\" : \"2022/03/12 12:00:00\" , \"title\" : \"Chocolate Cake\" , \"description\" : \"A rich and decadent chocolate cake recipe\" , \"preparation_time_minutes\" : 60 , \"servings\" : { \"min\" : 8 , \"max\" : 10 }, \"ingredients\" : [ { \"name\" : \"flour\" , \"quantity\" : \"2 cups\" }, { \"name\" : \"sugar\" , \"quantity\" : \"2 cups\" }, { \"name\" : \"cocoa powder\" , \"quantity\" : \"3/4 cup\" }, { \"name\" : \"baking powder\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"baking soda\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"salt\" , \"quantity\" : \"1 teaspoon\" }, { \"name\" : \"buttermilk\" , \"quantity\" : \"1 cup\" }, { \"name\" : \"vegetable oil\" , \"quantity\" : \"1/2 cup\" }, { \"name\" : \"eggs\" , \"quantity\" : \"2\" }, { \"name\" : \"vanilla extract\" , \"quantity\" : \"2 teaspoons\" }, { \"name\" : \"boiling water\" , \"quantity\" : \"1 cup\" } ] , \"steps\" : \"1. Preheat oven to 350 degrees F (175 degrees C). Grease and flour two 9-inch round cake pans. \\n 2. In a large mixing bowl, combine the flour, sugar, cocoa powder, baking powder, baking soda, and salt. Mix well. \\n 3. Add the buttermilk, vegetable oil, eggs, and vanilla extract. Beat with an electric mixer on medium speed for 2 minutes. \\n 4. Stir in the boiling water (the batter will be thin). Pour the batter into the prepared pans. \\n 5. Bake for 30 to 35 minutes, or until a toothpick inserted into the center of the cakes comes out clean. \\n 6. Allow the cakes to cool in the pans for 10 minutes, then remove them from the pans and cool completely on wire racks. \\n 7. Frost and decorate the cakes as desired.\" } You should see this return : { \"_index\" : \"receipe\" , \"_type\" : \"_doc\" , \"_id\" : \"p6va1oYBQIvdICuNjLvj\" , \"_version\" : 1 , \"result\" : \"created\" , \"_shards\" : { \"total\" : 2 , \"successful\" : 1 , \"failed\" : 0 } , \"_seq_no\" : 42 , \"_primary_term\" : 13 } With details, it means : _index : The name of the index where the document was created, which in this case is \"receipe\". _type : The document type, which is \"_doc\" by default in Elasticsearch 7.x and later. _id : The unique ID of the newly created document. Elasticsearch automatically generates an ID if you don't provide one explicitly for our case \"_id\" : \"p6va1oYBQIvdICuNjLvj\" _version : The version of the document after the create operation. The initial version is always 1. result: The result of the create operation, which is \"created\" in this case. This indicates that a new document was created. _shards : The number of shards involved in the create operation and the number of successful and failed shards. In this case, the create operation involved two shards and was successful on one shard. _seq_no and _primary_term : These values are used internally by Elasticsearch to manage replication and consistency. They are not relevant for most users. Overall, this response confirms that the document was created successfully in the \"receipe\" index with a unique ID, version 1, and one successful shard. If we want to create a document with a particular ID just modify the first line of the request by : POST receipe / _doc / 9999 It will create a \"receipe\" document with an ID of 9999 .","title":"Create documents"},{"location":"nosql/elastic/#read-documents","text":"Like we have seen before you can run : GET index / _doc / { document_id }","title":"Read documents"},{"location":"nosql/elastic/#update-documents","text":"To update a document in the index, use the following query: POST receipe / _update / { document_id } { \"doc\" : { \"description\" : \"A rich and decadent chocolate cake recipe with layers of buttercream frosting\" } } Replace {document_id} with the ID of the document you want to update. This query updates the description field.","title":"Update documents"},{"location":"nosql/elastic/#delete-documents","text":"To delete a document from the index, use the following query: DELETE receipe / _doc / { document_id } Replace {document_id} with the ID of the document you want to delete. This query deletes the specified document from the index. That's it for the CRUD operations! These are the basic operations you can perform on documents in Elasticsearch. There are many other advanced features and queries you can use to search, analyze, and visualize your data !","title":"Delete documents"},{"location":"nosql/elastic/#querying-elasticsearch-data","text":"Querying in Elasticsearch can seem more complex than a traditional approach like SQL because Elasticsearch is designed to handle unstructured and semi-structured data, whereas traditional databases like SQL are designed to handle structured data. In Elasticsearch, documents are stored as JSON objects, and each document can have different fields with different data types. This means that searching for information in Elasticsearch requires a different approach than searching for information in a traditional relational database, where the schema is predefined and all data is structured in tables with rows and columns. Elasticsearch provides a wide range of powerful query options that allow you to search for information in your data in ways that would be difficult or impossible with a traditional relational database. For example, you can use full-text search, fuzzy matching, phrase matching, and regular expressions to search for text data. You can use range queries and geo-queries to search for numerical and geographic data. You can use aggregations to perform statistical analysis on your data. And you can use highlighting and suggesters to provide more user-friendly search results. In addition, Elasticsearch is designed to be highly scalable and performant, which makes it an excellent choice for applications that require fast and efficient searching of large volumes of data. Elasticsearch can handle massive amounts of data and can be distributed across multiple nodes for even greater scalability and resilience. Overall, while querying in Elasticsearch may require a different approach than querying in a traditional relational database, the powerful query options and scalability that Elasticsearch provides make it a better choice for information searching and retrieval in many cases.","title":"Querying Elasticsearch data"},{"location":"nosql/elastic/#_search-endpoint","text":"The _search endpoint is Elasticsearch's primary endpoint for querying data. It allows you to search one or more indices and retrieve matching documents. The response includes a list of documents that match the query, along with metadata such as the relevance score and any aggregations that were requested. The main query types available in Elasticsearch are:","title":"_search endpoint"},{"location":"nosql/elastic/#match-query","text":"A match query retrieves documents that match a specific keyword or phrase. Here's an example: GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } } }","title":"Match query"},{"location":"nosql/elastic/#term-query","text":"A term query retrieves documents that contain an exact term or phrase. Here's an example: GET receipe/_search { \"query\": { \"term\": { \"title.keyword\": \"Chocolate Cake\" } } }","title":"Term query"},{"location":"nosql/elastic/#range-query","text":"A range query retrieves documents that contain a value within a specific range. Here's an example: GET receipe/_search { \"query\": { \"range\": { \"preparation_time_minutes\": { \"gte\": 60, \"lte\": 120 } } } }","title":"Range query"},{"location":"nosql/elastic/#boolean-query","text":"A query that combines multiple sub-queries using Boolean operators such as AND, OR, and NOT. GET receipe/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"title\": \"chocolate cake\" } }, { \"range\": { \"preparation_time_minutes\": { \"gte\": 60, \"lte\": 120 } } } ] } } }","title":"Boolean query"},{"location":"nosql/elastic/#exists-query","text":"An exists query retrieves documents that contain a specific field. Here's an example: GET receipe/_search { \"query\": { \"exists\": { \"field\": \"servings\" } } }","title":"Exists query"},{"location":"nosql/elastic/#prefix-query","text":"A prefix query retrieves documents that contain a specific prefix in a field. Here's an example: GET receipe/_search { \"query\": { \"prefix\": { \"title.keyword\": \"choc\" } } }","title":"Prefix query"},{"location":"nosql/elastic/#wildcard-query","text":"A query that retrieves documents that match a specified wildcard pattern in a specified field. GET receipe/_search { \"query\": { \"wildcard\": { \"title.keyword\": \"Choc*\" } } }","title":"Wildcard query"},{"location":"nosql/elastic/#regexp-query-a-query-that-retrieves-documents-that-match-a-specified-regular-expression-pattern-in-a-specified-field","text":"","title":"Regexp query: A query that retrieves documents that match a specified regular expression pattern in a specified field."},{"location":"nosql/elastic/#fuzzy-query","text":"A query that retrieves documents that are similar to a specified search term, accounting for minor spelling errors and variations. GET receipe/_search { \"query\": { \"fuzzy\": { \"title\": { \"value\": \"choclete\", \"fuzziness\": 2 } } } }","title":"Fuzzy query"},{"location":"nosql/elastic/#match-phrase-prefix-query","text":"A query that retrieves documents that contain a prefix of a specified phrase in a specified field GET receipe/_search { \"query\": { \"match_phrase_prefix\": { \"title\": \"chocolate c\" } } }","title":"Match Phrase Prefix query"},{"location":"nosql/elastic/#common-terms-query","text":"A query that retrieves documents that contain common terms in a specified field, while filtering out terms that are too common. GET receipe/_search { \"query\": { \"match_phrase_prefix\": { \"title\": \"chocolate c\" } } }","title":"Common Terms query"},{"location":"nosql/elastic/#query-string-query","text":"A query that allows you to use advanced search syntax to search for documents in one or more fields. GET receipe/_search { \"query\": { \"query_string\": { \"default_field\": \"title\", \"query\": \"chocolate AND cake\" } } }","title":"Query String query"},{"location":"nosql/elastic/#match-phrase-query","text":"A query that retrieves documents that contain a specific phrase in a specified field. GET receipe/_search { \"query\": { \"match_phrase\": { \"title\": \"chocolate cake\" } } }","title":"Match Phrase query"},{"location":"nosql/elastic/#match-boolean-prefix-query","text":"A query that retrieves documents that contain a prefix of a specific phrase, using boolean logic to filter out unwanted results. GET receipe/_search { \"query\": { \"match_bool_prefix\": { \"description\": \"chocolate ca\", \"operator\": \"and\" } } }","title":"Match Boolean Prefix query"},{"location":"nosql/elastic/#terms-query","text":"A query that retrieves documents that contain any of a set of specified terms in a specified field. GET receipe/_search { \"query\": { \"terms\": { \"ingredients.name.keyword\": [\"chocolate\", \"sugar\"] } } }","title":"Terms query"},{"location":"nosql/elastic/#nested-query","text":"A query that allows you to search within arrays of objects in a specific field. GET receipe/_search { \"query\": { \"nested\": { \"path\": \"ingredients\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"ingredients.name\": \"chocolate\" } }, { \"match\": { \"ingredients.quantity\": \"1 cup\" } } ] } } } } }","title":"Nested query"},{"location":"nosql/elastic/#geo-distance-query","text":"A query that retrieves documents that fall within a specified distance of a geographic location. GET some_geo_data_index/_search { \"query\": { \"bool\": { \"filter\": { \"geo_distance\": { \"distance\": \"50km\", \"location\": { \"lat\": 40.715, \"lon\": -74.011 } } } } } }","title":"Geo Distance query"},{"location":"nosql/elastic/#more-like-this-query","text":"A query that retrieves documents that are similar to a specified document. GET receipe/_search { \"query\": { \"more_like_this\": { \"fields\": [\"title\", \"description\"], \"like\": [ { \"_index\": \"receipe\", \"_id\": \"9999\" } ], \"min_term_freq\": 1, \"min_doc_freq\": 1 } } }","title":"More Like This query"},{"location":"nosql/elastic/#script-query","text":"A query that allows you to write custom scripts to search for documents. GET receipe/_search { \"query\": { \"script\": { \"script\": { \"source\": \"doc['preparation_time_minutes'].value > params.time\", \"params\": { \"time\": 60 } } } } }","title":"Script query"},{"location":"nosql/elastic/#highlighting","text":"A feature that allows you to highlight matching terms in your search results. GET receipe/_search { \"query\": { \"match\": { \"description\": \"chocolate\" } }, \"highlight\": { \"fields\": { \"description\": {} } } }","title":"Highlighting"},{"location":"nosql/elastic/#aggregations","text":"A feature that allows you to perform statistical analysis and grouping of your data. GET receipe/_search { \"aggs\": { \"group_by_ratings\": { \"terms\": { \"field\": \"ratings\" } } } }","title":"Aggregations"},{"location":"nosql/elastic/#sorting","text":"A feature that allows you to sort your search results by one or more fields. GET receipe/_search { \"sort\": [ { \"preparation_time_minutes\": \"asc\" }, { \"ratings\": \"desc\" } ], \"query\": { \"match\": { \"title\": \"cake\" } } }","title":"Sorting"},{"location":"nosql/elastic/#relevance-score","text":"A score that indicates how well a document matches a query. GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } }, \"explain\": true }","title":"Relevance Score"},{"location":"nosql/elastic/#suggesters","text":"A feature that allows you to provide suggestions for misspelled or incomplete search terms. GET receipe/_search { \"suggest\": { \"title-suggestion\": { \"text\": \"choclate cake\", \"term\": { \"field\": \"title\" } } } }","title":"Suggesters"},{"location":"nosql/elastic/#rescoring","text":"A feature that allows you to re-rank your search results using a different algorithm or set of parameters. GET receipe/_search { \"query\": { \"match\": { \"title\": \"chocolate cake\" } }, \"rescore\": { \"window_size\": 50, \"query\": { \"rescore_query\": { \"match\": { \"description\": \"chocolate\" } }, \"query_weight\": 0.7, \"rescore_query_weight\": 1.2 } } } These are the main query types available in Elasticsearch. It's important to consult the Elasticsearch documentation for more information and advanced query options.","title":"Rescoring"},{"location":"nosql/elastic/#aggregation-in-elasticsearch","text":"Aggregations are a powerful feature of Elasticsearch that allow you to perform statistical analysis on your data. Aggregations can be used to compute metrics like counts, sums, averages, and histograms, as well as to group data into buckets based on a specified field or set of fields.","title":"Aggregation in Elasticsearch"},{"location":"nosql/elastic/#terms-aggregation","text":"A terms aggregation allows you to group your data into buckets based on a specified field. Here's an example of a terms aggregation that groups documents in the \"receipe\" index by their ratings field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_ratings\": { \"terms\": { \"field\": \"ratings\" } } } } This query will return a response that includes the number of documents that fall into each bucket like this : \"aggregations\" : { \"group_by_ratings\" : { \"buckets\" : [ { \"key\" : 3.5 , \"doc_count\" : 3 }, { \"key\" : 4.0 , \"doc_count\" : 2 }, { \"key\" : 5.0 , \"doc_count\" : 2 } ] } }","title":"Terms Aggregation"},{"location":"nosql/elastic/#count-aggregation","text":"A count aggregation allows you to count the number of documents that match a specified query. Here's an example of a count aggregation that counts the number of documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"count\": { \"value_count\": { \"field\": \"_id\" } } } } This query will return a response that includes the total number of documents in the \"receipe\" index: \"aggregations\" : { \"count\" : { \"value\" : 7 } }","title":"Count Aggregation"},{"location":"nosql/elastic/#average-aggregation","text":"An average aggregation allows you to compute the average value of a specified field. Here's an example of an average aggregation that computes the average preparation_time_minutes for all documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"average_preparation_time\": { \"avg\": { \"field\": \"preparation_time_minutes\" } } } } This query will return a response that includes the average preparation_time_minutes for all documents in the \"receipe\" index : \"aggregations\" : { \"average_preparation_time\" : { \"value\" : 34.285714285714285 } }","title":"Average Aggregation"},{"location":"nosql/elastic/#max-and-min-aggregation","text":"A max or min aggregation allows you to compute the maximum or minimum value of a specified field. Here's an example of a max aggregation that computes the maximum servings.max value for all documents in the \"receipe\" index: GET receipe/_search { \"size\": 0, \"aggs\": { \"max_servings\": { \"max\": { \"field\": \"servings.max\" } } } } This query will return a response that includes the maximum servings.max value for all documents in the \"receipe\" index: \"aggregations\" : { \"max_servings\" : { \"value\" : 10 } } Similarly, you can use a min aggregation to compute the minimum value of a field.","title":"Max and Min Aggregation"},{"location":"nosql/elastic/#date-histogram-aggregation","text":"A date_histogram aggregation allows you to group your data into buckets based on a specified date field. Here's an example of a date_histogram aggregation that groups documents in the \"receipe\" index by their created field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_created\": { \"date_histogram\": { \"field\": \"created\", \"interval\": \"month\" } } } } This query will return a response that includes the number of documents that fall into each bucket: \"aggregations\" : { \"group_by_created\" : { \"buckets\" : [ { \"key_as_string\" : \"2022-03-01T00:00:00.000Z\" , \"key\" : 1646064000000 , \"doc_count\" : 3 }, { \"key_as_string\" : \"2022-04-01T00:00:00.000Z\" , \"key\" : 1648742400000 , \"doc_count\" : 1 }, { \"key_as_string\" : \"2022-06-01T00:00:00.000Z\" , \"key\" : 1654022400000 , \"doc_count\" : 2 }, { \"key_as_string\" : \"2022-07-01T00:00:00.000Z\" , \"key\" : 1656691200000 , \"doc_count\" : 1 } ] } } This example uses a month interval to group documents by month . You can also use other intervals like day , hour , minute , second , and so on.","title":"Date Histogram Aggregation"},{"location":"nosql/elastic/#filter-aggregation","text":"A filter aggregation allows you to filter your data by a specified query before applying any other aggregations. Here's an example of a filter aggregation that only includes documents in the \"receipe\" index that have a ratings field greater than or equal to 4.0: GET receipe/_search { \"size\": 0, \"aggs\": { \"highly_rated\": { \"filter\": { \"range\": { \"ratings\": { \"gte\": 4.0 } } }, \"aggs\": { \"group_by_servings\": { \"terms\": { \"field\": \"servings.max\" } } } } } } This example uses a range filter to only include documents with a ratings field greater than or equal to 4.0. You can use other filters like match , bool , term , and so on.","title":"Filter Aggregation"},{"location":"nosql/elastic/#nested-aggregation","text":"A nested aggregation allows you to perform aggregations on nested fields in your documents. Here's an example of a nested aggregation that groups documents in the \"receipe\" index by their ingredients.name field: GET receipe/_search { \"size\": 0, \"aggs\": { \"group_by_ingredient_name\": { \"nested\": { \"path\": \"ingredients\" }, \"aggs\": { \"group_by_name\": { \"terms\": { \"field\": \"ingredients.name\" } } } } } }","title":"Nested Aggregation"},{"location":"nosql/elastic/#wrap-up","text":"Elasticsearch is a powerful search and analytics engine that allows you to store, search, and analyze large amounts of data quickly and in near real-time. Elasticsearch provides a rich set of querying and aggregating capabilities that allow you to search and analyze your data in many different ways. Queries in Elasticsearch are used to retrieve specific documents or sets of documents that match certain criteria. Elasticsearch supports a wide variety of queries, including term, match, range, wildcard, and fuzzy queries. You can also combine multiple queries using boolean logic and use filters to narrow down your search results. Aggregations in Elasticsearch are used to compute and summarize statistics about your data. Elasticsearch supports a wide variety of aggregations, including sum, avg, min, max, date histogram, terms, and nested aggregations. Aggregations allow you to group your data into buckets based on one or more fields and compute statistics like counts, averages, sums, and more. Together, queries and aggregations in Elasticsearch allow you to search and analyze your data in many different ways, giving you valuable insights into your data and helping you make better business decisions.","title":"Wrap-up"},{"location":"nosql/elastic/#exercices","text":"","title":"Exercices"},{"location":"nosql/elastic/#movies-database","text":"Here some queries to practice on the movies database : Retrieve all Films titled \"Star Wars\" directed by \"George Lucas\" using boolean query. Retrieve all Films in which \"Harrison Ford\" played. Retrieve all Films in which \"Harrison Ford\" played and the plot contains \"Jones\". Retrieve all Films in which \"Harrison Ford\" played, the plot contains \"Jones\", but not the word \"Nazis\". Retrieve all Films directed by \"James Cameron\" with a rank below 1000 using boolean and range query. Retrieve all Films directed by \"James Cameron\" with a rank below 400. (Exact response: 2) Retrieve all Films directed by \"Quentin Tarantino\" with a rating above 5, but not categorized as an action or drama. Retrieve all Films directed by \"J.J. Abrams\" released between 2010 and 2015. Retrieve all Films with the word \"Star\" in the title and a rating above 7. Retrieve all Films with the word \"Trek\" in the title and a rating above 8 released after the year 2000.","title":"Movies database"},{"location":"nosql/elastic/#receipe-database","text":"Retrieve all documents in the index. Retrieve all documents in the index that have a preparation_time_minutes field greater than or equal to 60. Retrieve all documents in the index that have an ingredient with the name \"sugar\". Retrieve all documents in the index that have a servings.min field less than or equal to 4. Retrieve all documents in the index that have a ratings field greater than or equal to 4.5. Retrieve all documents in the index that have the word \"chicken\" in the title field. Retrieve all documents in the index that have the word \"vegetarian\" in the description field. Retrieve all documents in the index that have the word \"bake\" in the steps field. Retrieve all documents in the index that have a created field after January 1st, 2000. Retrieve all documents in the index that have an ingredient with the name \"flour\" and a servings.max field greater than or equal to 8. Compute the average preparation_time_minutes across all documents in the index. Group all documents in the index by the number of servings.min and compute the average preparation_time_minutes for each group. Compute the sum of preparation_time_minutes for all documents in the index that have the word \"chicken\" in the title field. Group all documents in the index by the servings.max field and compute the average ratings for each group. Compute the minimum and maximum preparation_time_minutes for all documents in the index that have an ingredient with the name \"sugar\".","title":"Receipe database"},{"location":"nosql/elastic/#account-database","text":"Retrieve all documents in the index with a balance field greater than or equal to 1000. Retrieve all documents in the index with a gender field equal to \"female\". Retrieve all documents in the index with an age field between 30 and 40. Retrieve all documents in the index with a state field equal to \"California\". Retrieve all documents in the index with an email field containing the word \"gmail\". Retrieve all documents in the index with a registered field after January 1st, 2022. Retrieve all documents in the index with a tags field containing the value \"neque\". Retrieve all documents in the index with a phone field starting with the area code \"510\". Retrieve all documents in the index with a isActive field set to true. Compute the average balance across all documents in the index. Group all documents in the index by the gender field and compute the average balance for each group. Compute the sum of balance for all documents in the index with a state field equal to \"California\". Group all documents in the index by the age field and compute the average balance for each group. Compute the minimum and maximum balance for all documents in the index with an email field containing the word \"quility\".","title":"Account database"},{"location":"nosql/elastic/#orders-database","text":"Retrieve all documents in the index. Retrieve all documents in the index with a total_amount field greater than or equal to 100. Retrieve all documents in the index with a status field equal to \"processed\". Retrieve all documents in the index with a salesman.name field containing the word \"Woodruff\". Retrieve all documents in the index with a sales_channel field equal to \"store\" and a total_amount field greater than 50. Compute the average total_amount across all documents in the index. Group all documents in the index by the sales_channel field and compute the sum of total_amount for each group. Compute the count of documents in the index with a status field equal to \"completed\". Group all documents in the index by the salesman.name field and compute the average total_amount for each group. Compute the minimum and maximum total_amount for all documents in the index with a purchased_at field in the year 2016.","title":"Orders database"},{"location":"nosql/intro/","text":"What are NoSQL databases? NoSQL databases, also known as \"non-relational\" databases, are a class of databases that don't use the traditional relational data model . Instead, they use various data models such as document-based, key-value, column-family, or graph-based to store and manage data. The term NoSQL was coined in the late 2000s to refer to databases that were not based on SQL , the traditional relational database query language. NoSQL databases are designed to handle large volumes of unstructured or semi-structured data that cannot be easily organized into tables with predefined columns and relationships. Key characteristics of NoSQL databases NoSQL databases are important in modern data-driven applications because they offer several advantages over traditional relational databases. For example: Schema flexibility : Unlike traditional relational databases, NoSQL databases offer schema flexibility, which means that they don't enforce a predefined schema for data storage. This allows for greater flexibility in data modeling and can simplify development processes, as it eliminates the need to define and maintain rigid schemas. Scalability : NoSQL databases are designed to scale horizontally, which means that they can handle large volumes of data by adding more servers to the cluster. This allows for better performance and can reduce infrastructure costs over time. High availability : NoSQL databases are designed to offer high availability, which means that they are able to maintain a high level of uptime even in the event of hardware failures or network outages. This is achieved through techniques such as replication and sharding. Data model diversity : NoSQL databases offer a variety of data models, such as document-based, key-value, column-family, and graph-based, which can be used to store different types of data. This allows developers to choose the most appropriate data model for their application, rather than being limited to a single relational data model. Performance : NoSQL databases can offer better performance than traditional relational databases, particularly when dealing with large data sets. This is because NoSQL databases are designed to optimize read and write operations for large-scale data processing. NoSQL databases have become increasingly popular in recent years, particularly in the context of big data and real-time data processing. They are used by a wide range of organizations, from startups to large enterprises, for applications such as e-commerce, social media, and mobile apps. Examples of NoSQL databases There is a lot of popular technologies of NoSQL databases such as : MongoDB : MongoDB is a document-based NoSQL database that uses JSON-like documents to store data. It offers scalability, flexibility, and high availability, and is often used for applications that require real-time analytics, content management, and social media. MongoDB is also known for its support for complex queries, indexing, and sharding. Elasticsearch : Elasticsearch is a document-based NoSQL database that is designed for full-text search and analytics. It offers fast search and retrieval of unstructured data, and is often used for applications that require real-time search, logging, and data analysis. Elasticsearch is also known for its support for indexing, sharding, and clustering. Redis : Redis is a key-value NoSQL database that uses in-memory data structures to store data. It offers high performance and scalability, and is often used for applications that require real-time data processing, caching, and message brokering. Redis is also known for its support for data types such as strings, hashes, lists, and sets. Neo4j : Neo4j is a graph-based NoSQL database that uses nodes and edges to store data. It offers high scalability and performance, and is often used for applications that require complex data modeling, such as social networking, recommendation engines, and fraud detection. Here's quick tips on how to choose between NoSQL databases technology based on the examples given: MongoDB: If you require a document-based data model and need to store JSON-like data, MongoDB may be a good fit. MongoDB is also a good choice for applications that require scalability, flexibility, and high availability, such as real-time analytics, content management, and social media. Redis: If you require fast data processing and caching, Redis may be a good fit. Redis is optimized for in-memory data storage and retrieval, and is often used for applications that require real-time data processing, message brokering, and caching. Neo4j: If you require complex data modeling and relationships, such as graph-based data, Neo4j may be a good fit. Neo4j is optimized for graph-based data storage and retrieval, and is often used for applications that require social networking, recommendation engines, and fraud detection. Elasticsearch: If you require fast search and retrieval of unstructured data, Elasticsearch may be a good fit. Elasticsearch is specifically designed for full-text search and analytics, and is often used for applications that require real-time search, logging, and data analysis. When choosing a NoSQL database technology, it's important to consider factors such as data model, scalability, performance, data consistency, and ease of use. It's also important to consider the specific use case and requirements of the application to determine which NoSQL technology is the best fit. Types of NoSQL databases NoSQL databases are a type of non-relational databases that are designed to handle large volumes of unstructured data. Unlike traditional relational databases, NoSQL databases do not rely on a fixed schema to store data, which makes them highly flexible and scalable. There are several types of NoSQL databases, each with its own strengths and weaknesses. In this tutorial, we will provide an overview of the most common types of NoSQL databases, including document-based, key-value, column-family, and graph-based databases. Document-Based Databases Document-based databases store data in a semi-structured format, such as JSON or XML. Each document in the database contains a set of key-value pairs that define its structure. Document-based databases are highly flexible and can easily accommodate changes to the data schema. One of the most popular document-based databases is MongoDB. MongoDB is known for its scalability, flexibility, and high availability. It is often used for real-time analytics, content management, and social media. Key-Value Databases Key-value databases store data as a collection of key-value pairs. Each key is unique and corresponds to a value, which can be a string, number, or more complex data structure. Key-value databases are highly efficient for read and write operations, making them ideal for applications that require high performance and scalability. One of the most popular key-value databases is Redis. Redis is known for its fast in-memory data storage and retrieval, and is often used for real-time data processing, caching, and message brokering. Column-Family Databases Column-family databases store data in column families, which are groups of related columns. Each column can contain multiple values, which are stored as a collection of key-value pairs. Column-family databases are highly scalable and can handle large volumes of data. One of the most popular column-family databases is Apache Cassandra. Cassandra is known for its scalability and high availability, and is often used for online transaction processing, financial services, and e-commerce. Graph-Based Databases Graph-based databases store data in nodes and edges, which represent entities and relationships between entities, respectively. Graph-based databases are highly flexible and can accommodate complex data models and relationships. One of the most popular graph-based databases is Neo4j. Neo4j is known for its performance and scalability, and is often used for applications that require complex data modeling, such as social networking, recommendation engines, and fraud detection. NoSQL databases offer several advantages over traditional relational databases, including flexibility, scalability, and performance. By understanding the different types of NoSQL databases, you can choose the one that best fits your application's requirements. In summary, document-based databases are ideal for applications that require flexibility and real-time analytics, key-value databases are ideal for applications that require high performance and scalability, column-family databases are ideal for applications that require scalability and high availability, and graph-based databases are ideal for applications that require complex data modeling and relationships. Use cases for NoSQL databases NoSQL databases are ideal for applications that require flexibility, scalability, and high performance. Some common use cases for NoSQL databases include: Real-time Analytics : NoSQL databases are ideal for real-time analytics applications that require fast and flexible data processing. With NoSQL databases, data can be stored in a semi-structured format, such as JSON or XML, allowing for easy data modeling and analysis. This makes them a great fit for applications that require real-time analytics, such as social media and e-commerce platforms. High-volume Transaction Processing : NoSQL databases are designed to handle high volumes of data and can scale horizontally to accommodate increased traffic. This makes them a great fit for high-volume transaction processing applications, such as financial services and e-commerce platforms. High-speed Content Delivery : NoSQL databases are optimized for fast read and write operations, making them ideal for high-speed content delivery applications, such as content management systems and media platforms. Scenarios where NoSQL Databases are a Better Fit than Relational Databases NoSQL databases are often a better fit than relational databases in scenarios that require: High Write Throughput: NoSQL databases are designed to handle high volumes of data and can scale horizontally to accommodate increased traffic. This makes them a great fit for applications that require high write throughput, such as social media platforms and financial services. Schema Flexibility: NoSQL databases do not enforce a fixed schema for data storage, allowing for greater flexibility in data modeling and simplified development processes. This makes them a great fit for applications that require schema flexibility, such as content management systems and real-time analytics. High Availability: NoSQL databases are designed to offer high availability, ensuring that data is accessible even in the event of hardware failures or network outages. This makes them a great fit for applications that require high availability, such as e-commerce platforms and financial services. Factors to Consider when Choosing Between NoSQL and Relational Databases When choosing between NoSQL and relational databases, it's important to consider several factors, including: Search performance is one of the key strengths of Elasticsearch compared to traditional SQL databases. Elasticsearch is optimized for full-text search, which means that it is designed to quickly search through large amounts of text data. In Elasticsearch, this is achieved through the use of an inverted index. An inverted index is a data structure that allows for very fast searching of text data. It works by creating an index of all the words in a set of documents, along with a list of which documents each word appears in. When a user searches for a specific term, Elasticsearch can quickly look up that term in the inverted index and return a list of documents that contain that term. This makes search performance much faster than traditional SQL databases, which typically use more complex query processing methods. Elasticsearch also provides features such as relevance scoring, which can help to improve search performance even further. Relevance scoring allows Elasticsearch to return search results that are more relevant to the user's query, based on factors such as the frequency of the search term in the document, the length of the document, and other factors. Overall, Elasticsearch's search performance is optimized for full-text search and can handle large amounts of unstructured or semi-structured data efficiently. SQL databases, on the other hand, may struggle with search performance when dealing with large amounts of text data, and may require more complex query processing to achieve the same results. Scalability : another area where Elasticsearch outperforms SQL databases. Elasticsearch is designed to scale horizontally across multiple nodes, which allows it to handle large amounts of data and search queries more efficiently than a single SQL database server. In Elasticsearch, data is divided into smaller segments, called shards, which can be distributed across multiple nodes in a cluster. When a search query is submitted, Elasticsearch can distribute the query across multiple shards in parallel, which can speed up search performance significantly. Additionally, Elasticsearch allows for the addition or removal of nodes in a cluster without any downtime or disruption, making it highly scalable. SQL databases, on the other hand, are typically limited to a single server, which can be a bottleneck when dealing with large amounts of data or high search query volumes. Scaling SQL databases usually involves upgrading to more powerful hardware or partitioning the data across multiple servers, which can be complex and expensive. Overall, Elasticsearch's scalability is a key advantage for applications that require large-scale search and analysis of unstructured or semi-structured data. By distributing data and search queries across multiple nodes, Elasticsearch can handle massive amounts of data and search requests more efficiently than SQL databases. Flexibility : is another area where Elasticsearch outperforms SQL databases. Elasticsearch is designed to handle unstructured or semi-structured data, which means it can be used for a wide range of use cases, including full-text search, log analysis, and machine learning. Elasticsearch can also handle data in different formats, such as JSON, CSV, and XML. In contrast, SQL databases are designed for structured data and have predefined schemas. SQL databases typically require data to be organized into tables with fixed columns and data types. This structure can be inflexible and can limit the types of data that can be stored and queried. Elasticsearch's flexible data model allows for more complex queries and analysis, which can be especially useful for applications such as log analysis, where the data may not conform to a fixed schema. Additionally, Elasticsearch's support for different data formats and sources means that it can be easily integrated with other data sources and systems. Overall, Elasticsearch's flexibility makes it a versatile tool for a wide range of use cases, from full-text search to machine learning, while SQL databases are typically limited to relational data management and querying. Distributed computing : is a computing paradigm that involves multiple computers working together as a single system to perform a task. Elasticsearch is designed to be a distributed system, which means that it can perform data processing and analysis across multiple nodes in a cluster. In Elasticsearch, data is divided into smaller segments, called shards, which can be distributed across multiple nodes in a cluster. When a query is submitted, Elasticsearch can distribute the query across multiple shards in parallel, which can speed up query performance significantly. Additionally, Elasticsearch allows for the addition or removal of nodes in a cluster without any downtime or disruption, making it highly scalable. Distributed computing in Elasticsearch also enables fault tolerance. If a node fails or becomes unavailable, Elasticsearch can automatically reassign its shards to other nodes in the cluster, ensuring that data and search capabilities are not lost. Overall, distributed computing in Elasticsearch enables fast query processing and scalability, as well as fault tolerance, making it an efficient and reliable solution for large-scale data processing and analysis. SQL databases, on the other hand, are typically designed for single-server deployments and may struggle to handle large amounts of data or high query volumes.","title":"Introduction NoSQL"},{"location":"nosql/intro/#what-are-nosql-databases","text":"NoSQL databases, also known as \"non-relational\" databases, are a class of databases that don't use the traditional relational data model . Instead, they use various data models such as document-based, key-value, column-family, or graph-based to store and manage data. The term NoSQL was coined in the late 2000s to refer to databases that were not based on SQL , the traditional relational database query language. NoSQL databases are designed to handle large volumes of unstructured or semi-structured data that cannot be easily organized into tables with predefined columns and relationships.","title":"What are NoSQL databases?"},{"location":"nosql/intro/#key-characteristics-of-nosql-databases","text":"NoSQL databases are important in modern data-driven applications because they offer several advantages over traditional relational databases. For example: Schema flexibility : Unlike traditional relational databases, NoSQL databases offer schema flexibility, which means that they don't enforce a predefined schema for data storage. This allows for greater flexibility in data modeling and can simplify development processes, as it eliminates the need to define and maintain rigid schemas. Scalability : NoSQL databases are designed to scale horizontally, which means that they can handle large volumes of data by adding more servers to the cluster. This allows for better performance and can reduce infrastructure costs over time. High availability : NoSQL databases are designed to offer high availability, which means that they are able to maintain a high level of uptime even in the event of hardware failures or network outages. This is achieved through techniques such as replication and sharding. Data model diversity : NoSQL databases offer a variety of data models, such as document-based, key-value, column-family, and graph-based, which can be used to store different types of data. This allows developers to choose the most appropriate data model for their application, rather than being limited to a single relational data model. Performance : NoSQL databases can offer better performance than traditional relational databases, particularly when dealing with large data sets. This is because NoSQL databases are designed to optimize read and write operations for large-scale data processing. NoSQL databases have become increasingly popular in recent years, particularly in the context of big data and real-time data processing. They are used by a wide range of organizations, from startups to large enterprises, for applications such as e-commerce, social media, and mobile apps.","title":"Key characteristics of NoSQL databases"},{"location":"nosql/intro/#examples-of-nosql-databases","text":"There is a lot of popular technologies of NoSQL databases such as : MongoDB : MongoDB is a document-based NoSQL database that uses JSON-like documents to store data. It offers scalability, flexibility, and high availability, and is often used for applications that require real-time analytics, content management, and social media. MongoDB is also known for its support for complex queries, indexing, and sharding. Elasticsearch : Elasticsearch is a document-based NoSQL database that is designed for full-text search and analytics. It offers fast search and retrieval of unstructured data, and is often used for applications that require real-time search, logging, and data analysis. Elasticsearch is also known for its support for indexing, sharding, and clustering. Redis : Redis is a key-value NoSQL database that uses in-memory data structures to store data. It offers high performance and scalability, and is often used for applications that require real-time data processing, caching, and message brokering. Redis is also known for its support for data types such as strings, hashes, lists, and sets. Neo4j : Neo4j is a graph-based NoSQL database that uses nodes and edges to store data. It offers high scalability and performance, and is often used for applications that require complex data modeling, such as social networking, recommendation engines, and fraud detection. Here's quick tips on how to choose between NoSQL databases technology based on the examples given: MongoDB: If you require a document-based data model and need to store JSON-like data, MongoDB may be a good fit. MongoDB is also a good choice for applications that require scalability, flexibility, and high availability, such as real-time analytics, content management, and social media. Redis: If you require fast data processing and caching, Redis may be a good fit. Redis is optimized for in-memory data storage and retrieval, and is often used for applications that require real-time data processing, message brokering, and caching. Neo4j: If you require complex data modeling and relationships, such as graph-based data, Neo4j may be a good fit. Neo4j is optimized for graph-based data storage and retrieval, and is often used for applications that require social networking, recommendation engines, and fraud detection. Elasticsearch: If you require fast search and retrieval of unstructured data, Elasticsearch may be a good fit. Elasticsearch is specifically designed for full-text search and analytics, and is often used for applications that require real-time search, logging, and data analysis. When choosing a NoSQL database technology, it's important to consider factors such as data model, scalability, performance, data consistency, and ease of use. It's also important to consider the specific use case and requirements of the application to determine which NoSQL technology is the best fit.","title":"Examples of NoSQL databases"},{"location":"nosql/intro/#types-of-nosql-databases","text":"NoSQL databases are a type of non-relational databases that are designed to handle large volumes of unstructured data. Unlike traditional relational databases, NoSQL databases do not rely on a fixed schema to store data, which makes them highly flexible and scalable. There are several types of NoSQL databases, each with its own strengths and weaknesses. In this tutorial, we will provide an overview of the most common types of NoSQL databases, including document-based, key-value, column-family, and graph-based databases.","title":"Types of NoSQL databases"},{"location":"nosql/intro/#document-based-databases","text":"Document-based databases store data in a semi-structured format, such as JSON or XML. Each document in the database contains a set of key-value pairs that define its structure. Document-based databases are highly flexible and can easily accommodate changes to the data schema. One of the most popular document-based databases is MongoDB. MongoDB is known for its scalability, flexibility, and high availability. It is often used for real-time analytics, content management, and social media.","title":"Document-Based Databases"},{"location":"nosql/intro/#key-value-databases","text":"Key-value databases store data as a collection of key-value pairs. Each key is unique and corresponds to a value, which can be a string, number, or more complex data structure. Key-value databases are highly efficient for read and write operations, making them ideal for applications that require high performance and scalability. One of the most popular key-value databases is Redis. Redis is known for its fast in-memory data storage and retrieval, and is often used for real-time data processing, caching, and message brokering.","title":"Key-Value Databases"},{"location":"nosql/intro/#column-family-databases","text":"Column-family databases store data in column families, which are groups of related columns. Each column can contain multiple values, which are stored as a collection of key-value pairs. Column-family databases are highly scalable and can handle large volumes of data. One of the most popular column-family databases is Apache Cassandra. Cassandra is known for its scalability and high availability, and is often used for online transaction processing, financial services, and e-commerce.","title":"Column-Family Databases"},{"location":"nosql/intro/#graph-based-databases","text":"Graph-based databases store data in nodes and edges, which represent entities and relationships between entities, respectively. Graph-based databases are highly flexible and can accommodate complex data models and relationships. One of the most popular graph-based databases is Neo4j. Neo4j is known for its performance and scalability, and is often used for applications that require complex data modeling, such as social networking, recommendation engines, and fraud detection. NoSQL databases offer several advantages over traditional relational databases, including flexibility, scalability, and performance. By understanding the different types of NoSQL databases, you can choose the one that best fits your application's requirements. In summary, document-based databases are ideal for applications that require flexibility and real-time analytics, key-value databases are ideal for applications that require high performance and scalability, column-family databases are ideal for applications that require scalability and high availability, and graph-based databases are ideal for applications that require complex data modeling and relationships.","title":"Graph-Based Databases"},{"location":"nosql/intro/#use-cases-for-nosql-databases","text":"NoSQL databases are ideal for applications that require flexibility, scalability, and high performance. Some common use cases for NoSQL databases include: Real-time Analytics : NoSQL databases are ideal for real-time analytics applications that require fast and flexible data processing. With NoSQL databases, data can be stored in a semi-structured format, such as JSON or XML, allowing for easy data modeling and analysis. This makes them a great fit for applications that require real-time analytics, such as social media and e-commerce platforms. High-volume Transaction Processing : NoSQL databases are designed to handle high volumes of data and can scale horizontally to accommodate increased traffic. This makes them a great fit for high-volume transaction processing applications, such as financial services and e-commerce platforms. High-speed Content Delivery : NoSQL databases are optimized for fast read and write operations, making them ideal for high-speed content delivery applications, such as content management systems and media platforms.","title":"Use cases for NoSQL databases"},{"location":"nosql/intro/#scenarios-where-nosql-databases-are-a-better-fit-than-relational-databases","text":"NoSQL databases are often a better fit than relational databases in scenarios that require: High Write Throughput: NoSQL databases are designed to handle high volumes of data and can scale horizontally to accommodate increased traffic. This makes them a great fit for applications that require high write throughput, such as social media platforms and financial services. Schema Flexibility: NoSQL databases do not enforce a fixed schema for data storage, allowing for greater flexibility in data modeling and simplified development processes. This makes them a great fit for applications that require schema flexibility, such as content management systems and real-time analytics. High Availability: NoSQL databases are designed to offer high availability, ensuring that data is accessible even in the event of hardware failures or network outages. This makes them a great fit for applications that require high availability, such as e-commerce platforms and financial services.","title":"Scenarios where NoSQL Databases are a Better Fit than Relational Databases"},{"location":"nosql/intro/#factors-to-consider-when-choosing-between-nosql-and-relational-databases","text":"When choosing between NoSQL and relational databases, it's important to consider several factors, including: Search performance is one of the key strengths of Elasticsearch compared to traditional SQL databases. Elasticsearch is optimized for full-text search, which means that it is designed to quickly search through large amounts of text data. In Elasticsearch, this is achieved through the use of an inverted index. An inverted index is a data structure that allows for very fast searching of text data. It works by creating an index of all the words in a set of documents, along with a list of which documents each word appears in. When a user searches for a specific term, Elasticsearch can quickly look up that term in the inverted index and return a list of documents that contain that term. This makes search performance much faster than traditional SQL databases, which typically use more complex query processing methods. Elasticsearch also provides features such as relevance scoring, which can help to improve search performance even further. Relevance scoring allows Elasticsearch to return search results that are more relevant to the user's query, based on factors such as the frequency of the search term in the document, the length of the document, and other factors. Overall, Elasticsearch's search performance is optimized for full-text search and can handle large amounts of unstructured or semi-structured data efficiently. SQL databases, on the other hand, may struggle with search performance when dealing with large amounts of text data, and may require more complex query processing to achieve the same results. Scalability : another area where Elasticsearch outperforms SQL databases. Elasticsearch is designed to scale horizontally across multiple nodes, which allows it to handle large amounts of data and search queries more efficiently than a single SQL database server. In Elasticsearch, data is divided into smaller segments, called shards, which can be distributed across multiple nodes in a cluster. When a search query is submitted, Elasticsearch can distribute the query across multiple shards in parallel, which can speed up search performance significantly. Additionally, Elasticsearch allows for the addition or removal of nodes in a cluster without any downtime or disruption, making it highly scalable. SQL databases, on the other hand, are typically limited to a single server, which can be a bottleneck when dealing with large amounts of data or high search query volumes. Scaling SQL databases usually involves upgrading to more powerful hardware or partitioning the data across multiple servers, which can be complex and expensive. Overall, Elasticsearch's scalability is a key advantage for applications that require large-scale search and analysis of unstructured or semi-structured data. By distributing data and search queries across multiple nodes, Elasticsearch can handle massive amounts of data and search requests more efficiently than SQL databases. Flexibility : is another area where Elasticsearch outperforms SQL databases. Elasticsearch is designed to handle unstructured or semi-structured data, which means it can be used for a wide range of use cases, including full-text search, log analysis, and machine learning. Elasticsearch can also handle data in different formats, such as JSON, CSV, and XML. In contrast, SQL databases are designed for structured data and have predefined schemas. SQL databases typically require data to be organized into tables with fixed columns and data types. This structure can be inflexible and can limit the types of data that can be stored and queried. Elasticsearch's flexible data model allows for more complex queries and analysis, which can be especially useful for applications such as log analysis, where the data may not conform to a fixed schema. Additionally, Elasticsearch's support for different data formats and sources means that it can be easily integrated with other data sources and systems. Overall, Elasticsearch's flexibility makes it a versatile tool for a wide range of use cases, from full-text search to machine learning, while SQL databases are typically limited to relational data management and querying. Distributed computing : is a computing paradigm that involves multiple computers working together as a single system to perform a task. Elasticsearch is designed to be a distributed system, which means that it can perform data processing and analysis across multiple nodes in a cluster. In Elasticsearch, data is divided into smaller segments, called shards, which can be distributed across multiple nodes in a cluster. When a query is submitted, Elasticsearch can distribute the query across multiple shards in parallel, which can speed up query performance significantly. Additionally, Elasticsearch allows for the addition or removal of nodes in a cluster without any downtime or disruption, making it highly scalable. Distributed computing in Elasticsearch also enables fault tolerance. If a node fails or becomes unavailable, Elasticsearch can automatically reassign its shards to other nodes in the cluster, ensuring that data and search capabilities are not lost. Overall, distributed computing in Elasticsearch enables fast query processing and scalability, as well as fault tolerance, making it an efficient and reliable solution for large-scale data processing and analysis. SQL databases, on the other hand, are typically designed for single-server deployments and may struggle to handle large amounts of data or high query volumes.","title":"Factors to Consider when Choosing Between NoSQL and Relational Databases"},{"location":"sql/02_db_kesako/","text":"What is a Database ? A database is an organized collection of data that is designed to be easy to access, manage, and update. Databases are used in a wide range of applications, from social media platforms to scientific research to financial systems. They are essential for storing and managing large amounts of data in a way that makes it easy to find, update, and retrieve the information you need quickly. Some examples One of the simplest examples of a database is an address book. Think about the contacts list on your phone. You have a list of people you know, along with their phone numbers, email addresses, and other details. This list is essentially a simple database, with each contact representing a record in the database. You can easily search for a contact by name, update their information, or delete a contact when you no longer need it. Another example of a database is an online store. When you shop online, you browse through a catalog of products, add items to your cart, and then complete your purchase. Behind the scenes, the online store is using a database to store information about the products, customers, and orders. When you search for a product, the database is used to find all the relevant information about that product, including the price, description, and availability. When you complete your purchase, the database is updated with information about the order, including the items you bought, the shipping address, and the payment method. Types of Databases There are many different types of databases, each with its own strengths and weaknesses. Some of the most common types of databases include: Relational databases : These databases organize data into tables with columns and rows, similar to a spreadsheet. They are the most common type of database and are used in many different applications. NoSQL databases : These databases are designed to handle large amounts of unstructured or semi-structured data. They are commonly used in big data applications, such as social media analytics and scientific research. Object-oriented databases : These databases store data as objects, which can be manipulated using object-oriented programming techniques. They are commonly used in software development and data modeling. Examples of day-to-day use cases for each type of database Relational databases Customer database example : A business might use a relational database to keep track of customer information, such as their name, contact details, and purchase history. They could then use this information to target customers with personalized marketing campaigns based on their previous purchases. Inventory management example : A store might use a relational database to manage their inventory, with one table for products and another table for suppliers. They could then use SQL queries to quickly retrieve information on which products are in stock, which products are selling quickly, and which suppliers they need to contact to restock their inventory. Employee scheduling example : A company might use a relational database to manage employee schedules, with one table for employees and another table for shifts. They could then use SQL queries to quickly retrieve information on which employees are available to work on a particular day or time, and which shifts still need to be filled. NoSQL databases Social media analytics: Social media platforms like Facebook and Twitter use NoSQL databases to store and analyze massive amounts of user data, such as likes, comments, and shares. This allows them to quickly retrieve and analyze user data to provide better ad targeting and personalized content. Internet of Things (IoT) devices: IoT devices like smart thermostats and security cameras generate a huge amount of data, which can be stored and analyzed in NoSQL databases. This allows manufacturers to track device usage patterns, identify and fix bugs, and improve device performance over time. Gaming: Many video games use NoSQL databases to store player data, such as character stats and in-game achievements. This allows players to continue their game progress across different devices, and enables game developers to quickly retrieve and analyze player data to identify areas for improvement. Object-oriented databases Geolocation data: Companies that rely on geolocation data, such as mapping and navigation services, often use object-oriented databases to store and retrieve this data. This allows them to quickly retrieve and analyze large amounts of geolocation data in real-time. E-commerce: An e-commerce website might use an object-oriented database to store and manage product information, such as product images and descriptions. This allows them to easily update and manage product information across multiple platforms, such as their website, mobile app, and social media. Medical records: Hospitals and healthcare providers often use object-oriented databases to manage patient medical records, which can include a wide range of data types, such as images, test results, and diagnoses. This allows healthcare providers to easily access and update patient information, and can help improve patient care and outcomes. In this part we will focus only on relational databases. Vocabulary MySQL client is a command-line tool that allows you to interact with the MySQL server, execute SQL queries, and manage databases. MySQL server is the software that stores and manages databases, and allows multiple clients to connect to it and perform operations on the databases. SQL (Structured Query Language) is the language used to interact with relational databases like MySQL, and it provides a standardized syntax for creating, modifying, and querying databases.","title":"What is a Database"},{"location":"sql/02_db_kesako/#what-is-a-database","text":"A database is an organized collection of data that is designed to be easy to access, manage, and update. Databases are used in a wide range of applications, from social media platforms to scientific research to financial systems. They are essential for storing and managing large amounts of data in a way that makes it easy to find, update, and retrieve the information you need quickly.","title":"What is a Database ?"},{"location":"sql/02_db_kesako/#some-examples","text":"One of the simplest examples of a database is an address book. Think about the contacts list on your phone. You have a list of people you know, along with their phone numbers, email addresses, and other details. This list is essentially a simple database, with each contact representing a record in the database. You can easily search for a contact by name, update their information, or delete a contact when you no longer need it. Another example of a database is an online store. When you shop online, you browse through a catalog of products, add items to your cart, and then complete your purchase. Behind the scenes, the online store is using a database to store information about the products, customers, and orders. When you search for a product, the database is used to find all the relevant information about that product, including the price, description, and availability. When you complete your purchase, the database is updated with information about the order, including the items you bought, the shipping address, and the payment method.","title":"Some examples"},{"location":"sql/02_db_kesako/#types-of-databases","text":"There are many different types of databases, each with its own strengths and weaknesses. Some of the most common types of databases include: Relational databases : These databases organize data into tables with columns and rows, similar to a spreadsheet. They are the most common type of database and are used in many different applications. NoSQL databases : These databases are designed to handle large amounts of unstructured or semi-structured data. They are commonly used in big data applications, such as social media analytics and scientific research. Object-oriented databases : These databases store data as objects, which can be manipulated using object-oriented programming techniques. They are commonly used in software development and data modeling.","title":"Types of Databases"},{"location":"sql/02_db_kesako/#examples-of-day-to-day-use-cases-for-each-type-of-database","text":"","title":"Examples of day-to-day use cases for each type of database"},{"location":"sql/02_db_kesako/#relational-databases","text":"Customer database example : A business might use a relational database to keep track of customer information, such as their name, contact details, and purchase history. They could then use this information to target customers with personalized marketing campaigns based on their previous purchases. Inventory management example : A store might use a relational database to manage their inventory, with one table for products and another table for suppliers. They could then use SQL queries to quickly retrieve information on which products are in stock, which products are selling quickly, and which suppliers they need to contact to restock their inventory. Employee scheduling example : A company might use a relational database to manage employee schedules, with one table for employees and another table for shifts. They could then use SQL queries to quickly retrieve information on which employees are available to work on a particular day or time, and which shifts still need to be filled.","title":"Relational databases"},{"location":"sql/02_db_kesako/#nosql-databases","text":"Social media analytics: Social media platforms like Facebook and Twitter use NoSQL databases to store and analyze massive amounts of user data, such as likes, comments, and shares. This allows them to quickly retrieve and analyze user data to provide better ad targeting and personalized content. Internet of Things (IoT) devices: IoT devices like smart thermostats and security cameras generate a huge amount of data, which can be stored and analyzed in NoSQL databases. This allows manufacturers to track device usage patterns, identify and fix bugs, and improve device performance over time. Gaming: Many video games use NoSQL databases to store player data, such as character stats and in-game achievements. This allows players to continue their game progress across different devices, and enables game developers to quickly retrieve and analyze player data to identify areas for improvement.","title":"NoSQL databases"},{"location":"sql/02_db_kesako/#object-oriented-databases","text":"Geolocation data: Companies that rely on geolocation data, such as mapping and navigation services, often use object-oriented databases to store and retrieve this data. This allows them to quickly retrieve and analyze large amounts of geolocation data in real-time. E-commerce: An e-commerce website might use an object-oriented database to store and manage product information, such as product images and descriptions. This allows them to easily update and manage product information across multiple platforms, such as their website, mobile app, and social media. Medical records: Hospitals and healthcare providers often use object-oriented databases to manage patient medical records, which can include a wide range of data types, such as images, test results, and diagnoses. This allows healthcare providers to easily access and update patient information, and can help improve patient care and outcomes. In this part we will focus only on relational databases.","title":"Object-oriented databases"},{"location":"sql/02_db_kesako/#vocabulary","text":"MySQL client is a command-line tool that allows you to interact with the MySQL server, execute SQL queries, and manage databases. MySQL server is the software that stores and manages databases, and allows multiple clients to connect to it and perform operations on the databases. SQL (Structured Query Language) is the language used to interact with relational databases like MySQL, and it provides a standardized syntax for creating, modifying, and querying databases.","title":"Vocabulary"},{"location":"sql/03_table/","text":"Tables & Keys In a database, a table is a collection of related data that is organized into rows and columns. Tables are the primary way to store data in a relational database management system (RDBMS). In order to organize data effectively, tables are structured with columns that define the data that can be stored in each row. Keys Keys are an important concept in database design. They are used to ensure data integrity and to establish relationships between tables. There are several types of keys in a database: Primary Key : A primary key is a column or set of columns that uniquely identifies each row in a table. The primary key is used to enforce data integrity and to ensure that there are no duplicate rows in the table. Foreign Key : A foreign key is a column or set of columns that refers to the primary key of another table. It is used to establish a relationship between two tables. Composite Key : A composite key is a combination of two or more columns that together uniquely identify each row in a table. Examples of social security number Primary Key : A primary key is a unique identifier for a record in a table. It is used to ensure that each record in the table is unique and can be easily identified. For example, a person's social security number (SSN) can be used as a primary key in a table of customer data to ensure that each customer is unique and can be easily searched for. Foreign Key : A foreign key is used to link two tables together in a relational database. It is a field in one table that refers to the primary key in another table. For example, a customer's order history can be linked to their customer record using a foreign key. The foreign key in the order history table would refer to the primary key in the customer table, allowing for easy retrieval of all orders associated with a particular customer. Composite Key : A composite key is a key that consists of more than one field. It is used to ensure that a combination of fields in a record is unique. For example, in a table of product inventory, a combination of the product name and the manufacturer's part number could be used as a composite key to ensure that each product is unique and can be easily searched for. Examples of social media database Primary Key : In a social media website's database, each user's unique username or email address can be used as a primary key to identify and manage their account. This ensures that each user has a unique identifier, making it easy for the website to maintain user data, track user activity, and provide personalized content. Foreign Key : In a hospital's database, a patient's medical record can be linked to their lab results using a foreign key. The foreign key would refer to the primary key in a table of lab test results, allowing doctors to easily access each patient's test results and track their medical history. Composite Key : In a hotel's database, a room reservation can be linked to a specific guest's booking using a composite key consisting of the guest's name and reservation number. This ensures that each guest has a unique reservation and allows the hotel to easily track each guest's room preference and check-in/check-out dates. Creating Tables Creating a table is the first step in building a database. The syntax for creating a table varies depending on the specific database management system being used. Here's an example of how to create a simple table with a primary key: CREATE TABLE employees ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), age INT ); In this example, we have created a table called employees with four columns: id , first_name , last_name , and age . The id column is designated as the primary key for the table. To create a table with a foreign key, we first need to create the primary key in the table it will reference, and then add the foreign key to the table we're creating. Here's an example: CREATE TABLE departments ( id INT PRIMARY KEY , name VARCHAR ( 50 ) ); CREATE TABLE employees ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), age INT , department_id INT , FOREIGN KEY ( department_id ) REFERENCES departments ( id ) ); In this example, we have created two tables - employees and departments . The departments table has two columns - id and name , with id designated as the primary key. The employees table has five columns - id , first_name , last_name , age , and department_id . The id column is the primary key, and the department_id column is a foreign key that references the id column in the departments table. By using tables and keys, we can organize data effectively and establish relationships between tables. This allows us to build powerful and flexible databases that can store and manipulate large amounts of data. Don't worry about understunding the syntaxe at this point we will explain more later \ud83e\udd13","title":"Tables & Keys"},{"location":"sql/03_table/#tables-keys","text":"In a database, a table is a collection of related data that is organized into rows and columns. Tables are the primary way to store data in a relational database management system (RDBMS). In order to organize data effectively, tables are structured with columns that define the data that can be stored in each row.","title":"Tables &amp; Keys"},{"location":"sql/03_table/#keys","text":"Keys are an important concept in database design. They are used to ensure data integrity and to establish relationships between tables. There are several types of keys in a database: Primary Key : A primary key is a column or set of columns that uniquely identifies each row in a table. The primary key is used to enforce data integrity and to ensure that there are no duplicate rows in the table. Foreign Key : A foreign key is a column or set of columns that refers to the primary key of another table. It is used to establish a relationship between two tables. Composite Key : A composite key is a combination of two or more columns that together uniquely identify each row in a table.","title":"Keys"},{"location":"sql/03_table/#examples-of-social-security-number","text":"Primary Key : A primary key is a unique identifier for a record in a table. It is used to ensure that each record in the table is unique and can be easily identified. For example, a person's social security number (SSN) can be used as a primary key in a table of customer data to ensure that each customer is unique and can be easily searched for. Foreign Key : A foreign key is used to link two tables together in a relational database. It is a field in one table that refers to the primary key in another table. For example, a customer's order history can be linked to their customer record using a foreign key. The foreign key in the order history table would refer to the primary key in the customer table, allowing for easy retrieval of all orders associated with a particular customer. Composite Key : A composite key is a key that consists of more than one field. It is used to ensure that a combination of fields in a record is unique. For example, in a table of product inventory, a combination of the product name and the manufacturer's part number could be used as a composite key to ensure that each product is unique and can be easily searched for.","title":"Examples of social security number"},{"location":"sql/03_table/#examples-of-social-media-database","text":"Primary Key : In a social media website's database, each user's unique username or email address can be used as a primary key to identify and manage their account. This ensures that each user has a unique identifier, making it easy for the website to maintain user data, track user activity, and provide personalized content. Foreign Key : In a hospital's database, a patient's medical record can be linked to their lab results using a foreign key. The foreign key would refer to the primary key in a table of lab test results, allowing doctors to easily access each patient's test results and track their medical history. Composite Key : In a hotel's database, a room reservation can be linked to a specific guest's booking using a composite key consisting of the guest's name and reservation number. This ensures that each guest has a unique reservation and allows the hotel to easily track each guest's room preference and check-in/check-out dates.","title":"Examples of social media database"},{"location":"sql/03_table/#creating-tables","text":"Creating a table is the first step in building a database. The syntax for creating a table varies depending on the specific database management system being used. Here's an example of how to create a simple table with a primary key: CREATE TABLE employees ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), age INT ); In this example, we have created a table called employees with four columns: id , first_name , last_name , and age . The id column is designated as the primary key for the table. To create a table with a foreign key, we first need to create the primary key in the table it will reference, and then add the foreign key to the table we're creating. Here's an example: CREATE TABLE departments ( id INT PRIMARY KEY , name VARCHAR ( 50 ) ); CREATE TABLE employees ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), age INT , department_id INT , FOREIGN KEY ( department_id ) REFERENCES departments ( id ) ); In this example, we have created two tables - employees and departments . The departments table has two columns - id and name , with id designated as the primary key. The employees table has five columns - id , first_name , last_name , age , and department_id . The id column is the primary key, and the department_id column is a foreign key that references the id column in the departments table. By using tables and keys, we can organize data effectively and establish relationships between tables. This allows us to build powerful and flexible databases that can store and manipulate large amounts of data. Don't worry about understunding the syntaxe at this point we will explain more later \ud83e\udd13","title":"Creating Tables"},{"location":"sql/04_basics/","text":"SQL Basics understunding Formal introduction Structured Query Language (SQL) is a programming language used to manage and manipulate relational databases. It is a standard language used by most databases to store, retrieve, and manipulate data. SQL is an essential skill for any developer working with databases. SQL commands SQL commands are used to interact with a database, such as inserting, updating, deleting, and selecting data. The basic SQL commands include SELECT, INSERT, UPDATE, DELETE , and CREATE . The SELECT command is used to retrieve data from a database, while the INSERT, UPDATE, and DELETE commands are used to modify data in a database. The CREATE command is used to create tables and other database objects. SQL commands are typically entered into a command line interface or a graphical user interface, such as MySQL Workbench or pgAdmin. These interfaces allow developers to visually design and manipulate databases, as well as enter SQL commands to manage data. SQL is widely used in web development, data analysis, and many other fields. It allows developers to easily store and retrieve data, as well as perform complex data manipulations and analysis. Understanding SQL and its commands is essential for developers working with relational databases, and is a valuable skill in today's tech industry. In the following sections, we will delve deeper into the world of SQL and explore each of the major SQL commands in more detail. By the end of this course, you will have a solid understanding of SQL, its uses, and how to use it to manage and manipulate data. With this knowledge, you will be equipped to tackle a variety of database-related challenges and create powerful data-driven applications. So let's continue with SQL installations \ud83d\ude80","title":"SQL Basic understanding"},{"location":"sql/04_basics/#sql-basics-understunding","text":"","title":"SQL Basics understunding"},{"location":"sql/04_basics/#formal-introduction","text":"Structured Query Language (SQL) is a programming language used to manage and manipulate relational databases. It is a standard language used by most databases to store, retrieve, and manipulate data. SQL is an essential skill for any developer working with databases.","title":"Formal introduction"},{"location":"sql/04_basics/#sql-commands","text":"SQL commands are used to interact with a database, such as inserting, updating, deleting, and selecting data. The basic SQL commands include SELECT, INSERT, UPDATE, DELETE , and CREATE . The SELECT command is used to retrieve data from a database, while the INSERT, UPDATE, and DELETE commands are used to modify data in a database. The CREATE command is used to create tables and other database objects. SQL commands are typically entered into a command line interface or a graphical user interface, such as MySQL Workbench or pgAdmin. These interfaces allow developers to visually design and manipulate databases, as well as enter SQL commands to manage data. SQL is widely used in web development, data analysis, and many other fields. It allows developers to easily store and retrieve data, as well as perform complex data manipulations and analysis. Understanding SQL and its commands is essential for developers working with relational databases, and is a valuable skill in today's tech industry. In the following sections, we will delve deeper into the world of SQL and explore each of the major SQL commands in more detail. By the end of this course, you will have a solid understanding of SQL, its uses, and how to use it to manage and manipulate data. With this knowledge, you will be equipped to tackle a variety of database-related challenges and create powerful data-driven applications. So let's continue with SQL installations \ud83d\ude80","title":"SQL commands"},{"location":"sql/05_MacOS_install/","text":"Install MySQL on Mac OS Spec Version OS : Catalina 10.15.7 or higher Version MAMP : 5.5 What is MAMP? MAMP is a free, open-source software that allows you to easily install and run Apache, PHP, and MySQL on your local machine. MAMP is a popular solution for web developers who want to develop and test websites locally before uploading them to a live server. In this section, we'll focus on how to use MAMP to set up and use MySQL on MacOS. Step 1: Download and Install MAMP The first step is to download and install MAMP on your MacOS machine. You can download the latest version of MAMP from the official website Once the download is complete, double-click on the downloaded file to begin the installation process. Follow the on-screen instructions to install MAMP on your machine. Step 2: Start MAMP Server After installing MAMP, you can start the server by double-clicking on the MAMP icon in the Applications folder. This will launch the MAMP control panel. Click on the Start Servers button to start the Apache and MySQL servers. You can check the status of the servers by looking at the indicators in the MAMP control panel. The Apache server is running if the status indicator is green, and the MySQL server is running if the status indicator is also green. Step 3: Create a test Database with graphic interface in MAMP Once the servers are running, you can create a database. To do this, click on the Open WebStart page button in the MAMP control panel. This will launch the MAMP homepage in your default browser. Click on the phpMyAdmin link on the left-hand side of the MAMP homepage to launch the phpMyAdmin interface. phpMyAdmin is a web-based interface that allows you to manage your MySQL databases. In the phpMyAdmin interface, click on the Databases tab and then enter a name for your new database in the Create Database field for our case test . Click on the Create button to create the database and add a table name departments who look like this in SQL command line : CREATE TABLE departments ( id INT PRIMARY KEY, name VARCHAR(50) ); after validating the table creation we have to specify two field like above : and then you can see your new database : Why MySQL Workbench MySQL Workbench and MAMP are two different software tools that can be used to work with MySQL databases, but they have different features and use cases. MySQL Workbench is a graphical user interface (or GUI) tool that is designed for database administrators and developers who need to manage and develop MySQL databases. It provides a wide range of features for working with databases, including: Creating and managing database schemas Designing and executing SQL queries Visualizing database structures with ER diagrams Managing user accounts and permissions Managing database backups and restores MySQL Workbench is a powerful tool for working with MySQL databases, but it can be complex and may have a steep learning curve for beginners and we thinks it's more easy to handle and query database. MAMP, on the other hand, is a lightweight web development environment that is designed to make it easy to set up a local web server on your computer. It includes a range of tools that are useful for web developers, including: Apache web server MySQL database server PHP scripting language MAMP is designed to be easy to use and configure, and is a good choice for beginners who want to set up a local web development environment quickly and easily. Difference between MySQL and MAMP In summary, MySQL Workbench is a powerful tool for working with MySQL databases, while MAMP is a lightweight web development environment that includes a MySQL database server as one of its components. Which one you choose to use depends on your specific needs and level of expertise. Install MySQL Workbench Go to the official download link Download the appropriate version of the software for your OS and lunch MAMP SQL server then open the software and click on the button MySQL Connection > Local Instance like the screen below : If you don't have change anything you have the following ids : user : root password : root Good you are now connected to MAMP SQL Server \ud83e\udd73 Wrap up In this tutorial, we have seen how to install MAMP on MacOS and use it to create and manage MySQL databases. Here some important notions about what you have learned through the MAMP installation and MySQL server and client: MAMP is a software package that allows you to easily install a local web server on your computer, which includes Apache, MySQL, and PHP. With MAMP, you can easily set up a MySQL server and manage databases and data with the MySQL client. To install MAMP on Windows, you can download the MAMP software package and follow the installation instructions. Once installed, you can access the MySQL server and client through the MAMP control panel. MySQL client is a command-line tool that allows you to interact with the MySQL server, execute SQL queries, and manage databases. MySQL server is the software that stores and manages databases, and allows multiple clients to connect to it and perform operations on the databases. SQL (Structured Query Language) is the language used to interact with relational databases like MySQL, and it provides a standardized syntax for creating, modifying, and querying databases. With the MySQL client, you can create and manage databases, create tables, and insert, update, and delete data with SQL queries. By practicing with MAMP and the MySQL Workbench, you have learned the basics of how to install and set up a MySQL server and interact with it. In short, SQL is the language used to communicate with databases, while the MySQL server is the software that manages the databases and listens for connections from clients like the MySQL client. The MySQL client is the tool used to connect to the MySQL server and execute SQL queries.","title":"Install MySQL on MacOS"},{"location":"sql/05_MacOS_install/#install-mysql-on-mac-os","text":"","title":"Install MySQL on Mac OS"},{"location":"sql/05_MacOS_install/#spec","text":"Version OS : Catalina 10.15.7 or higher Version MAMP : 5.5","title":"Spec"},{"location":"sql/05_MacOS_install/#what-is-mamp","text":"MAMP is a free, open-source software that allows you to easily install and run Apache, PHP, and MySQL on your local machine. MAMP is a popular solution for web developers who want to develop and test websites locally before uploading them to a live server. In this section, we'll focus on how to use MAMP to set up and use MySQL on MacOS.","title":"What is MAMP?"},{"location":"sql/05_MacOS_install/#step-1-download-and-install-mamp","text":"The first step is to download and install MAMP on your MacOS machine. You can download the latest version of MAMP from the official website Once the download is complete, double-click on the downloaded file to begin the installation process. Follow the on-screen instructions to install MAMP on your machine.","title":"Step 1: Download and Install MAMP"},{"location":"sql/05_MacOS_install/#step-2-start-mamp-server","text":"After installing MAMP, you can start the server by double-clicking on the MAMP icon in the Applications folder. This will launch the MAMP control panel. Click on the Start Servers button to start the Apache and MySQL servers. You can check the status of the servers by looking at the indicators in the MAMP control panel. The Apache server is running if the status indicator is green, and the MySQL server is running if the status indicator is also green.","title":"Step 2: Start MAMP Server"},{"location":"sql/05_MacOS_install/#step-3-create-a-test-database-with-graphic-interface-in-mamp","text":"Once the servers are running, you can create a database. To do this, click on the Open WebStart page button in the MAMP control panel. This will launch the MAMP homepage in your default browser. Click on the phpMyAdmin link on the left-hand side of the MAMP homepage to launch the phpMyAdmin interface. phpMyAdmin is a web-based interface that allows you to manage your MySQL databases. In the phpMyAdmin interface, click on the Databases tab and then enter a name for your new database in the Create Database field for our case test . Click on the Create button to create the database and add a table name departments who look like this in SQL command line : CREATE TABLE departments ( id INT PRIMARY KEY, name VARCHAR(50) ); after validating the table creation we have to specify two field like above : and then you can see your new database :","title":"Step 3: Create a test Database with graphic interface in MAMP"},{"location":"sql/05_MacOS_install/#why-mysql-workbench","text":"MySQL Workbench and MAMP are two different software tools that can be used to work with MySQL databases, but they have different features and use cases. MySQL Workbench is a graphical user interface (or GUI) tool that is designed for database administrators and developers who need to manage and develop MySQL databases. It provides a wide range of features for working with databases, including: Creating and managing database schemas Designing and executing SQL queries Visualizing database structures with ER diagrams Managing user accounts and permissions Managing database backups and restores MySQL Workbench is a powerful tool for working with MySQL databases, but it can be complex and may have a steep learning curve for beginners and we thinks it's more easy to handle and query database. MAMP, on the other hand, is a lightweight web development environment that is designed to make it easy to set up a local web server on your computer. It includes a range of tools that are useful for web developers, including: Apache web server MySQL database server PHP scripting language MAMP is designed to be easy to use and configure, and is a good choice for beginners who want to set up a local web development environment quickly and easily.","title":"Why MySQL Workbench"},{"location":"sql/05_MacOS_install/#difference-between-mysql-and-mamp","text":"In summary, MySQL Workbench is a powerful tool for working with MySQL databases, while MAMP is a lightweight web development environment that includes a MySQL database server as one of its components. Which one you choose to use depends on your specific needs and level of expertise.","title":"Difference between MySQL and MAMP"},{"location":"sql/05_MacOS_install/#install-mysql-workbench","text":"Go to the official download link Download the appropriate version of the software for your OS and lunch MAMP SQL server then open the software and click on the button MySQL Connection > Local Instance like the screen below : If you don't have change anything you have the following ids : user : root password : root Good you are now connected to MAMP SQL Server \ud83e\udd73","title":"Install MySQL Workbench"},{"location":"sql/05_MacOS_install/#wrap-up","text":"In this tutorial, we have seen how to install MAMP on MacOS and use it to create and manage MySQL databases. Here some important notions about what you have learned through the MAMP installation and MySQL server and client: MAMP is a software package that allows you to easily install a local web server on your computer, which includes Apache, MySQL, and PHP. With MAMP, you can easily set up a MySQL server and manage databases and data with the MySQL client. To install MAMP on Windows, you can download the MAMP software package and follow the installation instructions. Once installed, you can access the MySQL server and client through the MAMP control panel. MySQL client is a command-line tool that allows you to interact with the MySQL server, execute SQL queries, and manage databases. MySQL server is the software that stores and manages databases, and allows multiple clients to connect to it and perform operations on the databases. SQL (Structured Query Language) is the language used to interact with relational databases like MySQL, and it provides a standardized syntax for creating, modifying, and querying databases. With the MySQL client, you can create and manage databases, create tables, and insert, update, and delete data with SQL queries. By practicing with MAMP and the MySQL Workbench, you have learned the basics of how to install and set up a MySQL server and interact with it. In short, SQL is the language used to communicate with databases, while the MySQL server is the software that manages the databases and listens for connections from clients like the MySQL client. The MySQL client is the tool used to connect to the MySQL server and execute SQL queries.","title":"Wrap up"},{"location":"sql/06_b_docker_install/","text":"Install MySQL server with docker Before we get started, it's important to note that Docker is a containerization platform that allows you to run software applications in isolated environments called containers. This means that you can interact with MySQL server without installing it from scrach and run multiple instances of the same application without any interference between them. Now let's dive into the tutorial: Step 1: Install Docker If you don't have Docker installed on your system, you'll need to download and install it first. You can download Docker from the official website Step 2: Pull the MySQL Docker image In Docker, images are used to create containers, which are isolated environments that run applications. Images can be thought of as a template for containers, as they contain all the necessary files, libraries, and dependencies required for an application to run. The docker pull command is used to download an image from a remote registry, such as Docker Hub. When you run this command, Docker will search for the specified image in the remote registry and download it to your local machine. Once Docker is installed, open a terminal and run the following command to pull the MySQL Docker image: docker pull mysql This command will download the MySQL image to your local machine, which you can then use to create a container. It's important to note that you must have Docker installed on your local machine to use the docker pull command, as it is a Docker CLI command. Additionally, the image you're trying to pull must be available in the remote registry you're trying to access. Step 3: Start a MySQL container After you have pulled the MySQL Docker image, you can start a container using the following command: docker run --name mysql-container -e MYSQL_ROOT_PASSWORD = my-secret-pw -d mysql This is the command used to start a new MySQL container with the following options : --name mysql-container: This option is used to give a name to the new container. In this case, the container is named \"mysql-container\". -e MYSQL_ROOT_PASSWORD=my-secret-pw: This option sets an environment variable for the container, in this case, the MySQL - root password. This means that the password \"my-secret-pw\" will be used as the MySQL root password. -d: This option runs the container in detached mode, which means that it will run in the background and not attach to the terminal session. This is useful for long-running containers, such as database servers. mysql: This is the name of the Docker image that the container is based on. In this case, it is the official MySQL Docker image. So when you run this command, Docker will start a new container based on the MySQL image with the name \"mysql-container\", set the MySQL root password to \"my-secret-pw\", and run the container in the background it just send you the id of the container like 8c4f5828f04b it means it started fine. It's important to note that the MySQL root password should be changed to a more secure password before using the container in a production environment. Also, the name of the container and the password used can be changed to suit your specific needs. This command will start a new container named \"mysql-container\" with the root password my-secret-pw . You can verify if the container is running with the command in your terminal : docker ps You should see this in your terminal : Step 4: Access the MySQL container with the command line To access the MySQL container, you can use the following command: docker exec -it mysql-container mysql -p This command will start a MySQL shell session inside the container and prompt you to enter the password you set in the previous step. Step 5: Create a test database and a table Now that you have access to the MySQL container, you can create a database and a table using the following commands: CREATE DATABASE testdb ; USE testdb ; CREATE TABLE users ( id INT NOT NULL AUTO_INCREMENT , name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 50 ) NOT NULL , PRIMARY KEY ( id ) ); These commands will create a new database named \"testdb\" and a new table named \"users\" with three columns: \"id\", \"name\", and \"email\". Step 6: Insert values into the table You can insert values into the table using the following command: INSERT INTO users ( name , email ) VALUES ( 'John Doe' , 'johndoe@example.com' ); This command will insert a new row into the \"users\" table with the name \"John Doe\" and the email \"johndoe@example.com\". Step 7: Test a query Finally, you can test a query to retrieve the data you just inserted using the following command: SELECT * FROM users ; This command will retrieve all the data from the \"users\" table. And that's it! You've successfully installed MySQL with Docker and created a table, inserted values, and executed a test query \ud83e\udd73","title":"Install MySQL with docker"},{"location":"sql/06_b_docker_install/#install-mysql-server-with-docker","text":"Before we get started, it's important to note that Docker is a containerization platform that allows you to run software applications in isolated environments called containers. This means that you can interact with MySQL server without installing it from scrach and run multiple instances of the same application without any interference between them. Now let's dive into the tutorial:","title":"Install MySQL server with docker"},{"location":"sql/06_b_docker_install/#step-1-install-docker","text":"If you don't have Docker installed on your system, you'll need to download and install it first. You can download Docker from the official website","title":"Step 1: Install Docker"},{"location":"sql/06_b_docker_install/#step-2-pull-the-mysql-docker-image","text":"In Docker, images are used to create containers, which are isolated environments that run applications. Images can be thought of as a template for containers, as they contain all the necessary files, libraries, and dependencies required for an application to run. The docker pull command is used to download an image from a remote registry, such as Docker Hub. When you run this command, Docker will search for the specified image in the remote registry and download it to your local machine. Once Docker is installed, open a terminal and run the following command to pull the MySQL Docker image: docker pull mysql This command will download the MySQL image to your local machine, which you can then use to create a container. It's important to note that you must have Docker installed on your local machine to use the docker pull command, as it is a Docker CLI command. Additionally, the image you're trying to pull must be available in the remote registry you're trying to access.","title":"Step 2: Pull the MySQL Docker image"},{"location":"sql/06_b_docker_install/#step-3-start-a-mysql-container","text":"After you have pulled the MySQL Docker image, you can start a container using the following command: docker run --name mysql-container -e MYSQL_ROOT_PASSWORD = my-secret-pw -d mysql This is the command used to start a new MySQL container with the following options : --name mysql-container: This option is used to give a name to the new container. In this case, the container is named \"mysql-container\". -e MYSQL_ROOT_PASSWORD=my-secret-pw: This option sets an environment variable for the container, in this case, the MySQL - root password. This means that the password \"my-secret-pw\" will be used as the MySQL root password. -d: This option runs the container in detached mode, which means that it will run in the background and not attach to the terminal session. This is useful for long-running containers, such as database servers. mysql: This is the name of the Docker image that the container is based on. In this case, it is the official MySQL Docker image. So when you run this command, Docker will start a new container based on the MySQL image with the name \"mysql-container\", set the MySQL root password to \"my-secret-pw\", and run the container in the background it just send you the id of the container like 8c4f5828f04b it means it started fine. It's important to note that the MySQL root password should be changed to a more secure password before using the container in a production environment. Also, the name of the container and the password used can be changed to suit your specific needs. This command will start a new container named \"mysql-container\" with the root password my-secret-pw . You can verify if the container is running with the command in your terminal : docker ps You should see this in your terminal :","title":"Step 3: Start a MySQL container"},{"location":"sql/06_b_docker_install/#step-4-access-the-mysql-container-with-the-command-line","text":"To access the MySQL container, you can use the following command: docker exec -it mysql-container mysql -p This command will start a MySQL shell session inside the container and prompt you to enter the password you set in the previous step.","title":"Step 4: Access the MySQL container with the command line"},{"location":"sql/06_b_docker_install/#step-5-create-a-test-database-and-a-table","text":"Now that you have access to the MySQL container, you can create a database and a table using the following commands: CREATE DATABASE testdb ; USE testdb ; CREATE TABLE users ( id INT NOT NULL AUTO_INCREMENT , name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 50 ) NOT NULL , PRIMARY KEY ( id ) ); These commands will create a new database named \"testdb\" and a new table named \"users\" with three columns: \"id\", \"name\", and \"email\".","title":"Step 5: Create a test database and a table"},{"location":"sql/06_b_docker_install/#step-6-insert-values-into-the-table","text":"You can insert values into the table using the following command: INSERT INTO users ( name , email ) VALUES ( 'John Doe' , 'johndoe@example.com' ); This command will insert a new row into the \"users\" table with the name \"John Doe\" and the email \"johndoe@example.com\".","title":"Step 6: Insert values into the table"},{"location":"sql/06_b_docker_install/#step-7-test-a-query","text":"Finally, you can test a query to retrieve the data you just inserted using the following command: SELECT * FROM users ; This command will retrieve all the data from the \"users\" table. And that's it! You've successfully installed MySQL with Docker and created a table, inserted values, and executed a test query \ud83e\udd73","title":"Step 7: Test a query"},{"location":"sql/07_creating_tables/","text":"Creating Tables We said before, in SQL a table is a collection of data stored in rows and columns. To create a table, you need to define the table schema which includes the table name, column names, data types, and any constraints on the data. The CREATE TABLE statement is used to create a new table in a database. In-depth look at how to create tables To create a new table in SQL, you use the CREATE TABLE command followed by the table name and a list of column definitions. Each column definition specifies the column name, data type, and any constraints on the data. Here is an example of creating a simple table with two columns: CREATE TABLE users ( id INT PRIMARY KEY , name VARCHAR ( 50 ) NOT NULL ); This creates a table named users with two columns: id and name . The id column is defined as an integer and is marked as the primary key for the table. The name column is defined as a variable-length string with a maximum length of 50 characters and is marked as required (NOT NULL) . Overview of different data types and how to use them SQL provides a wide range of data types that can be used to define columns in a table. These data types include integers, floating-point numbers, strings, dates, and more. Here are some common data types: INT: used for integer values VARCHAR(n): used for variable-length character strings with a maximum length of n DATE: used for date values FLOAT: used for floating-point numbers BOOLEAN: used for boolean values Here is an example of creating a table with columns of different data types: CREATE TABLE products ( id INT PRIMARY KEY , name VARCHAR ( 50 ) NOT NULL , price FLOAT , in_stock BOOLEAN , created_at DATE ); Add primary and foreign keys and link an other table In a relational database, a primary key is a unique identifier for each row in a table. It is used to ensure that each row can be uniquely identified and is commonly used to link to other tables in the database. A foreign key is a column in one table that refers to the primary key of another table. This is used to create relationships between tables and enforce referential integrity. Here is an example of creating a table with a primary key and a foreign key: CREATE TABLE orders ( id INT PRIMARY KEY , product_id INT , quantity INT , FOREIGN KEY ( product_id ) REFERENCES products ( id ) ); This creates a table named orders with three columns: id , product_id , and quantity . The id column is defined as the primary key for the table. The product_id column is defined as a foreign key that references the id column in the products table, which creates a relationship between the two tables. Here, the quantity column is defined as an integer.","title":"Creating tables"},{"location":"sql/07_creating_tables/#creating-tables","text":"We said before, in SQL a table is a collection of data stored in rows and columns. To create a table, you need to define the table schema which includes the table name, column names, data types, and any constraints on the data. The CREATE TABLE statement is used to create a new table in a database.","title":"Creating Tables"},{"location":"sql/07_creating_tables/#in-depth-look-at-how-to-create-tables","text":"To create a new table in SQL, you use the CREATE TABLE command followed by the table name and a list of column definitions. Each column definition specifies the column name, data type, and any constraints on the data. Here is an example of creating a simple table with two columns: CREATE TABLE users ( id INT PRIMARY KEY , name VARCHAR ( 50 ) NOT NULL ); This creates a table named users with two columns: id and name . The id column is defined as an integer and is marked as the primary key for the table. The name column is defined as a variable-length string with a maximum length of 50 characters and is marked as required (NOT NULL) .","title":"In-depth look at how to create tables"},{"location":"sql/07_creating_tables/#overview-of-different-data-types-and-how-to-use-them","text":"SQL provides a wide range of data types that can be used to define columns in a table. These data types include integers, floating-point numbers, strings, dates, and more. Here are some common data types: INT: used for integer values VARCHAR(n): used for variable-length character strings with a maximum length of n DATE: used for date values FLOAT: used for floating-point numbers BOOLEAN: used for boolean values Here is an example of creating a table with columns of different data types: CREATE TABLE products ( id INT PRIMARY KEY , name VARCHAR ( 50 ) NOT NULL , price FLOAT , in_stock BOOLEAN , created_at DATE );","title":"Overview of different data types and how to use them"},{"location":"sql/07_creating_tables/#add-primary-and-foreign-keys-and-link-an-other-table","text":"In a relational database, a primary key is a unique identifier for each row in a table. It is used to ensure that each row can be uniquely identified and is commonly used to link to other tables in the database. A foreign key is a column in one table that refers to the primary key of another table. This is used to create relationships between tables and enforce referential integrity. Here is an example of creating a table with a primary key and a foreign key: CREATE TABLE orders ( id INT PRIMARY KEY , product_id INT , quantity INT , FOREIGN KEY ( product_id ) REFERENCES products ( id ) ); This creates a table named orders with three columns: id , product_id , and quantity . The id column is defined as the primary key for the table. The product_id column is defined as a foreign key that references the id column in the products table, which creates a relationship between the two tables. Here, the quantity column is defined as an integer.","title":"Add primary and foreign keys and link an other table"},{"location":"sql/08_insert/","text":"Inserting Data Once you have created a table, you can start inserting data into it. The process of inserting data involves specifying the table name, the columns to insert data into, and the values to be inserted. Overview of Different Insert Commands and Syntax There are a few different ways to insert data into a table in SQL, depending on how much information you have about the data you are inserting. Here are some common insert commands and their syntax: Inserting Values into Specific Columns You can use the INSERT INTO command to insert values into specific columns in a table. Here is the syntax for inserting a single row of data into a table: INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...); For example, to insert a new row into a users table with the values \"John Doe\" for the name column, \"johndoe@example.com\" for the email column, and 25 for the age column, you would use the following command: INSERT INTO users ( name , email , age ) VALUES ( 'John Doe' , 'johndoe@example.com' , 25 ); Inserting Values into All Columns If you have data to insert for every column in a table, you can omit the column names from the INSERT INTO command. Here is the syntax for inserting a single row of data into a table without specifying column names: INSERT INTO table_name VALUES ( value1 , value2 , value3 , ...); For example, to insert a new row into the users table with the values \"Jane Smith\" for the name column, \"janesmith@example.com\" for the email column, 30 for the age column, and \"female\" for the gender column, you would use the following command: INSERT INTO users VALUES ( 'Jane Smith' , 'janesmith@example.com' , 30 , 'female' ); Inserting Multiple Rows at Once You can also use the INSERT INTO command to insert multiple rows of data at once. Here is the syntax for inserting multiple rows of data into a table: INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...), ( value1 , value2 , value3 , ...), ( value1 , value2 , value3 , ...), ...; For example, to insert three new rows into the users table, you would use the following command: INSERT INTO users ( name , email , age ) VALUES ( 'Alice Johnson' , 'alicejohnson@example.com' , 35 ), ( 'Bob Williams' , 'bobwilliams@example.com' , 40 ), ( 'Charlie Brown' , 'charliebrown@example.com' , 45 ); By using these different insert commands and syntax, you can efficiently add data to your SQL database tables.","title":"Inserting Data"},{"location":"sql/08_insert/#inserting-data","text":"Once you have created a table, you can start inserting data into it. The process of inserting data involves specifying the table name, the columns to insert data into, and the values to be inserted.","title":"Inserting Data"},{"location":"sql/08_insert/#overview-of-different-insert-commands-and-syntax","text":"There are a few different ways to insert data into a table in SQL, depending on how much information you have about the data you are inserting. Here are some common insert commands and their syntax:","title":"Overview of Different Insert Commands and Syntax"},{"location":"sql/08_insert/#inserting-values-into-specific-columns","text":"You can use the INSERT INTO command to insert values into specific columns in a table. Here is the syntax for inserting a single row of data into a table: INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...); For example, to insert a new row into a users table with the values \"John Doe\" for the name column, \"johndoe@example.com\" for the email column, and 25 for the age column, you would use the following command: INSERT INTO users ( name , email , age ) VALUES ( 'John Doe' , 'johndoe@example.com' , 25 );","title":"Inserting Values into Specific Columns"},{"location":"sql/08_insert/#inserting-values-into-all-columns","text":"If you have data to insert for every column in a table, you can omit the column names from the INSERT INTO command. Here is the syntax for inserting a single row of data into a table without specifying column names: INSERT INTO table_name VALUES ( value1 , value2 , value3 , ...); For example, to insert a new row into the users table with the values \"Jane Smith\" for the name column, \"janesmith@example.com\" for the email column, 30 for the age column, and \"female\" for the gender column, you would use the following command: INSERT INTO users VALUES ( 'Jane Smith' , 'janesmith@example.com' , 30 , 'female' );","title":"Inserting Values into All Columns"},{"location":"sql/08_insert/#inserting-multiple-rows-at-once","text":"You can also use the INSERT INTO command to insert multiple rows of data at once. Here is the syntax for inserting multiple rows of data into a table: INSERT INTO table_name ( column1 , column2 , column3 , ...) VALUES ( value1 , value2 , value3 , ...), ( value1 , value2 , value3 , ...), ( value1 , value2 , value3 , ...), ...; For example, to insert three new rows into the users table, you would use the following command: INSERT INTO users ( name , email , age ) VALUES ( 'Alice Johnson' , 'alicejohnson@example.com' , 35 ), ( 'Bob Williams' , 'bobwilliams@example.com' , 40 ), ( 'Charlie Brown' , 'charliebrown@example.com' , 45 ); By using these different insert commands and syntax, you can efficiently add data to your SQL database tables.","title":"Inserting Multiple Rows at Once"},{"location":"sql/09_constraints/","text":"Constraints Constraints are rules that you can apply to a table in a database to enforce data integrity. They play a vital role in ensuring that the data within the database remains consistent and accurate. There are various types of constraints that can be applied to a table, each serving a specific purpose. Types of constraints One type of constraint is the primary key constraint, which enforces the uniqueness of a column or a group of columns within a table. Another type of constraint is the foreign key constraint, which establishes a relationship between two tables based on the values of their respective columns. Other types of constraints include the NOT NULL constraint, which ensures that a column cannot have a NULL value, and the UNIQUE constraint, which ensures that the values in a column are unique. Examples of customers table one without constraints and the other with constraints Without Constraints CREATE TABLE customers ( id INT , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), email VARCHAR ( 100 ) ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'John' , 'Doe' , 'john.doe@example.com' ), ( 2 , 'Jane' , 'Doe' , 'jane.doe@example.com' ), ( 3 , 'Bob' , 'Smith' , 'bob.smith@example.com' ), ( 4 , 'Alice' , 'Johnson' , 'alice.johnson@example.com' ); In this example, we create a table named customers with four columns - id , first_name , last_name , and email . We then insert some sample data into the table. However, there are no constraints set on the table to enforce any rules about the data being inserted. For example, we can insert multiple rows with the same id value, which can lead to inconsistencies in the data. With Constraints CREATE TABLE customers ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ) NOT NULL , last_name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 100 ) UNIQUE ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'John' , 'Doe' , 'john.doe@example.com' ), ( 2 , 'Jane' , 'Doe' , 'jane.doe@example.com' ), ( 3 , 'Bob' , 'Smith' , 'bob.smith@example.com' ), ( 4 , 'Alice' , 'Johnson' , 'alice.johnson@example.com' ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'Mark' , 'Smith' , 'mark.smith@example.com' ); -- This will fail due to duplicate primary key constraint INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 5 , 'Sam' , 'Jones' , 'bob.smith@example.com' ); -- This will fail due to unique constraint on email column In this example, we create the same customers table, but with additional constraints. We set the id column as the primary key, which means that it must be unique for each row. We also set the first_name and last_name columns as NOT NULL, which means that they cannot be empty. Finally, we set the email column as UNIQUE, which means that each email must be unique in the table. When we try to insert data into the table, the constraints are enforced. The first INSERT statement will work fine because it does not violate any constraints. However, the second INSERT statement will fail because it tries to insert a row with a duplicate id value, which violates the primary key constraint. Similarly, the third INSERT statement will fail because it tries to insert a row with a duplicate email value, which violates the unique constraint on the email column.","title":"Constraints"},{"location":"sql/09_constraints/#constraints","text":"Constraints are rules that you can apply to a table in a database to enforce data integrity. They play a vital role in ensuring that the data within the database remains consistent and accurate. There are various types of constraints that can be applied to a table, each serving a specific purpose.","title":"Constraints"},{"location":"sql/09_constraints/#types-of-constraints","text":"One type of constraint is the primary key constraint, which enforces the uniqueness of a column or a group of columns within a table. Another type of constraint is the foreign key constraint, which establishes a relationship between two tables based on the values of their respective columns. Other types of constraints include the NOT NULL constraint, which ensures that a column cannot have a NULL value, and the UNIQUE constraint, which ensures that the values in a column are unique.","title":"Types of constraints"},{"location":"sql/09_constraints/#examples-of-customers-table-one-without-constraints-and-the-other-with-constraints","text":"","title":"Examples of customers table one without constraints and the other with constraints"},{"location":"sql/09_constraints/#without-constraints","text":"CREATE TABLE customers ( id INT , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), email VARCHAR ( 100 ) ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'John' , 'Doe' , 'john.doe@example.com' ), ( 2 , 'Jane' , 'Doe' , 'jane.doe@example.com' ), ( 3 , 'Bob' , 'Smith' , 'bob.smith@example.com' ), ( 4 , 'Alice' , 'Johnson' , 'alice.johnson@example.com' ); In this example, we create a table named customers with four columns - id , first_name , last_name , and email . We then insert some sample data into the table. However, there are no constraints set on the table to enforce any rules about the data being inserted. For example, we can insert multiple rows with the same id value, which can lead to inconsistencies in the data.","title":"Without Constraints"},{"location":"sql/09_constraints/#with-constraints","text":"CREATE TABLE customers ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ) NOT NULL , last_name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 100 ) UNIQUE ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'John' , 'Doe' , 'john.doe@example.com' ), ( 2 , 'Jane' , 'Doe' , 'jane.doe@example.com' ), ( 3 , 'Bob' , 'Smith' , 'bob.smith@example.com' ), ( 4 , 'Alice' , 'Johnson' , 'alice.johnson@example.com' ); INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 1 , 'Mark' , 'Smith' , 'mark.smith@example.com' ); -- This will fail due to duplicate primary key constraint INSERT INTO customers ( id , first_name , last_name , email ) VALUES ( 5 , 'Sam' , 'Jones' , 'bob.smith@example.com' ); -- This will fail due to unique constraint on email column In this example, we create the same customers table, but with additional constraints. We set the id column as the primary key, which means that it must be unique for each row. We also set the first_name and last_name columns as NOT NULL, which means that they cannot be empty. Finally, we set the email column as UNIQUE, which means that each email must be unique in the table. When we try to insert data into the table, the constraints are enforced. The first INSERT statement will work fine because it does not violate any constraints. However, the second INSERT statement will fail because it tries to insert a row with a duplicate id value, which violates the primary key constraint. Similarly, the third INSERT statement will fail because it tries to insert a row with a duplicate email value, which violates the unique constraint on the email column.","title":"With Constraints"},{"location":"sql/10_update_delete/","text":"Update & Delete In addition to inserting data into tables, you may also need to modify or delete existing data. The SQL language provides several commands for updating and deleting data within tables. Updating Data To update data within a table, you can use the UPDATE command followed by the name of the table and the SET keyword. The SET keyword is followed by the column name you want to update, an equals sign, and the new value you want to set. Here's the basic syntax for updating data in a table: UPDATE table_name SET column_name = new_value WHERE condition ; In this syntax, the WHERE clause specifies which rows to update. Without a WHERE clause, all rows in the table would be updated. Here's an example that updates the price of a product in a table called products : UPDATE products SET price = 19 . 99 WHERE product_id = 1234 ; This statement updates the price column for the row where the product_id is equal to 1234. Deleting Data To delete data from a table, you can use the DELETE command followed by the name of the table. If you want to delete only certain rows, you can use a WHERE clause to specify which rows to delete. Here's the basic syntax for deleting data from a table: DELETE FROM table_name WHERE condition ; Here's an example that deletes a row from a table called orders : DELETE FROM orders WHERE order_id = 5678 ; This statement deletes only the row where the order_id is equal to 5678. Summarize : creating, inserting, updating and deleting Let's summarize the previous notions with an SQL example code to create a table, insert values into it, and update a field : -- Creating a table for products CREATE TABLE products ( id INT PRIMARY KEY , name VARCHAR ( 50 ), category VARCHAR ( 50 ), price DECIMAL ( 8 , 2 ) ); -- Inserting data into the table INSERT INTO products ( id , name , category , price ) VALUES ( 1 , 'Product A' , 'Category 1' , 10 . 99 ), ( 2 , 'Product B' , 'Category 2' , 19 . 99 ), ( 3 , 'Product C' , 'Category 1' , 5 . 99 ); -- Updating the price of Product A UPDATE products SET price = 12 . 99 WHERE id = 1 ; -- Deleting a record from the 'products' table DELETE FROM products WHERE product_id = 2 ; In this example, we first create a table named products with four columns: id , name , category , and price . We define the id column as the primary key, meaning it uniquely identifies each row in the table. Next, we insert three rows of data into the table using the INSERT INTO command. Each row represents a different product, with values for the id , name , category , and price columns. Finally, we update the price of Product A using the UPDATE command. We specify the table we want to update ( products ), the field we want to update ( price ), and the new value we want to set (12.99) . We use the WHERE clause to specify which row(s) we want to update; in this case, we only want to update the row with an id of 1 , which corresponds to Product A . Then, the DELETE command is used to remove the record from the products table where the value of the product_id field is equal to 2 . This will delete the second product from the table, which in this case is Product B . Deleting a single field in a row Here's an example of deleting a single field in a row of the products table: -- Delete the description of the product with id 3 UPDATE products SET description = NULL WHERE id = 3 ; In this example, we use the UPDATE command to modify the description field of the row with id equal to 3 . The SET keyword is used to specify the new value of the description field, which we set to NULL to delete the existing value. The WHERE clause is used to specify which row(s) to update. In this case, we're only updating the row with id equal to 3 . By setting the description field to NULL , we effectively delete the value of that field for that particular row.","title":"Update & Delete"},{"location":"sql/10_update_delete/#update-delete","text":"In addition to inserting data into tables, you may also need to modify or delete existing data. The SQL language provides several commands for updating and deleting data within tables.","title":"Update &amp; Delete"},{"location":"sql/10_update_delete/#updating-data","text":"To update data within a table, you can use the UPDATE command followed by the name of the table and the SET keyword. The SET keyword is followed by the column name you want to update, an equals sign, and the new value you want to set. Here's the basic syntax for updating data in a table: UPDATE table_name SET column_name = new_value WHERE condition ; In this syntax, the WHERE clause specifies which rows to update. Without a WHERE clause, all rows in the table would be updated. Here's an example that updates the price of a product in a table called products : UPDATE products SET price = 19 . 99 WHERE product_id = 1234 ; This statement updates the price column for the row where the product_id is equal to 1234.","title":"Updating Data"},{"location":"sql/10_update_delete/#deleting-data","text":"To delete data from a table, you can use the DELETE command followed by the name of the table. If you want to delete only certain rows, you can use a WHERE clause to specify which rows to delete. Here's the basic syntax for deleting data from a table: DELETE FROM table_name WHERE condition ; Here's an example that deletes a row from a table called orders : DELETE FROM orders WHERE order_id = 5678 ; This statement deletes only the row where the order_id is equal to 5678.","title":"Deleting Data"},{"location":"sql/10_update_delete/#summarize-creating-inserting-updating-and-deleting","text":"Let's summarize the previous notions with an SQL example code to create a table, insert values into it, and update a field : -- Creating a table for products CREATE TABLE products ( id INT PRIMARY KEY , name VARCHAR ( 50 ), category VARCHAR ( 50 ), price DECIMAL ( 8 , 2 ) ); -- Inserting data into the table INSERT INTO products ( id , name , category , price ) VALUES ( 1 , 'Product A' , 'Category 1' , 10 . 99 ), ( 2 , 'Product B' , 'Category 2' , 19 . 99 ), ( 3 , 'Product C' , 'Category 1' , 5 . 99 ); -- Updating the price of Product A UPDATE products SET price = 12 . 99 WHERE id = 1 ; -- Deleting a record from the 'products' table DELETE FROM products WHERE product_id = 2 ; In this example, we first create a table named products with four columns: id , name , category , and price . We define the id column as the primary key, meaning it uniquely identifies each row in the table. Next, we insert three rows of data into the table using the INSERT INTO command. Each row represents a different product, with values for the id , name , category , and price columns. Finally, we update the price of Product A using the UPDATE command. We specify the table we want to update ( products ), the field we want to update ( price ), and the new value we want to set (12.99) . We use the WHERE clause to specify which row(s) we want to update; in this case, we only want to update the row with an id of 1 , which corresponds to Product A . Then, the DELETE command is used to remove the record from the products table where the value of the product_id field is equal to 2 . This will delete the second product from the table, which in this case is Product B .","title":"Summarize : creating, inserting, updating and deleting"},{"location":"sql/10_update_delete/#deleting-a-single-field-in-a-row","text":"Here's an example of deleting a single field in a row of the products table: -- Delete the description of the product with id 3 UPDATE products SET description = NULL WHERE id = 3 ; In this example, we use the UPDATE command to modify the description field of the row with id equal to 3 . The SET keyword is used to specify the new value of the description field, which we set to NULL to delete the existing value. The WHERE clause is used to specify which row(s) to update. In this case, we're only updating the row with id equal to 3 . By setting the description field to NULL , we effectively delete the value of that field for that particular row.","title":"Deleting a single field in a row"},{"location":"sql/11_queries/","text":"Basic Queries In the world of database management, tables and queries go hand in hand. Queries are a fundamental component of any database, as they allow you to retrieve and manipulate data in meaningful ways. In this chapter, we will explore the basics of SQL queries and how they are used to extract data from tables. To demonstrate this, we will use two example tables, the orders and customers tables. These tables will be linked together using a foreign key to show how queries can retrieve data from multiple tables at once. Understanding queries and the relationship between tables is essential for effective database management, as it enables developers to extract valuable insights and make informed decisions based on data. This is the example table called customers : CREATE TABLE customers ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), email VARCHAR ( 100 ), address VARCHAR ( 100 ), city VARCHAR ( 50 ), state VARCHAR ( 50 ), zip_code VARCHAR ( 20 ) ); This table has columns for a customer's ID, first name, last name, email address, street address, city, state, and zip code. The id column is the primary key for the table, which means that each row in the table is uniquely identified by its value in the id column. This is the example of an orders table: CREATE TABLE orders ( order_id INT PRIMARY KEY , customer_id INT , order_date DATE , total_price DECIMAL ( 10 , 2 ) ); This table has four columns: order_id , customer_id , order_date , and total_price . The order_id column is the primary key of the table, which means that each row has a unique value in that column. The customer_id column is a foreign key that references the customer_id column in the customers table. This establishes a relationship between the two tables. The orders table contains information about each order placed by a customer. The customer_id column is used to link each order to a specific customer in the customers table. The order_date column contains the date that the order was placed, and the total_price column contains the total price of the order. By joining the orders table with the customers table on the customer_id column, we can retrieve information about both the customer and their order in a single query. Overview of Different SELECT Commands and Syntax The SELECT statement has a variety of options for retrieving and manipulating data. Here are some examples: The WHERE clause is used to filter data based on a specified condition: SELECT * FROM customers WHERE city = 'London' ; This statement retrieves all columns and rows from the customers table where the city is London . The ORDER BY clause is used to sort data by one or more columns: SELECT * FROM customers ORDER BY last_name ; This statement retrieves all columns and rows from the customers table, sorted by the last_name column. The GROUP BY clause is used to group data by one or more columns: SELECT city , COUNT ( * ) FROM customers GROUP BY city ; This statement retrieves the city column and a count of how many times each city appears in the customers table. The JOIN command is used to combine data from two or more tables: SELECT * FROM customers JOIN orders ON customers . customer_id = orders . customer_id ; This statement retrieves all columns and rows from both the customers and orders tables where the customer_id column matches in both tables.","title":"Basic Queries"},{"location":"sql/11_queries/#basic-queries","text":"In the world of database management, tables and queries go hand in hand. Queries are a fundamental component of any database, as they allow you to retrieve and manipulate data in meaningful ways. In this chapter, we will explore the basics of SQL queries and how they are used to extract data from tables. To demonstrate this, we will use two example tables, the orders and customers tables. These tables will be linked together using a foreign key to show how queries can retrieve data from multiple tables at once. Understanding queries and the relationship between tables is essential for effective database management, as it enables developers to extract valuable insights and make informed decisions based on data. This is the example table called customers : CREATE TABLE customers ( id INT PRIMARY KEY , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), email VARCHAR ( 100 ), address VARCHAR ( 100 ), city VARCHAR ( 50 ), state VARCHAR ( 50 ), zip_code VARCHAR ( 20 ) ); This table has columns for a customer's ID, first name, last name, email address, street address, city, state, and zip code. The id column is the primary key for the table, which means that each row in the table is uniquely identified by its value in the id column. This is the example of an orders table: CREATE TABLE orders ( order_id INT PRIMARY KEY , customer_id INT , order_date DATE , total_price DECIMAL ( 10 , 2 ) ); This table has four columns: order_id , customer_id , order_date , and total_price . The order_id column is the primary key of the table, which means that each row has a unique value in that column. The customer_id column is a foreign key that references the customer_id column in the customers table. This establishes a relationship between the two tables. The orders table contains information about each order placed by a customer. The customer_id column is used to link each order to a specific customer in the customers table. The order_date column contains the date that the order was placed, and the total_price column contains the total price of the order. By joining the orders table with the customers table on the customer_id column, we can retrieve information about both the customer and their order in a single query.","title":"Basic Queries"},{"location":"sql/11_queries/#overview-of-different-select-commands-and-syntax","text":"The SELECT statement has a variety of options for retrieving and manipulating data. Here are some examples: The WHERE clause is used to filter data based on a specified condition: SELECT * FROM customers WHERE city = 'London' ; This statement retrieves all columns and rows from the customers table where the city is London . The ORDER BY clause is used to sort data by one or more columns: SELECT * FROM customers ORDER BY last_name ; This statement retrieves all columns and rows from the customers table, sorted by the last_name column. The GROUP BY clause is used to group data by one or more columns: SELECT city , COUNT ( * ) FROM customers GROUP BY city ; This statement retrieves the city column and a count of how many times each city appears in the customers table. The JOIN command is used to combine data from two or more tables: SELECT * FROM customers JOIN orders ON customers . customer_id = orders . customer_id ; This statement retrieves all columns and rows from both the customers and orders tables where the customer_id column matches in both tables.","title":"Overview of Different SELECT Commands and Syntax"},{"location":"sql/12_company_db_1/","text":"Company Database Introduction In the modern era of technology, most businesses depend on software to manage and track various aspects of their operations. One of the most important types of software that businesses rely on is the database management system (DBMS). A DBMS allows companies to store, manage, and retrieve information in a structured and organized way. A company database is a type of DBMS that is specifically designed to help organizations store and manage information about their employees, customers, products, and services. A company database can be used for a wide range of purposes, such as tracking inventory, processing transactions, generating reports, and analyzing data. By keeping all the relevant information in a centralized location, a company database provides a more efficient and accurate way to manage and analyze data. This, in turn, enables businesses to make informed decisions based on reliable and up-to-date information. Having a well-designed and properly maintained company database is vital to the success of any business. A good company database can help to improve operational efficiency, streamline processes, and enhance customer satisfaction. It can also help organizations to identify trends, analyze performance, and make strategic decisions based on real data. In this chapter, we will explore the various components of a company database, including tables, fields, and relationships. We will also learn how to design and build a company database from scratch, as well as how to use SQL to retrieve, analyze, and manipulate data. By the end of this tutorial, you should have a good understanding how to build a database from scratch for your own business or project. Summary of the project This project will focus on building a company database that includes seven tables: * employees * departments * projects * department_projects * employee_projects * jobs * location These tables will be linked together using foreign keys and relationships, allowing organizations to easily access and manage information. The project will also include the creation of primary keys, indexes, and constraints to ensure the integrity and consistency of the data. This database will provide a robust platform for companies to store, organize, and access data in a way that enhances their ability to make data-driven decisions. Set up the project Start by opening your MySQL client and connecting to your server. In our case just start MAMP like in the installation section. Database creation Create a new database call company with the graphic interface like in the installation section or with the SQL command line : CREATE DATABASE company ; if you used the command line option run also this command : USE company; like you've guess it just tell to MySQL to use our database for the futur queries. Tables creation This is the SQL script that creates the necessary tables : -- Create Employees table CREATE TABLE Employees ( EmployeeID INT AUTO_INCREMENT PRIMARY KEY , FirstName VARCHAR ( 50 ), LastName VARCHAR ( 50 ), Email VARCHAR ( 50 ), Phone VARCHAR ( 20 ), HireDate DATE , Salary DECIMAL ( 10 , 2 ), CommissionPct DECIMAL ( 4 , 2 ), ManagerID INT , DepartmentID INT , JobID INT , LocationID INT , CONSTRAINT fk_manager FOREIGN KEY ( ManagerID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_department FOREIGN KEY ( DepartmentID ) REFERENCES Departments ( DepartmentID ), CONSTRAINT fk_job FOREIGN KEY ( JobID ) REFERENCES Jobs ( JobID ), CONSTRAINT fk_location FOREIGN KEY ( LocationID ) REFERENCES Locations ( LocationID ) ); -- Create Departments table CREATE TABLE Departments ( DepartmentID INT AUTO_INCREMENT PRIMARY KEY , DepartmentName VARCHAR ( 50 ), ManagerID INT , LocationID INT , CONSTRAINT fk_department_manager FOREIGN KEY ( ManagerID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_department_location FOREIGN KEY ( LocationID ) REFERENCES Locations ( LocationID ) ); -- Create Projects table CREATE TABLE Projects ( ProjectID INT AUTO_INCREMENT PRIMARY KEY , ProjectName VARCHAR ( 50 ), StartDate DATE , EndDate DATE , Budget DECIMAL ( 15 , 2 ) ); -- Create Department_Projects table CREATE TABLE Department_Projects ( DepartmentID INT , ProjectID INT , CONSTRAINT fk_department_project_department FOREIGN KEY ( DepartmentID ) REFERENCES Departments ( DepartmentID ), CONSTRAINT fk_department_project_project FOREIGN KEY ( ProjectID ) REFERENCES Projects ( ProjectID ) ); -- Create Employee_Projects table CREATE TABLE Employee_Projects ( EmployeeID INT , ProjectID INT , HoursWorked DECIMAL ( 8 , 2 ), CONSTRAINT fk_employee_project_employee FOREIGN KEY ( EmployeeID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_employee_project_project FOREIGN KEY ( ProjectID ) REFERENCES Projects ( ProjectID ) ); -- Create Jobs table CREATE TABLE Jobs ( JobID INT AUTO_INCREMENT PRIMARY KEY , JobTitle VARCHAR ( 50 ), MinSalary DECIMAL ( 10 , 2 ), MaxSalary DECIMAL ( 10 , 2 ) ); -- Create Locations table CREATE TABLE Locations ( LocationID INT AUTO_INCREMENT PRIMARY KEY , Address VARCHAR ( 50 ), City VARCHAR ( 50 ), StateProvince VARCHAR ( 50 ), Country VARCHAR ( 50 ), PostalCode VARCHAR ( 50 ) ); Note that the foreign keys are created using the CONSTRAINT keyword and the REFERENCES keyword to specify the table and column to which the key refers. The AUTO_INCREMENT keyword is used to specify that the primary key column should automatically increment for each new row. Insert data Then populate the tables with this script : -- Insert 10 employees INSERT INTO employees ( first_name , last_name , email , phone , hire_date , job_id , salary , manager_id , department_id ) VALUES ( 'John' , 'Doe' , 'johndoe@example.com' , '555-555-1234' , '2022-01-01' , 1 , 50000 , NULL , 1 ), ( 'Jane' , 'Doe' , 'janedoe@example.com' , '555-555-5678' , '2022-01-01' , 2 , 60000 , 1 , 1 ), ( 'Bob' , 'Smith' , 'bobsmith@example.com' , '555-555-9012' , '2022-02-01' , 3 , 75000 , 2 , 2 ), ( 'Alice' , 'Johnson' , 'alicejohnson@example.com' , '555-555-3456' , '2022-02-01' , 4 , 85000 , 2 , 2 ), ( 'Mark' , 'Lee' , 'marklee@example.com' , '555-555-7890' , '2022-03-01' , 5 , 95000 , 2 , 3 ), ( 'Emily' , 'Chen' , 'emilychen@example.com' , '555-555-2345' , '2022-03-01' , 5 , 80000 , 2 , 3 ), ( 'Sara' , 'Kim' , 'sarakim@example.com' , '555-555-6789' , '2022-04-01' , 6 , 70000 , 3 , 4 ), ( 'Michael' , 'Wu' , 'michaelwu@example.com' , '555-555-0123' , '2022-04-01' , 7 , 65000 , 3 , 4 ), ( 'David' , 'Nguyen' , 'davidnguyen@example.com' , '555-555-4567' , '2022-05-01' , 8 , 55000 , 4 , 5 ), ( 'Jennifer' , 'Garcia' , 'jennifergarcia@example.com' , '555-555-8901' , '2022-05-01' , 9 , 60000 , 4 , 5 ); -- Insert 3 departments INSERT INTO departments ( name , manager_id , location_id ) VALUES ( 'Engineering' , 1 , 1 ), ( 'Marketing' , 2 , 2 ), ( 'Sales' , 3 , 3 ); -- Insert 4 projects INSERT INTO projects ( name , start_date , end_date ) VALUES ( 'Project A' , '2022-01-01' , '2022-03-01' ), ( 'Project B' , '2022-02-01' , '2022-05-01' ), ( 'Project C' , '2022-03-01' , '2022-06-01' ), ( 'Project D' , '2022-04-01' , '2022-07-01' ); -- Insert 4 department_projects relationships INSERT INTO department_projects ( department_id , project_id ) VALUES ( 1 , 1 ), ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ); -- insert 10 rows into department_projects table INSERT INTO department_projects ( department_id , project_id ) VALUES ( 1 , 1 ), ( 1 , 2 ), ( 2 , 1 ), ( 2 , 3 ), ( 3 , 2 ), ( 3 , 3 ), ( 4 , 1 ), ( 4 , 2 ), ( 5 , 1 ), ( 5 , 3 ); -- insert 10 rows into jobs table INSERT INTO jobs ( title , min_salary , max_salary ) VALUES ( 'Manager' , 70000 , 120000 ), ( 'Salesperson' , 20000 , 40000 ), ( 'Developer' , 50000 , 100000 ), ( 'Accountant' , 35000 , 60000 ), ( 'HR Manager' , 45000 , 80000 ), ( 'Marketing Specialist' , 40000 , 75000 ), ( 'Administrative Assistant' , 25000 , 35000 ), ( 'Designer' , 45000 , 80000 ), ( 'Writer' , 30000 , 50000 ), ( 'Engineer' , 55000 , 90000 ); -- insert 10 rows into location table INSERT INTO location ( city , state , country ) VALUES ( 'New York' , 'NY' , 'USA' ), ( 'Los Angeles' , 'CA' , 'USA' ), ( 'San Francisco' , 'CA' , 'USA' ), ( 'Chicago' , 'IL' , 'USA' ), ( 'Houston' , 'TX' , 'USA' ), ( 'London' , NULL , 'England' ), ( 'Paris' , NULL , 'France' ), ( 'Berlin' , NULL , 'Germany' ), ( 'Sydney' , NULL , 'Australia' ), ( 'Tokyo' , NULL , 'Japan' ); -- Insert 10 rows into the employee_projects table INSERT INTO employee_projects ( employee_id , project_id , start_date , end_date ) VALUES ( 1 , 1 , '2021-01-01' , '2021-06-30' ), ( 1 , 2 , '2021-07-01' , '2021-12-31' ), ( 2 , 1 , '2021-01-01' , '2021-06-30' ), ( 2 , 3 , '2021-07-01' , '2021-12-31' ), ( 3 , 2 , '2021-01-01' , '2021-06-30' ), ( 3 , 3 , '2021-07-01' , '2021-12-31' ), ( 4 , 2 , '2021-01-01' , '2021-06-30' ), ( 4 , 1 , '2021-07-01' , '2021-12-31' ), ( 5 , 3 , '2021-01-01' , '2021-06-30' ), ( 5 , 1 , '2021-07-01' , '2021-12-31' ); More informations about CONSTRAINT and REFERENCES The CONSTRAINT and REFERENCES keywords are used to create foreign key constraints in SQL. A foreign key constraint is a rule that ensures the values in a column or set of columns in one table are matched by values in another table. In the example code I provided earlier, we used the CONSTRAINT keyword to create foreign key constraints between the employee_projects and employees tables, as well as between the department_projects and departments tables. Here is an example of the foreign key constraint between the employee_projects and employees tables: CREATE TABLE employee_projects ( id INT AUTO_INCREMENT PRIMARY KEY, employee_id INT, project_id INT, hours_worked DECIMAL(5,2), CONSTRAINT fk_employee_project_employee FOREIGN KEY (employee_id) REFERENCES employees (id) ); In this example, we first create the employee_projects table with the id, employee_id, project_id, and hours_worked columns. Then we use the CONSTRAINT keyword to create a foreign key constraint named fk_employee_project_employee. The FOREIGN KEY clause specifies the employee_id column as the foreign key column, and the REFERENCES clause specifies the employees table and the id column as the referenced column. This foreign key constraint ensures that every value in the employee_id column in the employee_projects table must exist in the id column of the employees table. The CONSTRAINT and REFERENCES keywords are powerful tools that allow you to establish relationships between tables in a database. They can help ensure data integrity and improve the accuracy and reliability of your queries. Tests some queries for verification --> TO TEST Let's take a look to ten example queries to verify the data in the seven tables: Retrieve all employees who work in the \"Sales\" department: SELECT e.first_name, e.last_name, d.department_name FROM employees e JOIN departments d ON e.department_id = d.department_id WHERE d.department_name = 'Sales'; Retrieve all projects that are assigned to the \"Marketing\" department: SELECT p.project_name, d.department_name FROM projects p JOIN department_projects dp ON p.project_id = dp.project_id JOIN departments d ON dp.department_id = d.department_id WHERE d.department_name = 'Marketing'; Retrieve all projects that have an estimated cost greater than $100,000: SELECT project_name, estimated_cost FROM projects WHERE estimated_cost > 100000; Retrieve all employees who are working on the \"Big Project\": SELECT e.first_name, e.last_name, p.project_name FROM employees e JOIN employee_projects ep ON e.employee_id = ep.employee_id JOIN projects p ON ep.project_id = p.project_id WHERE p.project_name = 'Big Project'; Retrieve all job titles and the number of employees who hold each job title: SELECT j.job_title, COUNT(*) AS num_employees FROM employees e JOIN jobs j ON e.job_id = j.job_id GROUP BY j.job_title; List all employees and their department: SELECT e.employee_id, e.first_name, e.last_name, d.department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id; List all departments and their location: SELECT d.department_name, l.city, l.state FROM departments d INNER JOIN location l ON d.location_id = l.location_id; List all projects and their department: SELECT p.project_id, p.project_name, d.department_name FROM projects p INNER JOIN department_projects dp ON p.project_id = dp.project_id INNER JOIN departments d ON dp.department_id = d.department_id; List all employees and the projects they are working on: SELECT e.first_name, e.last_name, p.project_name FROM employees e INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id INNER JOIN projects p ON ep.project_id = p.project_id; List all employees, their job title, and salary: SELECT e.first_name, e.last_name, j.job_title, j.salary FROM employees e INNER JOIN jobs j ON e.job_id = j.job_id; The importance of schema and organization Creating a schema and diagrams for a database is critical, especially when dealing with large databases with many tables and relationships. Without proper organization and documentation, it can be challenging to understand the structure and relationships between different tables. This is particularly true when many people are working on the database or when there is a lot of data being added and updated regularly. Having a clear schema and diagrams can help developers and users understand the structure and relationships of the data, leading to more efficient and effective use of the database. Additionally, it can help to identify and prevent errors in the data or in the database design itself. Overall, investing time in creating a clear schema and diagrams can save time and resources in the long run and make the database easier to manage and use. Wrap-up In this project, we learned how to create a company database using MySQL. We created 7 tables: employees, departments, projects, department_projects, employee_projects, jobs, and location. We added primary keys to all tables with auto-increment options for unique identification and added foreign keys to establish relationships between tables. We inserted data into each table and test some queries. We also learned about the importance of schema and diagrams for databases, especially as the number of tables and relationships grows, and the significance of foreign keys in ensuring data integrity and consistency.","title":"Build a company database from scratch"},{"location":"sql/12_company_db_1/#company-database-introduction","text":"In the modern era of technology, most businesses depend on software to manage and track various aspects of their operations. One of the most important types of software that businesses rely on is the database management system (DBMS). A DBMS allows companies to store, manage, and retrieve information in a structured and organized way. A company database is a type of DBMS that is specifically designed to help organizations store and manage information about their employees, customers, products, and services. A company database can be used for a wide range of purposes, such as tracking inventory, processing transactions, generating reports, and analyzing data. By keeping all the relevant information in a centralized location, a company database provides a more efficient and accurate way to manage and analyze data. This, in turn, enables businesses to make informed decisions based on reliable and up-to-date information. Having a well-designed and properly maintained company database is vital to the success of any business. A good company database can help to improve operational efficiency, streamline processes, and enhance customer satisfaction. It can also help organizations to identify trends, analyze performance, and make strategic decisions based on real data. In this chapter, we will explore the various components of a company database, including tables, fields, and relationships. We will also learn how to design and build a company database from scratch, as well as how to use SQL to retrieve, analyze, and manipulate data. By the end of this tutorial, you should have a good understanding how to build a database from scratch for your own business or project.","title":"Company Database Introduction"},{"location":"sql/12_company_db_1/#summary-of-the-project","text":"This project will focus on building a company database that includes seven tables: * employees * departments * projects * department_projects * employee_projects * jobs * location These tables will be linked together using foreign keys and relationships, allowing organizations to easily access and manage information. The project will also include the creation of primary keys, indexes, and constraints to ensure the integrity and consistency of the data. This database will provide a robust platform for companies to store, organize, and access data in a way that enhances their ability to make data-driven decisions.","title":"Summary of the project"},{"location":"sql/12_company_db_1/#set-up-the-project","text":"Start by opening your MySQL client and connecting to your server. In our case just start MAMP like in the installation section.","title":"Set up the project"},{"location":"sql/12_company_db_1/#database-creation","text":"Create a new database call company with the graphic interface like in the installation section or with the SQL command line : CREATE DATABASE company ; if you used the command line option run also this command : USE company; like you've guess it just tell to MySQL to use our database for the futur queries.","title":"Database creation"},{"location":"sql/12_company_db_1/#tables-creation","text":"This is the SQL script that creates the necessary tables : -- Create Employees table CREATE TABLE Employees ( EmployeeID INT AUTO_INCREMENT PRIMARY KEY , FirstName VARCHAR ( 50 ), LastName VARCHAR ( 50 ), Email VARCHAR ( 50 ), Phone VARCHAR ( 20 ), HireDate DATE , Salary DECIMAL ( 10 , 2 ), CommissionPct DECIMAL ( 4 , 2 ), ManagerID INT , DepartmentID INT , JobID INT , LocationID INT , CONSTRAINT fk_manager FOREIGN KEY ( ManagerID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_department FOREIGN KEY ( DepartmentID ) REFERENCES Departments ( DepartmentID ), CONSTRAINT fk_job FOREIGN KEY ( JobID ) REFERENCES Jobs ( JobID ), CONSTRAINT fk_location FOREIGN KEY ( LocationID ) REFERENCES Locations ( LocationID ) ); -- Create Departments table CREATE TABLE Departments ( DepartmentID INT AUTO_INCREMENT PRIMARY KEY , DepartmentName VARCHAR ( 50 ), ManagerID INT , LocationID INT , CONSTRAINT fk_department_manager FOREIGN KEY ( ManagerID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_department_location FOREIGN KEY ( LocationID ) REFERENCES Locations ( LocationID ) ); -- Create Projects table CREATE TABLE Projects ( ProjectID INT AUTO_INCREMENT PRIMARY KEY , ProjectName VARCHAR ( 50 ), StartDate DATE , EndDate DATE , Budget DECIMAL ( 15 , 2 ) ); -- Create Department_Projects table CREATE TABLE Department_Projects ( DepartmentID INT , ProjectID INT , CONSTRAINT fk_department_project_department FOREIGN KEY ( DepartmentID ) REFERENCES Departments ( DepartmentID ), CONSTRAINT fk_department_project_project FOREIGN KEY ( ProjectID ) REFERENCES Projects ( ProjectID ) ); -- Create Employee_Projects table CREATE TABLE Employee_Projects ( EmployeeID INT , ProjectID INT , HoursWorked DECIMAL ( 8 , 2 ), CONSTRAINT fk_employee_project_employee FOREIGN KEY ( EmployeeID ) REFERENCES Employees ( EmployeeID ), CONSTRAINT fk_employee_project_project FOREIGN KEY ( ProjectID ) REFERENCES Projects ( ProjectID ) ); -- Create Jobs table CREATE TABLE Jobs ( JobID INT AUTO_INCREMENT PRIMARY KEY , JobTitle VARCHAR ( 50 ), MinSalary DECIMAL ( 10 , 2 ), MaxSalary DECIMAL ( 10 , 2 ) ); -- Create Locations table CREATE TABLE Locations ( LocationID INT AUTO_INCREMENT PRIMARY KEY , Address VARCHAR ( 50 ), City VARCHAR ( 50 ), StateProvince VARCHAR ( 50 ), Country VARCHAR ( 50 ), PostalCode VARCHAR ( 50 ) ); Note that the foreign keys are created using the CONSTRAINT keyword and the REFERENCES keyword to specify the table and column to which the key refers. The AUTO_INCREMENT keyword is used to specify that the primary key column should automatically increment for each new row.","title":"Tables creation"},{"location":"sql/12_company_db_1/#insert-data","text":"Then populate the tables with this script : -- Insert 10 employees INSERT INTO employees ( first_name , last_name , email , phone , hire_date , job_id , salary , manager_id , department_id ) VALUES ( 'John' , 'Doe' , 'johndoe@example.com' , '555-555-1234' , '2022-01-01' , 1 , 50000 , NULL , 1 ), ( 'Jane' , 'Doe' , 'janedoe@example.com' , '555-555-5678' , '2022-01-01' , 2 , 60000 , 1 , 1 ), ( 'Bob' , 'Smith' , 'bobsmith@example.com' , '555-555-9012' , '2022-02-01' , 3 , 75000 , 2 , 2 ), ( 'Alice' , 'Johnson' , 'alicejohnson@example.com' , '555-555-3456' , '2022-02-01' , 4 , 85000 , 2 , 2 ), ( 'Mark' , 'Lee' , 'marklee@example.com' , '555-555-7890' , '2022-03-01' , 5 , 95000 , 2 , 3 ), ( 'Emily' , 'Chen' , 'emilychen@example.com' , '555-555-2345' , '2022-03-01' , 5 , 80000 , 2 , 3 ), ( 'Sara' , 'Kim' , 'sarakim@example.com' , '555-555-6789' , '2022-04-01' , 6 , 70000 , 3 , 4 ), ( 'Michael' , 'Wu' , 'michaelwu@example.com' , '555-555-0123' , '2022-04-01' , 7 , 65000 , 3 , 4 ), ( 'David' , 'Nguyen' , 'davidnguyen@example.com' , '555-555-4567' , '2022-05-01' , 8 , 55000 , 4 , 5 ), ( 'Jennifer' , 'Garcia' , 'jennifergarcia@example.com' , '555-555-8901' , '2022-05-01' , 9 , 60000 , 4 , 5 ); -- Insert 3 departments INSERT INTO departments ( name , manager_id , location_id ) VALUES ( 'Engineering' , 1 , 1 ), ( 'Marketing' , 2 , 2 ), ( 'Sales' , 3 , 3 ); -- Insert 4 projects INSERT INTO projects ( name , start_date , end_date ) VALUES ( 'Project A' , '2022-01-01' , '2022-03-01' ), ( 'Project B' , '2022-02-01' , '2022-05-01' ), ( 'Project C' , '2022-03-01' , '2022-06-01' ), ( 'Project D' , '2022-04-01' , '2022-07-01' ); -- Insert 4 department_projects relationships INSERT INTO department_projects ( department_id , project_id ) VALUES ( 1 , 1 ), ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ); -- insert 10 rows into department_projects table INSERT INTO department_projects ( department_id , project_id ) VALUES ( 1 , 1 ), ( 1 , 2 ), ( 2 , 1 ), ( 2 , 3 ), ( 3 , 2 ), ( 3 , 3 ), ( 4 , 1 ), ( 4 , 2 ), ( 5 , 1 ), ( 5 , 3 ); -- insert 10 rows into jobs table INSERT INTO jobs ( title , min_salary , max_salary ) VALUES ( 'Manager' , 70000 , 120000 ), ( 'Salesperson' , 20000 , 40000 ), ( 'Developer' , 50000 , 100000 ), ( 'Accountant' , 35000 , 60000 ), ( 'HR Manager' , 45000 , 80000 ), ( 'Marketing Specialist' , 40000 , 75000 ), ( 'Administrative Assistant' , 25000 , 35000 ), ( 'Designer' , 45000 , 80000 ), ( 'Writer' , 30000 , 50000 ), ( 'Engineer' , 55000 , 90000 ); -- insert 10 rows into location table INSERT INTO location ( city , state , country ) VALUES ( 'New York' , 'NY' , 'USA' ), ( 'Los Angeles' , 'CA' , 'USA' ), ( 'San Francisco' , 'CA' , 'USA' ), ( 'Chicago' , 'IL' , 'USA' ), ( 'Houston' , 'TX' , 'USA' ), ( 'London' , NULL , 'England' ), ( 'Paris' , NULL , 'France' ), ( 'Berlin' , NULL , 'Germany' ), ( 'Sydney' , NULL , 'Australia' ), ( 'Tokyo' , NULL , 'Japan' ); -- Insert 10 rows into the employee_projects table INSERT INTO employee_projects ( employee_id , project_id , start_date , end_date ) VALUES ( 1 , 1 , '2021-01-01' , '2021-06-30' ), ( 1 , 2 , '2021-07-01' , '2021-12-31' ), ( 2 , 1 , '2021-01-01' , '2021-06-30' ), ( 2 , 3 , '2021-07-01' , '2021-12-31' ), ( 3 , 2 , '2021-01-01' , '2021-06-30' ), ( 3 , 3 , '2021-07-01' , '2021-12-31' ), ( 4 , 2 , '2021-01-01' , '2021-06-30' ), ( 4 , 1 , '2021-07-01' , '2021-12-31' ), ( 5 , 3 , '2021-01-01' , '2021-06-30' ), ( 5 , 1 , '2021-07-01' , '2021-12-31' );","title":"Insert data"},{"location":"sql/12_company_db_1/#more-informations-about-constraint-and-references","text":"The CONSTRAINT and REFERENCES keywords are used to create foreign key constraints in SQL. A foreign key constraint is a rule that ensures the values in a column or set of columns in one table are matched by values in another table. In the example code I provided earlier, we used the CONSTRAINT keyword to create foreign key constraints between the employee_projects and employees tables, as well as between the department_projects and departments tables. Here is an example of the foreign key constraint between the employee_projects and employees tables: CREATE TABLE employee_projects ( id INT AUTO_INCREMENT PRIMARY KEY, employee_id INT, project_id INT, hours_worked DECIMAL(5,2), CONSTRAINT fk_employee_project_employee FOREIGN KEY (employee_id) REFERENCES employees (id) ); In this example, we first create the employee_projects table with the id, employee_id, project_id, and hours_worked columns. Then we use the CONSTRAINT keyword to create a foreign key constraint named fk_employee_project_employee. The FOREIGN KEY clause specifies the employee_id column as the foreign key column, and the REFERENCES clause specifies the employees table and the id column as the referenced column. This foreign key constraint ensures that every value in the employee_id column in the employee_projects table must exist in the id column of the employees table. The CONSTRAINT and REFERENCES keywords are powerful tools that allow you to establish relationships between tables in a database. They can help ensure data integrity and improve the accuracy and reliability of your queries.","title":"More informations about CONSTRAINT and REFERENCES"},{"location":"sql/12_company_db_1/#tests-some-queries-for-verification-to-test","text":"Let's take a look to ten example queries to verify the data in the seven tables: Retrieve all employees who work in the \"Sales\" department: SELECT e.first_name, e.last_name, d.department_name FROM employees e JOIN departments d ON e.department_id = d.department_id WHERE d.department_name = 'Sales'; Retrieve all projects that are assigned to the \"Marketing\" department: SELECT p.project_name, d.department_name FROM projects p JOIN department_projects dp ON p.project_id = dp.project_id JOIN departments d ON dp.department_id = d.department_id WHERE d.department_name = 'Marketing'; Retrieve all projects that have an estimated cost greater than $100,000: SELECT project_name, estimated_cost FROM projects WHERE estimated_cost > 100000; Retrieve all employees who are working on the \"Big Project\": SELECT e.first_name, e.last_name, p.project_name FROM employees e JOIN employee_projects ep ON e.employee_id = ep.employee_id JOIN projects p ON ep.project_id = p.project_id WHERE p.project_name = 'Big Project'; Retrieve all job titles and the number of employees who hold each job title: SELECT j.job_title, COUNT(*) AS num_employees FROM employees e JOIN jobs j ON e.job_id = j.job_id GROUP BY j.job_title; List all employees and their department: SELECT e.employee_id, e.first_name, e.last_name, d.department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id; List all departments and their location: SELECT d.department_name, l.city, l.state FROM departments d INNER JOIN location l ON d.location_id = l.location_id; List all projects and their department: SELECT p.project_id, p.project_name, d.department_name FROM projects p INNER JOIN department_projects dp ON p.project_id = dp.project_id INNER JOIN departments d ON dp.department_id = d.department_id; List all employees and the projects they are working on: SELECT e.first_name, e.last_name, p.project_name FROM employees e INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id INNER JOIN projects p ON ep.project_id = p.project_id; List all employees, their job title, and salary: SELECT e.first_name, e.last_name, j.job_title, j.salary FROM employees e INNER JOIN jobs j ON e.job_id = j.job_id;","title":"Tests some queries for verification --&gt; TO TEST"},{"location":"sql/12_company_db_1/#the-importance-of-schema-and-organization","text":"Creating a schema and diagrams for a database is critical, especially when dealing with large databases with many tables and relationships. Without proper organization and documentation, it can be challenging to understand the structure and relationships between different tables. This is particularly true when many people are working on the database or when there is a lot of data being added and updated regularly. Having a clear schema and diagrams can help developers and users understand the structure and relationships of the data, leading to more efficient and effective use of the database. Additionally, it can help to identify and prevent errors in the data or in the database design itself. Overall, investing time in creating a clear schema and diagrams can save time and resources in the long run and make the database easier to manage and use.","title":"The importance of schema and organization"},{"location":"sql/12_company_db_1/#wrap-up","text":"In this project, we learned how to create a company database using MySQL. We created 7 tables: employees, departments, projects, department_projects, employee_projects, jobs, and location. We added primary keys to all tables with auto-increment options for unique identification and added foreign keys to establish relationships between tables. We inserted data into each table and test some queries. We also learned about the importance of schema and diagrams for databases, especially as the number of tables and relationships grows, and the significance of foreign keys in ensuring data integrity and consistency.","title":"Wrap-up"},{"location":"sql/13_queries_db/","text":"More queries In this chapter we will be working on the MySQL Sample Database. The MySQL Sample Database provides a sample database called \"employees\" that you can use to practice SQL queries. You can download the database and load it into your MySQL server. Link to the database Download the database and load it into MySQLWorkbench \ud83d\udea7 You need to run MySQL server (with MAMP for example) before lunching MySQLWorkbench \ud83d\udea7 Dowload the MySQL Sample Database You can follow the documentation above or just go to : link and download the repo as zip. Load MySQL Sample Database into MySQLWorkbench When you have downloaded the git repo as zip go to your Download files and unzip the folder and open MySQLWorkbench then go to > File > Run SQL Scripts and load the file employees.sql Run a test query We will running a test query for testing our database, we must open a new file for writing our query for that you can click on the file icon button like in the screen below or go to File > New Query Tab use employees ; SELECT d . dept_name , AVG ( s . salary ) AS avg_salary FROM departments d INNER JOIN dept_emp de ON d . dept_no = de . dept_no INNER JOIN salaries s ON de . emp_no = s . emp_no GROUP BY d . dept_name ; Notice that, the first line use employees; is here to tell to our software to use the employees database then you can see the result of our test query in the window bellow : We will study in detail this query later don't worry. In-depth look at more basic queries in SQL We encourage you to pratice the queries into MySQLWorkbench, let's review some basics ! SELECT statement: The SELECT statement is used to retrieve data from a table in a database. It can take multiple arguments, which are separated by commas. The * character can be used as a shorthand to select all columns in a table . We often use the AS keyword is used to assign a name to a column in the output. SELECT first_name , last_name , salary AS \"Annual Salary\" FROM employees ; In this example, the AS keyword is used to assign a new name to the \"salary\" column in the output. The new name is \"Annual Salary\". This query selects the first name, last name, and salary of all the employees in the \"employees\" table, but it renames the \"salary\" column as \"Annual Salary\" in the output. Note that the AS keyword is optional, and you can also use a space or equals sign to assign a name to a column. For example, the following query is equivalent to the one above: SELECT first_name , last_name , salary \"Annual Salary\" FROM employees ; In both cases, the output column is named \"Annual Salary\". WHERE clause: The WHERE clause is used to filter the results returned by a SELECT statement. It contains a logical expression that evaluates to true or false for each row in the table. SELECT * FROM orders WHERE order_date >= '2022-01-01' ; This query selects all the columns from the \"orders\" table where the order date is on or after January 1, 2022. This is an other example : SELECT first_name , last_name , salary * 12 AS \"Annual Salary\" FROM employees WHERE hire_date >= '2005-01-01' ; In this example, the WHERE clause is used to filter the results to include only employees hired on or after January 1, 2005. The AS keyword is used to assign a new name to the \"salary * 12\" expression in the output. The new name is \"Annual Salary\". This query selects the first name, last name, and annual salary of all the employees in the \"employees\" table who were hired on or after January 1, 2005. The annual salary is calculated by multiplying the monthly salary by 12. Note that the order of the SQL clauses matters. The WHERE clause is used to filter the results before the AS keyword is used to assign a new name to the output column. JOIN clause The JOIN clause is used to combine rows from two or more tables based on a related column between them. Here's an example: SELECT customers . first_name , customers . last_name , orders . order_date FROM customers INNER JOIN orders ON customers . customer_id = orders . customer_id ; This query selects the first name, last name, and order date of all customers who have placed an order. The results are obtained by joining the \"customers\" and \"orders\" tables on the customer_id column. We will discuss nore about JOIN later don't worry. ORDER BY clause The ORDER BY clause is used to sort the results returned by a SELECT statement based on one or more columns. Here's an example: SELECT product_name , unit_price FROM products ORDER BY unit_price DESC ; This query selects the product name and unit price of all the products in the \"products\" table and sorts the results in descending order based on the unit price. GROUP BY clause The GROUP BY clause is used to group the rows returned by a SELECT statement based on one or more columns. The columns listed in the SELECT statement must be either in the GROUP BY clause or have an aggregate function applied to them. Aggregate functions like COUNT, SUM, AVG, MAX, and MIN can be used to perform calculations on the grouped data. SELECT category_id , COUNT ( * ) AS num_products FROM products GROUP BY category_id ; This query groups the products in the \"products\" table by their category and counts the number of products in each category. The COUNT(*) function is used to count the number of rows in each group, and the AS keyword is used to assign the name \"num_products\" to the output column. Here an other example : SELECT department , AVG ( salary ) AS \"Average Salary\" FROM employees GROUP BY department ORDER BY \"Average Salary\" DESC ; In this example, the GROUP BY clause is used to group the employees by department, and the AVG() function is used to calculate the average salary for each department. The AS keyword is used to assign a new name to the \"AVG(salary)\" expression in the output. The new name is \"Average Salary\". The ORDER BY keyword is used to sort the results in descending order based on the \"Average Salary\" column. Note that we need to enclose the output column name in double quotes because it contains a space. This query selects the department and average salary of all the employees in the \"employees\" table, grouped by department, and sorted in descending order by average salary. Note that when using the GROUP BY clause, the SELECT statement can only include the columns that are specified in the GROUP BY clause or have an aggregate function applied to them. Any other columns will result in an error, unless they are included in an aggregate function. In this example, we only select the department and average salary columns because the department column is included in the GROUP BY clause. Wrap up These are just a few examples of basic SQL queries, but they provide a good foundation for building more complex queries. By combining these statements with other SQL clauses, you can perform powerful data analysis and extract valuable insights from your data. Let's summarize what we've learn in this section : The SELECT statement is used to retrieve data from a table in a database. It can take multiple arguments, which are separated by commas. The * character can be used as a shorthand to select all columns in a table. The AS keyword is used to assign a name to a column in the output. The WHERE clause is used to filter the results returned by a SELECT statement. It contains a logical expression that evaluates to true or false for each row in the table. The JOIN clause is used to combine rows from two or more tables based on a related column between them. It can be used to join tables on a primary key/foreign key relationship or on a common column. The GROUP BY clause is used to group the rows returned by a SELECT statement based on one or more columns. The columns listed in the SELECT statement must be either in the GROUP BY clause or have an aggregate function applied to them. Aggregate functions like COUNT, SUM, AVG, MAX, and MIN can be used to perform calculations on the grouped data. The ORDER BY clause is used to sort the results returned by a SELECT statement based on one or more columns. It can be used to sort in ascending (ASC) or descending (DESC) order. The AS keyword is used to assign a new name to a column or an expression in the output. SQL keywords are not case-sensitive, but it is a best practice to use them in uppercase to make the code more readable. The order of the SQL clauses matters, and it can affect the output of the query.","title":"More Queries"},{"location":"sql/13_queries_db/#more-queries","text":"In this chapter we will be working on the MySQL Sample Database. The MySQL Sample Database provides a sample database called \"employees\" that you can use to practice SQL queries. You can download the database and load it into your MySQL server. Link to the database","title":"More queries"},{"location":"sql/13_queries_db/#download-the-database-and-load-it-into-mysqlworkbench","text":"\ud83d\udea7 You need to run MySQL server (with MAMP for example) before lunching MySQLWorkbench \ud83d\udea7","title":"Download the database and load it into MySQLWorkbench"},{"location":"sql/13_queries_db/#dowload-the-mysql-sample-database","text":"You can follow the documentation above or just go to : link and download the repo as zip.","title":"Dowload the MySQL Sample Database"},{"location":"sql/13_queries_db/#load-mysql-sample-database-into-mysqlworkbench","text":"When you have downloaded the git repo as zip go to your Download files and unzip the folder and open MySQLWorkbench then go to > File > Run SQL Scripts and load the file employees.sql","title":"Load MySQL Sample Database into MySQLWorkbench"},{"location":"sql/13_queries_db/#run-a-test-query","text":"We will running a test query for testing our database, we must open a new file for writing our query for that you can click on the file icon button like in the screen below or go to File > New Query Tab use employees ; SELECT d . dept_name , AVG ( s . salary ) AS avg_salary FROM departments d INNER JOIN dept_emp de ON d . dept_no = de . dept_no INNER JOIN salaries s ON de . emp_no = s . emp_no GROUP BY d . dept_name ; Notice that, the first line use employees; is here to tell to our software to use the employees database then you can see the result of our test query in the window bellow : We will study in detail this query later don't worry.","title":"Run a test query"},{"location":"sql/13_queries_db/#in-depth-look-at-more-basic-queries-in-sql","text":"We encourage you to pratice the queries into MySQLWorkbench, let's review some basics !","title":"In-depth look at more basic queries in SQL"},{"location":"sql/13_queries_db/#select-statement","text":"The SELECT statement is used to retrieve data from a table in a database. It can take multiple arguments, which are separated by commas. The * character can be used as a shorthand to select all columns in a table . We often use the AS keyword is used to assign a name to a column in the output. SELECT first_name , last_name , salary AS \"Annual Salary\" FROM employees ; In this example, the AS keyword is used to assign a new name to the \"salary\" column in the output. The new name is \"Annual Salary\". This query selects the first name, last name, and salary of all the employees in the \"employees\" table, but it renames the \"salary\" column as \"Annual Salary\" in the output. Note that the AS keyword is optional, and you can also use a space or equals sign to assign a name to a column. For example, the following query is equivalent to the one above: SELECT first_name , last_name , salary \"Annual Salary\" FROM employees ; In both cases, the output column is named \"Annual Salary\".","title":"SELECT statement:"},{"location":"sql/13_queries_db/#where-clause","text":"The WHERE clause is used to filter the results returned by a SELECT statement. It contains a logical expression that evaluates to true or false for each row in the table. SELECT * FROM orders WHERE order_date >= '2022-01-01' ; This query selects all the columns from the \"orders\" table where the order date is on or after January 1, 2022. This is an other example : SELECT first_name , last_name , salary * 12 AS \"Annual Salary\" FROM employees WHERE hire_date >= '2005-01-01' ; In this example, the WHERE clause is used to filter the results to include only employees hired on or after January 1, 2005. The AS keyword is used to assign a new name to the \"salary * 12\" expression in the output. The new name is \"Annual Salary\". This query selects the first name, last name, and annual salary of all the employees in the \"employees\" table who were hired on or after January 1, 2005. The annual salary is calculated by multiplying the monthly salary by 12. Note that the order of the SQL clauses matters. The WHERE clause is used to filter the results before the AS keyword is used to assign a new name to the output column.","title":"WHERE clause:"},{"location":"sql/13_queries_db/#join-clause","text":"The JOIN clause is used to combine rows from two or more tables based on a related column between them. Here's an example: SELECT customers . first_name , customers . last_name , orders . order_date FROM customers INNER JOIN orders ON customers . customer_id = orders . customer_id ; This query selects the first name, last name, and order date of all customers who have placed an order. The results are obtained by joining the \"customers\" and \"orders\" tables on the customer_id column. We will discuss nore about JOIN later don't worry.","title":"JOIN clause"},{"location":"sql/13_queries_db/#order-by-clause","text":"The ORDER BY clause is used to sort the results returned by a SELECT statement based on one or more columns. Here's an example: SELECT product_name , unit_price FROM products ORDER BY unit_price DESC ; This query selects the product name and unit price of all the products in the \"products\" table and sorts the results in descending order based on the unit price.","title":"ORDER BY clause"},{"location":"sql/13_queries_db/#group-by-clause","text":"The GROUP BY clause is used to group the rows returned by a SELECT statement based on one or more columns. The columns listed in the SELECT statement must be either in the GROUP BY clause or have an aggregate function applied to them. Aggregate functions like COUNT, SUM, AVG, MAX, and MIN can be used to perform calculations on the grouped data. SELECT category_id , COUNT ( * ) AS num_products FROM products GROUP BY category_id ; This query groups the products in the \"products\" table by their category and counts the number of products in each category. The COUNT(*) function is used to count the number of rows in each group, and the AS keyword is used to assign the name \"num_products\" to the output column. Here an other example : SELECT department , AVG ( salary ) AS \"Average Salary\" FROM employees GROUP BY department ORDER BY \"Average Salary\" DESC ; In this example, the GROUP BY clause is used to group the employees by department, and the AVG() function is used to calculate the average salary for each department. The AS keyword is used to assign a new name to the \"AVG(salary)\" expression in the output. The new name is \"Average Salary\". The ORDER BY keyword is used to sort the results in descending order based on the \"Average Salary\" column. Note that we need to enclose the output column name in double quotes because it contains a space. This query selects the department and average salary of all the employees in the \"employees\" table, grouped by department, and sorted in descending order by average salary. Note that when using the GROUP BY clause, the SELECT statement can only include the columns that are specified in the GROUP BY clause or have an aggregate function applied to them. Any other columns will result in an error, unless they are included in an aggregate function. In this example, we only select the department and average salary columns because the department column is included in the GROUP BY clause.","title":"GROUP BY clause"},{"location":"sql/13_queries_db/#wrap-up","text":"These are just a few examples of basic SQL queries, but they provide a good foundation for building more complex queries. By combining these statements with other SQL clauses, you can perform powerful data analysis and extract valuable insights from your data. Let's summarize what we've learn in this section : The SELECT statement is used to retrieve data from a table in a database. It can take multiple arguments, which are separated by commas. The * character can be used as a shorthand to select all columns in a table. The AS keyword is used to assign a name to a column in the output. The WHERE clause is used to filter the results returned by a SELECT statement. It contains a logical expression that evaluates to true or false for each row in the table. The JOIN clause is used to combine rows from two or more tables based on a related column between them. It can be used to join tables on a primary key/foreign key relationship or on a common column. The GROUP BY clause is used to group the rows returned by a SELECT statement based on one or more columns. The columns listed in the SELECT statement must be either in the GROUP BY clause or have an aggregate function applied to them. Aggregate functions like COUNT, SUM, AVG, MAX, and MIN can be used to perform calculations on the grouped data. The ORDER BY clause is used to sort the results returned by a SELECT statement based on one or more columns. It can be used to sort in ascending (ASC) or descending (DESC) order. The AS keyword is used to assign a new name to a column or an expression in the output. SQL keywords are not case-sensitive, but it is a best practice to use them in uppercase to make the code more readable. The order of the SQL clauses matters, and it can affect the output of the query.","title":"Wrap up"},{"location":"sql/14_functions/","text":"SQL Functions Introduction to Functions in SQL SQL functions are built-in functions that are used to perform operations on data in a database. They take one or more arguments as input, perform a specific operation, and return a result. Functions can be used in SELECT, WHERE, HAVING, and ORDER BY clauses of a SQL query. There are many different types of functions in SQL, including aggregate functions, scalar functions, date and time functions, and string functions. Each type of function performs a specific operation on data and returns a result. Overall, SQL functions are essential for data analysis because they allow you to perform complex calculations, filter data based on specific criteria, clean up messy data, aggregate data to provide insights into trends and patterns, and transform data from one format to another. By mastering SQL functions, you can become a more effective data analyst and make more informed decisions based on your data. Overview of Different Types of Functions and Examples Aggregate Functions Aggregate functions are used to perform calculations on groups of rows and return a single value. Some common aggregate functions are: COUNT() : returns the number of rows in a table or the number of non-null values in a column. SELECT COUNT ( * ) as num_employees FROM employees ; This query counts the number of rows in the \"employees\" table and assigns the name \"num_employees\" to the output column. SUM() : returns the sum of values in a column. SELECT SUM(salary) as total_salary FROM salaries; This query calculates the total salary of all employees in the \"salaries\" table and assigns the name \"total_salary\" to the output column. AVG() : returns the average value of a column. SELECT AVG ( salary ) as avg_salary FROM salaries ; This query calculates the average salary of all employees in the \"salaries\" table and assigns the name \"avg_salary\" to the output column. MAX() : returns the maximum value in a column. SELECT MAX ( salary ) as max_salary FROM salaries ; This query finds the highest salary in the \"salaries\" table and assigns the name \"max_salary\" to the output column. MIN() : returns the minimum value in a column. SELECT MIN ( salary ) as min_salary FROM salaries ; This query finds the lowest salary in the \"salaries\" table and assigns the name \"min_salary\" to the output column. Scalar Functions Scalar functions are used to perform operations on individual values and return a single value. Some common scalar functions are: UPPER() : converts a string to uppercase. SELECT UPPER ( first_name ) as upper_first_name FROM employees ; This query converts the first name of all employees in the \"employees\" table to uppercase and assigns the name \"upper_first_name\" to the output column. LOWER() : converts a string to lowercase. SELECT LOWER ( last_name ) as lower_last_name FROM employees ; This query converts the last name of all employees in the \"employees\" table to lowercase and assigns the name \"lower_last_name\" to the output column. LENGTH() : returns the length of a string. SELECT first_name , LENGTH ( first_name ) as name_length FROM employees ; This query returns the first name of all employees in the \"employees\" table, and calculates the length of each name and assigns the name \"name_length\" to the output column. Date and Time Functions Date and time functions are used to perform operations on date and time values. Some common date and time functions are: DATE() : extracts the date part from a datetime value. SELECT hire_date , DATE ( hire_date ) as hire_date_only FROM employees ; This query extracts the date part from the \"hire_date\" column of the \"employees\" table and assigns the name \"hire_date_only\" to the output column. YEAR() : returns the year from a date value. SELECT hire_date , YEAR ( hire_date ) as hire_year FROM employees ; This query returns the \"hire_date\" column of the \"employees\" table and calculates the year each employee was hired. The name \"hire_year\" is assigned to the output column. Same MONTH() function : SELECT hire_date , MONTH ( hire_date ) as hire_month FROM employees ; This query returns the \"hire_date\" column of the \"employees\" table and calculates the month each employee was hired. The name \"hire_month\" is assigned to the output column. String Functions String functions are used to perform operations on string values. Some common string functions are: CONCAT(): concatenates two or more strings together. SELECT CONCAT ( first_name , ' ' , last_name ) as full_name FROM employees ; This query combines the first name and last name columns of the \"employees\" table and assigns the name \"full_name\" to the output column. LEFT(): returns the leftmost characters of a string. SELECT first_name , LEFT ( first_name , 3 ) as initial FROM employees ; This query returns the first name of all employees in the \"employees\" table and extracts the first three characters of each name. The name \"initial\" is assigned to the output column. REPLACE(): replaces a substring in a string with another substring. SELECT REPLACE ( email , 'gmail' , 'yahoo' ) as new_email FROM employees ; This query returns the email column of the \"employees\" table and replaces the substring 'gmail' with 'yahoo' in each email address. The name \"new_email\" is assigned to the output column. Mix up Let's take a look at three examples of SQL queries that use a mix of functions on the \"employees\" database again. Find the average salary of employees by department, and round the results to two decimal places: SELECT department , ROUND ( AVG ( salary ), 2 ) as avg_salary FROM employees JOIN dept_emp ON employees . emp_no = dept_emp . emp_no JOIN departments ON dept_emp . dept_no = departments . dept_no GROUP BY department ; This query joins the \"employees\", \"dept_emp\", and \"departments\" tables, and uses the AVG() function to calculate the average salary of employees in each department. The ROUND() function is used to round the results to two decimal places. The output includes the department name and the average salary for each department. Don't worry about the JOIN clause we will get to it in detail later. Find the top 10 most common first names among employees, and show the number of employees with each name: SELECT first_name , COUNT ( * ) as num_employees FROM employees GROUP BY first_name ORDER BY num_employees DESC LIMIT 10 ; This query uses the COUNT() function to count the number of employees with each first name, and the GROUP BY clause to group the results by first name. The ORDER BY clause is used to sort the results in descending order by the number of employees, and the LIMIT clause is used to show only the top 10 results. Find the number of employees hired in each year, and show the results as a percentage of the total number of employees: SELECT YEAR ( hire_date ) as hire_year , COUNT ( * ) / ( SELECT COUNT ( * ) FROM employees ) * 100 as percentage FROM employees GROUP BY hire_year ; This query uses the YEAR() function to extract the year from the hire date of each employee, and the COUNT() function to count the number of employees hired in each year. The subquery (SELECT COUNT(*) FROM employees) is used to calculate the total number of employees in the \"employees\" table. The percentage of employees hired in each year is calculated by dividing the count by the total number of employees and multiplying by 100. The output includes the hire year and the percentage of employees hired in that year. Find the number of employees born in each month, and sort the results by month: SELECT MONTH ( birth_date ) as birth_month , COUNT ( * ) as num_employees FROM employees GROUP BY birth_month ORDER BY birth_month ; This query uses the MONTH() function to extract the month from the birth date of each employee, and the COUNT() function to count the number of employees born in each month. The GROUP BY clause is used to group the results by birth month, and the ORDER BY clause is used to sort the results by month. Find the number of employees who were hired in each year, and show the results as a bar chart: SELECT YEAR ( hire_date ) as hire_year , COUNT ( * ) as num_employees FROM employees GROUP BY hire_year ; This query uses the YEAR() function to extract the year from the hire date of each employee, and the COUNT() function to count the number of employees hired in each year. The GROUP BY clause is used to group the results by hire year. You can visualize the results as a bar chart in a data visualization tool, such as Tableau or Power BI, to see the distribution of hires over time. Conclusion SQL functions are powerful tools that allow you to perform operations on data and return meaningful results. They can be used to calculate aggregate values, manipulate strings, and work with date and time values. By understanding the different types of functions available in SQL and how to use them in queries, you can perform complex data analysis and retrieve valuable insights from your data.","title":"SQL Functions"},{"location":"sql/14_functions/#sql-functions","text":"","title":"SQL Functions"},{"location":"sql/14_functions/#introduction-to-functions-in-sql","text":"SQL functions are built-in functions that are used to perform operations on data in a database. They take one or more arguments as input, perform a specific operation, and return a result. Functions can be used in SELECT, WHERE, HAVING, and ORDER BY clauses of a SQL query. There are many different types of functions in SQL, including aggregate functions, scalar functions, date and time functions, and string functions. Each type of function performs a specific operation on data and returns a result. Overall, SQL functions are essential for data analysis because they allow you to perform complex calculations, filter data based on specific criteria, clean up messy data, aggregate data to provide insights into trends and patterns, and transform data from one format to another. By mastering SQL functions, you can become a more effective data analyst and make more informed decisions based on your data.","title":"Introduction to Functions in SQL"},{"location":"sql/14_functions/#overview-of-different-types-of-functions-and-examples","text":"","title":"Overview of Different Types of Functions and Examples"},{"location":"sql/14_functions/#aggregate-functions","text":"Aggregate functions are used to perform calculations on groups of rows and return a single value. Some common aggregate functions are: COUNT() : returns the number of rows in a table or the number of non-null values in a column. SELECT COUNT ( * ) as num_employees FROM employees ; This query counts the number of rows in the \"employees\" table and assigns the name \"num_employees\" to the output column. SUM() : returns the sum of values in a column. SELECT SUM(salary) as total_salary FROM salaries; This query calculates the total salary of all employees in the \"salaries\" table and assigns the name \"total_salary\" to the output column. AVG() : returns the average value of a column. SELECT AVG ( salary ) as avg_salary FROM salaries ; This query calculates the average salary of all employees in the \"salaries\" table and assigns the name \"avg_salary\" to the output column. MAX() : returns the maximum value in a column. SELECT MAX ( salary ) as max_salary FROM salaries ; This query finds the highest salary in the \"salaries\" table and assigns the name \"max_salary\" to the output column. MIN() : returns the minimum value in a column. SELECT MIN ( salary ) as min_salary FROM salaries ; This query finds the lowest salary in the \"salaries\" table and assigns the name \"min_salary\" to the output column.","title":"Aggregate Functions"},{"location":"sql/14_functions/#scalar-functions","text":"Scalar functions are used to perform operations on individual values and return a single value. Some common scalar functions are: UPPER() : converts a string to uppercase. SELECT UPPER ( first_name ) as upper_first_name FROM employees ; This query converts the first name of all employees in the \"employees\" table to uppercase and assigns the name \"upper_first_name\" to the output column. LOWER() : converts a string to lowercase. SELECT LOWER ( last_name ) as lower_last_name FROM employees ; This query converts the last name of all employees in the \"employees\" table to lowercase and assigns the name \"lower_last_name\" to the output column. LENGTH() : returns the length of a string. SELECT first_name , LENGTH ( first_name ) as name_length FROM employees ; This query returns the first name of all employees in the \"employees\" table, and calculates the length of each name and assigns the name \"name_length\" to the output column.","title":"Scalar Functions"},{"location":"sql/14_functions/#date-and-time-functions","text":"Date and time functions are used to perform operations on date and time values. Some common date and time functions are: DATE() : extracts the date part from a datetime value. SELECT hire_date , DATE ( hire_date ) as hire_date_only FROM employees ; This query extracts the date part from the \"hire_date\" column of the \"employees\" table and assigns the name \"hire_date_only\" to the output column. YEAR() : returns the year from a date value. SELECT hire_date , YEAR ( hire_date ) as hire_year FROM employees ; This query returns the \"hire_date\" column of the \"employees\" table and calculates the year each employee was hired. The name \"hire_year\" is assigned to the output column. Same MONTH() function : SELECT hire_date , MONTH ( hire_date ) as hire_month FROM employees ; This query returns the \"hire_date\" column of the \"employees\" table and calculates the month each employee was hired. The name \"hire_month\" is assigned to the output column.","title":"Date and Time Functions"},{"location":"sql/14_functions/#string-functions","text":"String functions are used to perform operations on string values. Some common string functions are: CONCAT(): concatenates two or more strings together. SELECT CONCAT ( first_name , ' ' , last_name ) as full_name FROM employees ; This query combines the first name and last name columns of the \"employees\" table and assigns the name \"full_name\" to the output column. LEFT(): returns the leftmost characters of a string. SELECT first_name , LEFT ( first_name , 3 ) as initial FROM employees ; This query returns the first name of all employees in the \"employees\" table and extracts the first three characters of each name. The name \"initial\" is assigned to the output column. REPLACE(): replaces a substring in a string with another substring. SELECT REPLACE ( email , 'gmail' , 'yahoo' ) as new_email FROM employees ; This query returns the email column of the \"employees\" table and replaces the substring 'gmail' with 'yahoo' in each email address. The name \"new_email\" is assigned to the output column.","title":"String Functions"},{"location":"sql/14_functions/#mix-up","text":"Let's take a look at three examples of SQL queries that use a mix of functions on the \"employees\" database again.","title":"Mix up"},{"location":"sql/14_functions/#find-the-average-salary-of-employees-by-department-and-round-the-results-to-two-decimal-places","text":"SELECT department , ROUND ( AVG ( salary ), 2 ) as avg_salary FROM employees JOIN dept_emp ON employees . emp_no = dept_emp . emp_no JOIN departments ON dept_emp . dept_no = departments . dept_no GROUP BY department ; This query joins the \"employees\", \"dept_emp\", and \"departments\" tables, and uses the AVG() function to calculate the average salary of employees in each department. The ROUND() function is used to round the results to two decimal places. The output includes the department name and the average salary for each department. Don't worry about the JOIN clause we will get to it in detail later.","title":"Find the average salary of employees by department, and round the results to two decimal places:"},{"location":"sql/14_functions/#find-the-top-10-most-common-first-names-among-employees-and-show-the-number-of-employees-with-each-name","text":"SELECT first_name , COUNT ( * ) as num_employees FROM employees GROUP BY first_name ORDER BY num_employees DESC LIMIT 10 ; This query uses the COUNT() function to count the number of employees with each first name, and the GROUP BY clause to group the results by first name. The ORDER BY clause is used to sort the results in descending order by the number of employees, and the LIMIT clause is used to show only the top 10 results.","title":"Find the top 10 most common first names among employees, and show the number of employees with each name:"},{"location":"sql/14_functions/#find-the-number-of-employees-hired-in-each-year-and-show-the-results-as-a-percentage-of-the-total-number-of-employees","text":"SELECT YEAR ( hire_date ) as hire_year , COUNT ( * ) / ( SELECT COUNT ( * ) FROM employees ) * 100 as percentage FROM employees GROUP BY hire_year ; This query uses the YEAR() function to extract the year from the hire date of each employee, and the COUNT() function to count the number of employees hired in each year. The subquery (SELECT COUNT(*) FROM employees) is used to calculate the total number of employees in the \"employees\" table. The percentage of employees hired in each year is calculated by dividing the count by the total number of employees and multiplying by 100. The output includes the hire year and the percentage of employees hired in that year.","title":"Find the number of employees hired in each year, and show the results as a percentage of the total number of employees:"},{"location":"sql/14_functions/#find-the-number-of-employees-born-in-each-month-and-sort-the-results-by-month","text":"SELECT MONTH ( birth_date ) as birth_month , COUNT ( * ) as num_employees FROM employees GROUP BY birth_month ORDER BY birth_month ; This query uses the MONTH() function to extract the month from the birth date of each employee, and the COUNT() function to count the number of employees born in each month. The GROUP BY clause is used to group the results by birth month, and the ORDER BY clause is used to sort the results by month.","title":"Find the number of employees born in each month, and sort the results by month:"},{"location":"sql/14_functions/#find-the-number-of-employees-who-were-hired-in-each-year-and-show-the-results-as-a-bar-chart","text":"SELECT YEAR ( hire_date ) as hire_year , COUNT ( * ) as num_employees FROM employees GROUP BY hire_year ; This query uses the YEAR() function to extract the year from the hire date of each employee, and the COUNT() function to count the number of employees hired in each year. The GROUP BY clause is used to group the results by hire year. You can visualize the results as a bar chart in a data visualization tool, such as Tableau or Power BI, to see the distribution of hires over time.","title":"Find the number of employees who were hired in each year, and show the results as a bar chart:"},{"location":"sql/14_functions/#conclusion","text":"SQL functions are powerful tools that allow you to perform operations on data and return meaningful results. They can be used to calculate aggregate values, manipulate strings, and work with date and time values. By understanding the different types of functions available in SQL and how to use them in queries, you can perform complex data analysis and retrieve valuable insights from your data.","title":"Conclusion"},{"location":"sql/15_Wildcards/","text":"Wildcards & Unions Wildcaards Wildcards are special characters that are used in SQL to represent one or more characters in a string. They are used in conjunction with the LIKE operator to perform pattern matching on text values. Wildcards allow you to search for strings that match a specific pattern, even if you don't know the exact value of the string. Wildcards are particularly useful when searching for records that have similar but not identical values in a column. For example, you can use a wildcard to find all employees with a first name that starts with the letter \"J\", or all employees with a last name that ends in \"son\". Overview of Different Wildcard Characters and Their Uses There are three main wildcard characters in SQL: the percent sign (%), the underscore (_), and the square brackets ([]). Each wildcard character serves a different purpose and can be used in different ways. The Percent Sign (%) The percent sign is used to represent zero or more characters in a string. It can be used at the beginning, end, or in the middle of a search pattern. For example, to find all employees with a first name that starts with the letter \"J\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE 'J%' ; This query returns all employees whose first name starts with the letter \"J\". The % wildcard character is used to match any number of characters that come after the letter \"J\". Similarly, to find all employees with a last name that ends in \"son\", you can use the following SQL query: SELECT * FROM employees WHERE last_name LIKE '%son' ; This query returns all employees whose last name ends in the letters \"son\". The % wildcard character is used to match any number of characters that come before the letters \"son\". The Underscore (_) The underscore is used to represent a single character in a string. It can be used at the beginning, end, or in the middle of a search pattern. For example, to find all employees with a first name that starts with the letter \"J\" and has a second letter that is an \"o\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE 'J_o%' ; This query returns all employees whose first name starts with the letter \"J\", has a second letter that is an \"o\", and has any number of characters that come after the second letter. The Square Brackets ([]) The square brackets are used to represent a single character that can be any one of the characters specified within the brackets. For example, to find all employees with a first name that starts with the letters \"J\" or \"P\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE '[JP]%' ; This query returns all employees whose first name starts with the letters \"J\" or \"P\". The square brackets are used to specify that the first letter can be either \"J\" or \"P\". You can also use the square brackets to search for ranges of characters. For example, to find all employees with a last name that starts with the letters \"M\" to \"Z\", you can use the following SQL query: SELECT * FROM employees WHERE last_name LIKE '[M-Z]%' ; This query returns all employees whose last name starts with any letter from \"M\" to \"Z\". The square brackets and the \"-\" symbol are used to specify the range of letters. Union Union is a set operation in SQL that is used to combine the results of two or more SELECT statements into a single result set. Union returns a distinct set of rows that are present in either of the two or more SELECT statements. Union can be used to combine data from multiple tables, or to combine data from a single table that is split across multiple columns. Union can be useful when you need to combine data from multiple sources or when you want to merge two or more tables with similar structure. Union can help simplify data analysis by providing a consolidated view of data that is spread across multiple sources. Overview of How to Use Union to Combine Data The syntax for using Union in SQL is as follows: SELECT column1 , column2 , ... FROM table1 UNION SELECT column1 , column2 , ... FROM table2 ; In this syntax, the first SELECT statement selects data from table1 and the second SELECT statement selects data from table2 . The column names and data types of the columns in each SELECT statement must match. Union combines the results of the two SELECT statements and removes any duplicate rows. The columns in the result set are determined by the columns in the first SELECT statement. Here are a few examples of how to use Union to combine data: Example 1: Combine Data from Two Tables Suppose you have two tables in the \"employees\" database: \"sales\" and \"marketing\". Both tables have the same structure and contain sales data for different regions. You can use Union to combine the sales data from both tables into a single result set: SELECT region , sales_amount , year FROM sales UNION SELECT region , sales_amount , year FROM marketing ; This query combines the sales data from the \"sales\" and \"marketing\" tables and returns a result set that includes the region, sales amount, and year for each sale. Union removes any duplicate rows from the result set. Example 2: Combine Data from Multiple Columns Suppose you have a table in the \"employees\" database that stores the names of employees in two columns: \"first_name\" and \"last_name\". You can use Union to combine the data from both columns into a single column: SELECT first_name as name FROM employees UNION SELECT last_name as name FROM employees ; This query combines the data from the \"first_name\" and \"last_name\" columns into a single column called \"name\". Union removes any duplicate names from the result set. Example 3: Use Union All to Include Duplicate Rows By default, Union removes duplicate rows from the result set. If you want to include all rows from both SELECT statements, including duplicates, you can use Union All instead: SELECT region , sales_amount , year FROM sales UNION ALL SELECT region , sales_amount , year FROM marketing ; This query combines the sales data from the \"sales\" and \"marketing\" tables and returns a result set that includes all rows, including duplicates.","title":"Wildcards & Unions"},{"location":"sql/15_Wildcards/#wildcards-unions","text":"","title":"Wildcards &amp; Unions"},{"location":"sql/15_Wildcards/#wildcaards","text":"Wildcards are special characters that are used in SQL to represent one or more characters in a string. They are used in conjunction with the LIKE operator to perform pattern matching on text values. Wildcards allow you to search for strings that match a specific pattern, even if you don't know the exact value of the string. Wildcards are particularly useful when searching for records that have similar but not identical values in a column. For example, you can use a wildcard to find all employees with a first name that starts with the letter \"J\", or all employees with a last name that ends in \"son\".","title":"Wildcaards"},{"location":"sql/15_Wildcards/#overview-of-different-wildcard-characters-and-their-uses","text":"There are three main wildcard characters in SQL: the percent sign (%), the underscore (_), and the square brackets ([]). Each wildcard character serves a different purpose and can be used in different ways.","title":"Overview of Different Wildcard Characters and Their Uses"},{"location":"sql/15_Wildcards/#the-percent-sign","text":"The percent sign is used to represent zero or more characters in a string. It can be used at the beginning, end, or in the middle of a search pattern. For example, to find all employees with a first name that starts with the letter \"J\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE 'J%' ; This query returns all employees whose first name starts with the letter \"J\". The % wildcard character is used to match any number of characters that come after the letter \"J\". Similarly, to find all employees with a last name that ends in \"son\", you can use the following SQL query: SELECT * FROM employees WHERE last_name LIKE '%son' ; This query returns all employees whose last name ends in the letters \"son\". The % wildcard character is used to match any number of characters that come before the letters \"son\".","title":"The Percent Sign (%)"},{"location":"sql/15_Wildcards/#the-underscore-_","text":"The underscore is used to represent a single character in a string. It can be used at the beginning, end, or in the middle of a search pattern. For example, to find all employees with a first name that starts with the letter \"J\" and has a second letter that is an \"o\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE 'J_o%' ; This query returns all employees whose first name starts with the letter \"J\", has a second letter that is an \"o\", and has any number of characters that come after the second letter.","title":"The Underscore (_)"},{"location":"sql/15_Wildcards/#the-square-brackets","text":"The square brackets are used to represent a single character that can be any one of the characters specified within the brackets. For example, to find all employees with a first name that starts with the letters \"J\" or \"P\", you can use the following SQL query: SELECT * FROM employees WHERE first_name LIKE '[JP]%' ; This query returns all employees whose first name starts with the letters \"J\" or \"P\". The square brackets are used to specify that the first letter can be either \"J\" or \"P\". You can also use the square brackets to search for ranges of characters. For example, to find all employees with a last name that starts with the letters \"M\" to \"Z\", you can use the following SQL query: SELECT * FROM employees WHERE last_name LIKE '[M-Z]%' ; This query returns all employees whose last name starts with any letter from \"M\" to \"Z\". The square brackets and the \"-\" symbol are used to specify the range of letters.","title":"The Square Brackets ([])"},{"location":"sql/15_Wildcards/#union","text":"Union is a set operation in SQL that is used to combine the results of two or more SELECT statements into a single result set. Union returns a distinct set of rows that are present in either of the two or more SELECT statements. Union can be used to combine data from multiple tables, or to combine data from a single table that is split across multiple columns. Union can be useful when you need to combine data from multiple sources or when you want to merge two or more tables with similar structure. Union can help simplify data analysis by providing a consolidated view of data that is spread across multiple sources.","title":"Union"},{"location":"sql/15_Wildcards/#overview-of-how-to-use-union-to-combine-data","text":"The syntax for using Union in SQL is as follows: SELECT column1 , column2 , ... FROM table1 UNION SELECT column1 , column2 , ... FROM table2 ; In this syntax, the first SELECT statement selects data from table1 and the second SELECT statement selects data from table2 . The column names and data types of the columns in each SELECT statement must match. Union combines the results of the two SELECT statements and removes any duplicate rows. The columns in the result set are determined by the columns in the first SELECT statement. Here are a few examples of how to use Union to combine data:","title":"Overview of How to Use Union to Combine Data"},{"location":"sql/15_Wildcards/#example-1-combine-data-from-two-tables","text":"Suppose you have two tables in the \"employees\" database: \"sales\" and \"marketing\". Both tables have the same structure and contain sales data for different regions. You can use Union to combine the sales data from both tables into a single result set: SELECT region , sales_amount , year FROM sales UNION SELECT region , sales_amount , year FROM marketing ; This query combines the sales data from the \"sales\" and \"marketing\" tables and returns a result set that includes the region, sales amount, and year for each sale. Union removes any duplicate rows from the result set.","title":"Example 1: Combine Data from Two Tables"},{"location":"sql/15_Wildcards/#example-2-combine-data-from-multiple-columns","text":"Suppose you have a table in the \"employees\" database that stores the names of employees in two columns: \"first_name\" and \"last_name\". You can use Union to combine the data from both columns into a single column: SELECT first_name as name FROM employees UNION SELECT last_name as name FROM employees ; This query combines the data from the \"first_name\" and \"last_name\" columns into a single column called \"name\". Union removes any duplicate names from the result set.","title":"Example 2: Combine Data from Multiple Columns"},{"location":"sql/15_Wildcards/#example-3-use-union-all-to-include-duplicate-rows","text":"By default, Union removes duplicate rows from the result set. If you want to include all rows from both SELECT statements, including duplicates, you can use Union All instead: SELECT region , sales_amount , year FROM sales UNION ALL SELECT region , sales_amount , year FROM marketing ; This query combines the sales data from the \"sales\" and \"marketing\" tables and returns a result set that includes all rows, including duplicates.","title":"Example 3: Use Union All to Include Duplicate Rows"},{"location":"sql/17_joins/","text":"Joins Explanation of Joins in SQL Joins in SQL are used to combine data from two or more tables in a relational database. Joins allow you to retrieve data that is spread across multiple tables by linking related data together. A join creates a new virtual table that contains data from the tables being joined. The data in the virtual table is a combination of data from the original tables that match a specific condition. The condition for joining tables is typically based on the values of a common column or set of columns in each table. Joins are an essential tool for retrieving complex data from a database. By linking related data together, joins allow you to retrieve data that is spread across multiple tables in a single query. Overview of Different Types of Joins and How to Use Them There are several types of joins in SQL, including inner join , left join , right join , and full outer join . Each type of join is used to combine data from two or more tables in a different way. Here a summary schema for each type of join : Inner Join : This join returns only the rows that have matching values in both tables. In the image, the result of an inner join between tables A and B is shown. Only the rows that have matching values in both tables are included in the result set. Left Join : This join returns all the rows from the left table and the matching rows from the right table. If there is no match in the right table, the result will contain NULL values for those columns. In the image, the result of a left join between tables A and B is shown. All the rows from table A are included in the result set, and the matching rows from table B are included. Rows in table A that have no matching rows in table B are included, with NULL values for the columns in table B. Right Join : This join returns all the rows from the right table and the matching rows from the left table. If there is no match in the left table, the result will contain NULL values for those columns. In the image, the result of a right join between tables A and B is shown. All the rows from table B are included in the result set, and the matching rows from table A are included. Rows in table B that have no matching rows in table A are included, with NULL values for the columns in table A. Full Outer Join : This join returns all the rows from both tables, with NULL values in the columns where there is no match. In the image, the result of a full outer join between tables A and B is shown. All the rows from both tables are included in the result set, with NULL values in the columns where there is no match. Left Outer Join or Left Excluding Join : This join returns all the rows from the left table that do not have a matching row in the right table. In the image, the result of a left outer join (or left excluding join) between tables A and B is shown. Only the rows from table A that do not have a matching row in table B are included in the result set. Right Outer Join or Right Excluding Join : This join returns all the rows from the right table that do not have a matching row in the left table. In the image, the result of a right outer join (or right excluding join) between tables A and B is shown. Only the rows from table B that do not have a matching row in table A are included in the result set. Some examples on MySQL Employees Database Inner Join An inner join returns only the rows that have matching values in both tables being joined. The syntax for an inner join is as follows: SELECT column1 , column2 , ... FROM table1 INNER JOIN table2 ON table1 . column = table2 . column ; In this syntax, the INNER JOIN keyword specifies that an inner join is being performed. The ON keyword specifies the condition for joining the tables. The columns being joined must have the same data type and contain similar data. Here's an example of an inner join that combines data from the \"employees\" and \"departments\" tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees INNER JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The INNER JOIN operator links the \"employees\" and \"departments\" tables on the \"dept_no\" column, and returns only the rows where there is a match between the two tables. Left Join A left join returns all the rows from the left table and the matching rows from the right table. If there are no matching rows in the right table, the result set will contain NULL values for the columns in the right table. The syntax for a left join is as follows: SELECT column1 , column2 , ... FROM table1 LEFT JOIN table2 ON table1 . column = table2 . column ; In this syntax, the LEFT JOIN keyword specifies that a left join is being performed. The ON keyword specifies the condition for joining the tables. Here's an example of a left join that combines data from the employees and departments tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The LEFT JOIN operator links the employees and departments tables on the dept_no column, and returns all the rows from the employees table, and the matching rows from the departments table. Right Join A right join returns all the rows from the right table and the matching rows from the left table. If there are no matching rows in the left table, the result set will contain NULL values for the columns in the left table. Here's an example of a right join that combines data from the employees and departments tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The RIGHT JOIN operator links the employees and departments tables on the dept_no column, and returns all the rows from the departments table, and the matching rows from the employees table. Full Outer Join A full outer join returns all the rows from both tables being joined, and NULL values for the columns that do not have matching values in the other table. The syntax for a full outer join varies depending on the database management system being used. In MySQL, a full outer join can be simulated using a combination of left join and union operators: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . dept_no = departments . dept_no UNION SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . dept_no = departments . dept_no WHERE employees . dept_no IS NULL ; This query combines the results of a left join and a right join to simulate a full outer join. The first SELECT statement performs a left join and returns all the rows from the employees table and the matching rows from the departments table. The second SELECT statement performs a right join and returns all the rows from the departments table and the matching rows from the employees table where there is no match in the employees table. The UNION operator combines the results of the two SELECT statements. This is the same version with the full outer join keyword : SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees FULL OUTER JOIN departments ON employees . dept_no = departments . dept_no WHERE employees . dept_no IS NULL OR departments . dept_no IS NULL ; In this query, the FULL OUTER JOIN returns all the rows from both tables, including those that do not have a match in the other table. The WHERE clause filters the result set to include only the rows where either the employees.dept_no or departments.dept_no is NULL , which indicates that there is no match in the other table. More examples of left join , inner join and right join Inner Join Example 1 Suppose you want to retrieve data that shows the salary of each employee along with the department name for the department they work in. You can use an inner join to link the employees and dept_emp tables on the emp_no column and the departments and dept_emp tables on the dept_no column: SELECT e . emp_no , e . first_name , e . last_name , d . dept_name , s . salary FROM employees e INNER JOIN dept_emp de ON e . emp_no = de . emp_no INNER JOIN departments d ON de . dept_no = d . dept_no INNER JOIN salaries s ON e . emp_no = s . emp_no ; This query returns a result set that includes the employee number, first name, last name, department name, and salary for each employee. The INNER JOIN operator links the employees and dept_emp tables on the emp_no column, and links the departments and dept_emp tables on the dept_no column, and links the salaries table on the emp_no column. Inner Join Example 2 Suppose you want to retrieve data that shows the department name and manager's name for each department in the company. You can use an inner join to link the departments and dept_manager tables on the dept_no column, and link the employees table on the emp_no column to get the name of the manager: SELECT d . dept_name , e . first_name , e . last_name FROM departments d INNER JOIN dept_manager dm ON d . dept_no = dm . dept_no INNER JOIN employees e ON dm . emp_no = e . emp_no ; This query returns a result set that includes the department name and the first and last name of the manager for each department. The INNER JOIN operator links the \"departments\" and \"dept_manager\" tables on the \"dept_no\" column, and links the \"employees\" table on the \"emp_no\" column to get the name of the manager. Left Join Example 1 Suppose you want to retrieve data that shows the name and department of each employee, even if they are not currently assigned to a department. You can use a left join to link the \"employees\" and \"dept_emp\" tables on the \"emp_no\" column, and link the \"departments\" table on the \"dept_no\" column: SELECT e . first_name , e . last_name , d . dept_name FROM employees e LEFT JOIN dept_emp de ON e . emp_no = de . emp_no LEFT JOIN departments d ON de . dept_no = d . dept_no ; This query returns a result set that includes the first name, last name, and department name for each employee. The LEFT JOIN operator links the \"employees\" and \"dept_emp\" tables on the \"emp_no\" column, and links the \"departments\" table on the \"dept_no\" column. Even if an employee is not currently assigned to a department, their name will still appear in the result set with a NULL value for the \"dept_name\" column. Left Join Example 2 Suppose you want to retrieve data that shows the total number of sales made by each employee, even if they have not made any sales. You can use a left join to link the \"employees\" and \"sales\" tables on the \"emp_no\" column: SELECT e . emp_no , e . first_name , e . last_name , COUNT ( s . sales_amount ) as total_sales FROM employees e LEFT JOIN sales s ON e . emp_no = s . emp_no GROUP BY e . emp_no ; Right Join Example 1 Retrieving data that shows the name and department of each employee, even if the department has no employees assigned to it. SELECT e . first_name , e . last_name , d . dept_name FROM dept_emp de RIGHT JOIN employees e ON de . emp_no = e . emp_no RIGHT JOIN departments d ON de . dept_no = d . dept_no ; Right Join Example 2 Retrieving data that shows the total number of sales made by each employee, even if they have not made any sales. SELECT e . emp_no , e . first_name , e . last_name , COUNT ( s . sales_amount ) as total_sales FROM sales s RIGHT JOIN employees e ON s . emp_no = e . emp_no GROUP BY e . emp_no ;","title":"Joins"},{"location":"sql/17_joins/#joins","text":"","title":"Joins"},{"location":"sql/17_joins/#explanation-of-joins-in-sql","text":"Joins in SQL are used to combine data from two or more tables in a relational database. Joins allow you to retrieve data that is spread across multiple tables by linking related data together. A join creates a new virtual table that contains data from the tables being joined. The data in the virtual table is a combination of data from the original tables that match a specific condition. The condition for joining tables is typically based on the values of a common column or set of columns in each table. Joins are an essential tool for retrieving complex data from a database. By linking related data together, joins allow you to retrieve data that is spread across multiple tables in a single query.","title":"Explanation of Joins in SQL"},{"location":"sql/17_joins/#overview-of-different-types-of-joins-and-how-to-use-them","text":"There are several types of joins in SQL, including inner join , left join , right join , and full outer join . Each type of join is used to combine data from two or more tables in a different way. Here a summary schema for each type of join : Inner Join : This join returns only the rows that have matching values in both tables. In the image, the result of an inner join between tables A and B is shown. Only the rows that have matching values in both tables are included in the result set. Left Join : This join returns all the rows from the left table and the matching rows from the right table. If there is no match in the right table, the result will contain NULL values for those columns. In the image, the result of a left join between tables A and B is shown. All the rows from table A are included in the result set, and the matching rows from table B are included. Rows in table A that have no matching rows in table B are included, with NULL values for the columns in table B. Right Join : This join returns all the rows from the right table and the matching rows from the left table. If there is no match in the left table, the result will contain NULL values for those columns. In the image, the result of a right join between tables A and B is shown. All the rows from table B are included in the result set, and the matching rows from table A are included. Rows in table B that have no matching rows in table A are included, with NULL values for the columns in table A. Full Outer Join : This join returns all the rows from both tables, with NULL values in the columns where there is no match. In the image, the result of a full outer join between tables A and B is shown. All the rows from both tables are included in the result set, with NULL values in the columns where there is no match. Left Outer Join or Left Excluding Join : This join returns all the rows from the left table that do not have a matching row in the right table. In the image, the result of a left outer join (or left excluding join) between tables A and B is shown. Only the rows from table A that do not have a matching row in table B are included in the result set. Right Outer Join or Right Excluding Join : This join returns all the rows from the right table that do not have a matching row in the left table. In the image, the result of a right outer join (or right excluding join) between tables A and B is shown. Only the rows from table B that do not have a matching row in table A are included in the result set.","title":"Overview of Different Types of Joins and How to Use Them"},{"location":"sql/17_joins/#some-examples-on-mysql-employees-database","text":"","title":"Some examples on MySQL Employees Database"},{"location":"sql/17_joins/#inner-join","text":"An inner join returns only the rows that have matching values in both tables being joined. The syntax for an inner join is as follows: SELECT column1 , column2 , ... FROM table1 INNER JOIN table2 ON table1 . column = table2 . column ; In this syntax, the INNER JOIN keyword specifies that an inner join is being performed. The ON keyword specifies the condition for joining the tables. The columns being joined must have the same data type and contain similar data. Here's an example of an inner join that combines data from the \"employees\" and \"departments\" tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees INNER JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The INNER JOIN operator links the \"employees\" and \"departments\" tables on the \"dept_no\" column, and returns only the rows where there is a match between the two tables.","title":"Inner Join"},{"location":"sql/17_joins/#left-join","text":"A left join returns all the rows from the left table and the matching rows from the right table. If there are no matching rows in the right table, the result set will contain NULL values for the columns in the right table. The syntax for a left join is as follows: SELECT column1 , column2 , ... FROM table1 LEFT JOIN table2 ON table1 . column = table2 . column ; In this syntax, the LEFT JOIN keyword specifies that a left join is being performed. The ON keyword specifies the condition for joining the tables. Here's an example of a left join that combines data from the employees and departments tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The LEFT JOIN operator links the employees and departments tables on the dept_no column, and returns all the rows from the employees table, and the matching rows from the departments table.","title":"Left Join"},{"location":"sql/17_joins/#right-join","text":"A right join returns all the rows from the right table and the matching rows from the left table. If there are no matching rows in the left table, the result set will contain NULL values for the columns in the left table. Here's an example of a right join that combines data from the employees and departments tables: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . dept_no = departments . dept_no ; This query returns a result set that includes the employee number, first name, last name, and department name for each employee. The RIGHT JOIN operator links the employees and departments tables on the dept_no column, and returns all the rows from the departments table, and the matching rows from the employees table.","title":"Right Join"},{"location":"sql/17_joins/#full-outer-join","text":"A full outer join returns all the rows from both tables being joined, and NULL values for the columns that do not have matching values in the other table. The syntax for a full outer join varies depending on the database management system being used. In MySQL, a full outer join can be simulated using a combination of left join and union operators: SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . dept_no = departments . dept_no UNION SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . dept_no = departments . dept_no WHERE employees . dept_no IS NULL ; This query combines the results of a left join and a right join to simulate a full outer join. The first SELECT statement performs a left join and returns all the rows from the employees table and the matching rows from the departments table. The second SELECT statement performs a right join and returns all the rows from the departments table and the matching rows from the employees table where there is no match in the employees table. The UNION operator combines the results of the two SELECT statements. This is the same version with the full outer join keyword : SELECT employees . emp_no , employees . first_name , employees . last_name , departments . dept_name FROM employees FULL OUTER JOIN departments ON employees . dept_no = departments . dept_no WHERE employees . dept_no IS NULL OR departments . dept_no IS NULL ; In this query, the FULL OUTER JOIN returns all the rows from both tables, including those that do not have a match in the other table. The WHERE clause filters the result set to include only the rows where either the employees.dept_no or departments.dept_no is NULL , which indicates that there is no match in the other table.","title":"Full Outer Join"},{"location":"sql/17_joins/#more-examples-of-left-join-inner-join-and-right-join","text":"","title":"More examples of left join, inner join and right join"},{"location":"sql/17_joins/#inner-join-example-1","text":"Suppose you want to retrieve data that shows the salary of each employee along with the department name for the department they work in. You can use an inner join to link the employees and dept_emp tables on the emp_no column and the departments and dept_emp tables on the dept_no column: SELECT e . emp_no , e . first_name , e . last_name , d . dept_name , s . salary FROM employees e INNER JOIN dept_emp de ON e . emp_no = de . emp_no INNER JOIN departments d ON de . dept_no = d . dept_no INNER JOIN salaries s ON e . emp_no = s . emp_no ; This query returns a result set that includes the employee number, first name, last name, department name, and salary for each employee. The INNER JOIN operator links the employees and dept_emp tables on the emp_no column, and links the departments and dept_emp tables on the dept_no column, and links the salaries table on the emp_no column.","title":"Inner Join Example 1"},{"location":"sql/17_joins/#inner-join-example-2","text":"Suppose you want to retrieve data that shows the department name and manager's name for each department in the company. You can use an inner join to link the departments and dept_manager tables on the dept_no column, and link the employees table on the emp_no column to get the name of the manager: SELECT d . dept_name , e . first_name , e . last_name FROM departments d INNER JOIN dept_manager dm ON d . dept_no = dm . dept_no INNER JOIN employees e ON dm . emp_no = e . emp_no ; This query returns a result set that includes the department name and the first and last name of the manager for each department. The INNER JOIN operator links the \"departments\" and \"dept_manager\" tables on the \"dept_no\" column, and links the \"employees\" table on the \"emp_no\" column to get the name of the manager.","title":"Inner Join Example 2"},{"location":"sql/17_joins/#left-join-example-1","text":"Suppose you want to retrieve data that shows the name and department of each employee, even if they are not currently assigned to a department. You can use a left join to link the \"employees\" and \"dept_emp\" tables on the \"emp_no\" column, and link the \"departments\" table on the \"dept_no\" column: SELECT e . first_name , e . last_name , d . dept_name FROM employees e LEFT JOIN dept_emp de ON e . emp_no = de . emp_no LEFT JOIN departments d ON de . dept_no = d . dept_no ; This query returns a result set that includes the first name, last name, and department name for each employee. The LEFT JOIN operator links the \"employees\" and \"dept_emp\" tables on the \"emp_no\" column, and links the \"departments\" table on the \"dept_no\" column. Even if an employee is not currently assigned to a department, their name will still appear in the result set with a NULL value for the \"dept_name\" column.","title":"Left Join Example 1"},{"location":"sql/17_joins/#left-join-example-2","text":"Suppose you want to retrieve data that shows the total number of sales made by each employee, even if they have not made any sales. You can use a left join to link the \"employees\" and \"sales\" tables on the \"emp_no\" column: SELECT e . emp_no , e . first_name , e . last_name , COUNT ( s . sales_amount ) as total_sales FROM employees e LEFT JOIN sales s ON e . emp_no = s . emp_no GROUP BY e . emp_no ;","title":"Left Join Example 2"},{"location":"sql/17_joins/#right-join-example-1","text":"Retrieving data that shows the name and department of each employee, even if the department has no employees assigned to it. SELECT e . first_name , e . last_name , d . dept_name FROM dept_emp de RIGHT JOIN employees e ON de . emp_no = e . emp_no RIGHT JOIN departments d ON de . dept_no = d . dept_no ;","title":"Right Join Example 1"},{"location":"sql/17_joins/#right-join-example-2","text":"Retrieving data that shows the total number of sales made by each employee, even if they have not made any sales. SELECT e . emp_no , e . first_name , e . last_name , COUNT ( s . sales_amount ) as total_sales FROM sales s RIGHT JOIN employees e ON s . emp_no = e . emp_no GROUP BY e . emp_no ;","title":"Right Join Example 2"},{"location":"sql/18_Nested_Queries/","text":"Nested Queries Introduction to Nested Queries A nested query, also known as a subquery, is a query that is nested inside another query. A subquery can be used to retrieve data that will be used in the main query, allowing for complex queries that would be difficult to write using a single query. Subqueries can be used with various clauses in SQL, such as SELECT , WHERE , and HAVING . Overview of How to Use Subqueries A subquery is typically enclosed in parentheses and used in conjunction with an operator such as IN , EXISTS , or =. The subquery can be used in various parts of a query, depending on the desired result. Here's an example of a subquery used in a SELECT statement: SELECT first_name , last_name , birth_date FROM employees WHERE birth_date > ( SELECT birth_date FROM employees WHERE emp_no = 10001 ); This query returns a result set that includes the first name, last name, and birth date for each employee whose birth date is later than that of the employee with emp_no = 10001 . The subquery is used in the WHERE clause to retrieve the birth date of the employee with emp_no = 10001 . Here's an example of a subquery used in a HAVING clause: SELECT departments . dept_name , AVG ( salaries . salary ) AS avg_salary FROM employees INNER JOIN dept_emp ON employees . emp_no = dept_emp . emp_no INNER JOIN departments ON dept_emp . dept_no = departments . dept_no INNER JOIN salaries ON employees . emp_no = salaries . emp_no GROUP BY departments . dept_name HAVING AVG ( salaries . salary ) > ( SELECT AVG ( salary ) FROM salaries ); This query joins the employees, salaries, dept_emp, and departments tables together based on the employee number, department number, and salary information. It then groups the results by department name using the GROUP BY clause and calculates the average salary for each department using the AVG function. The HAVING clause is used to filter the results based on the condition that the average salary for a department is greater than the overall average salary of all employees, which is calculated using a subquery that selects the average salary from the salaries table. Some examples with MySQL Employees database Example 1: Retrieving data for employees who are currently managers SELECT first_name , last_name , hire_date FROM employees WHERE emp_no IN ( SELECT emp_no FROM dept_manager ); This query returns a result set that includes the first name, last name, and hire date for each employee who is currently a manager. The subquery is used in the WHERE clause to retrieve the employee numbers of all employees who are currently department managers. Example 2: Retrieving data for employees who were hired in the same year as a specific employee SELECT first_name , last_name , hire_date FROM employees WHERE YEAR ( hire_date ) = ( SELECT YEAR ( hire_date ) FROM employees WHERE emp_no = 10001 ); This query returns a result set that includes the first name, last name, and hire date for each employee who was hired in the same year as the employee with emp_no = 10001. The subquery is used in the WHERE clause to retrieve the year in which the employee with emp_no = 10001 was hired. Example 3: Find the 10th employee with the highest salary, along with their job title SELECT employees . emp_no , employees . first_name , employees . last_name , MAX ( salaries . salary ) AS max_salary , MAX ( titles . title ) AS title FROM employees JOIN salaries ON employees . emp_no = salaries . emp_no JOIN titles ON employees . emp_no = titles . emp_no GROUP BY employees . emp_no ORDER BY max_salary DESC LIMIT 10 ; The query starts by selecting specific columns from the employees table, including the employee number, first name, and last name. It then joins the salaries and titles tables to the employees table based on the employee number, using the JOIN clause. The MAX function is used to find the maximum salary and job title for each employee. The GROUP BY clause is used to group the results by employee number. This ensures that the maximum salary and job title returned for each employee correspond to the same person. The ORDER BY clause is used to sort the results in descending order based on the maximum salary, so that the employee with the highest salary is at the top. Finally, the LIMIT clause is used to limit the results to only the top row, which corresponds to the employee with the highest salary.","title":"Nested Queries"},{"location":"sql/18_Nested_Queries/#nested-queries","text":"","title":"Nested Queries"},{"location":"sql/18_Nested_Queries/#introduction-to-nested-queries","text":"A nested query, also known as a subquery, is a query that is nested inside another query. A subquery can be used to retrieve data that will be used in the main query, allowing for complex queries that would be difficult to write using a single query. Subqueries can be used with various clauses in SQL, such as SELECT , WHERE , and HAVING .","title":"Introduction to Nested Queries"},{"location":"sql/18_Nested_Queries/#overview-of-how-to-use-subqueries","text":"A subquery is typically enclosed in parentheses and used in conjunction with an operator such as IN , EXISTS , or =. The subquery can be used in various parts of a query, depending on the desired result. Here's an example of a subquery used in a SELECT statement: SELECT first_name , last_name , birth_date FROM employees WHERE birth_date > ( SELECT birth_date FROM employees WHERE emp_no = 10001 ); This query returns a result set that includes the first name, last name, and birth date for each employee whose birth date is later than that of the employee with emp_no = 10001 . The subquery is used in the WHERE clause to retrieve the birth date of the employee with emp_no = 10001 . Here's an example of a subquery used in a HAVING clause: SELECT departments . dept_name , AVG ( salaries . salary ) AS avg_salary FROM employees INNER JOIN dept_emp ON employees . emp_no = dept_emp . emp_no INNER JOIN departments ON dept_emp . dept_no = departments . dept_no INNER JOIN salaries ON employees . emp_no = salaries . emp_no GROUP BY departments . dept_name HAVING AVG ( salaries . salary ) > ( SELECT AVG ( salary ) FROM salaries ); This query joins the employees, salaries, dept_emp, and departments tables together based on the employee number, department number, and salary information. It then groups the results by department name using the GROUP BY clause and calculates the average salary for each department using the AVG function. The HAVING clause is used to filter the results based on the condition that the average salary for a department is greater than the overall average salary of all employees, which is calculated using a subquery that selects the average salary from the salaries table.","title":"Overview of How to Use Subqueries"},{"location":"sql/18_Nested_Queries/#some-examples-with-mysql-employees-database","text":"","title":"Some examples with MySQL Employees database"},{"location":"sql/18_Nested_Queries/#example-1-retrieving-data-for-employees-who-are-currently-managers","text":"SELECT first_name , last_name , hire_date FROM employees WHERE emp_no IN ( SELECT emp_no FROM dept_manager ); This query returns a result set that includes the first name, last name, and hire date for each employee who is currently a manager. The subquery is used in the WHERE clause to retrieve the employee numbers of all employees who are currently department managers.","title":"Example 1: Retrieving data for employees who are currently managers"},{"location":"sql/18_Nested_Queries/#example-2-retrieving-data-for-employees-who-were-hired-in-the-same-year-as-a-specific-employee","text":"SELECT first_name , last_name , hire_date FROM employees WHERE YEAR ( hire_date ) = ( SELECT YEAR ( hire_date ) FROM employees WHERE emp_no = 10001 ); This query returns a result set that includes the first name, last name, and hire date for each employee who was hired in the same year as the employee with emp_no = 10001. The subquery is used in the WHERE clause to retrieve the year in which the employee with emp_no = 10001 was hired.","title":"Example 2: Retrieving data for employees who were hired in the same year as a specific employee"},{"location":"sql/18_Nested_Queries/#example-3-find-the-10th-employee-with-the-highest-salary-along-with-their-job-title","text":"SELECT employees . emp_no , employees . first_name , employees . last_name , MAX ( salaries . salary ) AS max_salary , MAX ( titles . title ) AS title FROM employees JOIN salaries ON employees . emp_no = salaries . emp_no JOIN titles ON employees . emp_no = titles . emp_no GROUP BY employees . emp_no ORDER BY max_salary DESC LIMIT 10 ; The query starts by selecting specific columns from the employees table, including the employee number, first name, and last name. It then joins the salaries and titles tables to the employees table based on the employee number, using the JOIN clause. The MAX function is used to find the maximum salary and job title for each employee. The GROUP BY clause is used to group the results by employee number. This ensures that the maximum salary and job title returned for each employee correspond to the same person. The ORDER BY clause is used to sort the results in descending order based on the maximum salary, so that the employee with the highest salary is at the top. Finally, the LIMIT clause is used to limit the results to only the top row, which corresponds to the employee with the highest salary.","title":"Example 3: Find the 10th employee with the highest salary, along with their job title"},{"location":"sql/19_delete/","text":"On delete Introduction to ON DELETE ON DELETE is a referential action that can be applied to foreign key constraints in SQL. It specifies what should happen to data in the child table when a row is deleted from the parent table. The different options available for ON DELETE include CASCADE , SET NULL , RESTRICT , and NO ACTION . Using ON DELETE is important for maintaining data integrity in a database. Overview of How to Use ON DELETE Here's an example of creating a foreign key with ON DELETE CASCADE in SQL: ALTER TABLE employees ADD CONSTRAINT emp_dept_fk FOREIGN KEY ( dept_no ) REFERENCES departments ( dept_no ) ON DELETE CASCADE ; In this example, a foreign key constraint named \"emp_dept_fk\" is added to the \"employees\" table. The foreign key references the \"dept_no\" column in the \"departments\" table, and specifies \"ON DELETE CASCADE.\" This means that when a row is deleted from the \"departments\" table, any corresponding rows in the \"employees\" table will also be deleted. Here's an example of creating a foreign key with \"ON DELETE SET NULL\" in SQL: ALTER TABLE employees ADD CONSTRAINT emp_manager_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE SET NULL ; In this example, a foreign key constraint named \"emp_manager_fk\" is added to the \"employees\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE SET NULL.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in other tables will have the foreign key column value set to NULL. Here's an example of creating a foreign key with \"ON DELETE RESTRICT\" in SQL: ALTER TABLE departments ADD CONSTRAINT dept_manager_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE RESTRICT ; In this example, a foreign key constraint named \"dept_manager_fk\" is added to the \"departments\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE RESTRICT.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in the \"departments\" table cannot be deleted. Here's an example of creating a foreign key with \"ON DELETE NO ACTION\" in SQL: ALTER TABLE dept_emp ADD CONSTRAINT dept_emp_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE NO ACTION ; In this example, a foreign key constraint named \"dept_emp_fk\" is added to the \"dept_emp\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE NO ACTION.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in the \"dept_emp\" table will cause an error and the delete operation will be rejected. Conclusion Using \"ON DELETE\" in SQL is important for maintaining data integrity in a database. The different options available for \"ON DELETE\" include \"CASCADE,\" \"SET NULL,\" \"RESTRICT,\" and \"NO ACTION.\" By understanding how to use \"ON DELETE\" in SQL, you can ensure that your database remains consistent and accurate over time.","title":"On Delete"},{"location":"sql/19_delete/#on-delete","text":"","title":"On delete"},{"location":"sql/19_delete/#introduction-to-on-delete","text":"ON DELETE is a referential action that can be applied to foreign key constraints in SQL. It specifies what should happen to data in the child table when a row is deleted from the parent table. The different options available for ON DELETE include CASCADE , SET NULL , RESTRICT , and NO ACTION . Using ON DELETE is important for maintaining data integrity in a database.","title":"Introduction to ON DELETE"},{"location":"sql/19_delete/#overview-of-how-to-use-on-delete","text":"Here's an example of creating a foreign key with ON DELETE CASCADE in SQL: ALTER TABLE employees ADD CONSTRAINT emp_dept_fk FOREIGN KEY ( dept_no ) REFERENCES departments ( dept_no ) ON DELETE CASCADE ; In this example, a foreign key constraint named \"emp_dept_fk\" is added to the \"employees\" table. The foreign key references the \"dept_no\" column in the \"departments\" table, and specifies \"ON DELETE CASCADE.\" This means that when a row is deleted from the \"departments\" table, any corresponding rows in the \"employees\" table will also be deleted. Here's an example of creating a foreign key with \"ON DELETE SET NULL\" in SQL: ALTER TABLE employees ADD CONSTRAINT emp_manager_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE SET NULL ; In this example, a foreign key constraint named \"emp_manager_fk\" is added to the \"employees\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE SET NULL.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in other tables will have the foreign key column value set to NULL. Here's an example of creating a foreign key with \"ON DELETE RESTRICT\" in SQL: ALTER TABLE departments ADD CONSTRAINT dept_manager_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE RESTRICT ; In this example, a foreign key constraint named \"dept_manager_fk\" is added to the \"departments\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE RESTRICT.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in the \"departments\" table cannot be deleted. Here's an example of creating a foreign key with \"ON DELETE NO ACTION\" in SQL: ALTER TABLE dept_emp ADD CONSTRAINT dept_emp_fk FOREIGN KEY ( emp_no ) REFERENCES employees ( emp_no ) ON DELETE NO ACTION ; In this example, a foreign key constraint named \"dept_emp_fk\" is added to the \"dept_emp\" table. The foreign key references the \"emp_no\" column in the \"employees\" table, and specifies \"ON DELETE NO ACTION.\" This means that when a row is deleted from the \"employees\" table, any corresponding rows in the \"dept_emp\" table will cause an error and the delete operation will be rejected.","title":"Overview of How to Use ON DELETE"},{"location":"sql/19_delete/#conclusion","text":"Using \"ON DELETE\" in SQL is important for maintaining data integrity in a database. The different options available for \"ON DELETE\" include \"CASCADE,\" \"SET NULL,\" \"RESTRICT,\" and \"NO ACTION.\" By understanding how to use \"ON DELETE\" in SQL, you can ensure that your database remains consistent and accurate over time.","title":"Conclusion"},{"location":"sql/20_triggers/","text":"Triggers Introduction to Triggers in SQL In SQL, a trigger is a special kind of stored program that is automatically executed in response to specific events or actions that occur within a database. Triggers can be used to automate tasks, maintain data integrity, and enforce business rules. They are often used in conjunction with other SQL features, such as constraints and indexes, to ensure that a database remains consistent and accurate over time. Overview of How to Use Triggers to Automate Tasks Here's an example of creating a trigger in SQL that automatically sets the \"to_date\" field of a row in the \"dept_emp\" table to the current date whenever a new row is inserted: CREATE TRIGGER update_dept_emp_to_date BEFORE INSERT ON dept_emp FOR EACH ROW SET NEW . to_date = NOW (); In this example, a trigger named \"update_dept_emp_to_date\" is created using the CREATE TRIGGER statement. The trigger is defined to execute \"BEFORE INSERT\" on the \"dept_emp\" table, and is set to execute \"FOR EACH ROW\" that is inserted. The body of the trigger consists of a single statement that sets the value of the \"to_date\" field for the newly inserted row to the current date and time, using the NOW() function. Here's an example of creating a trigger in SQL that prevents the deletion of a row in the \"employees\" table if that row is currently assigned to a department: CREATE TRIGGER prevent_emp_delete BEFORE DELETE ON employees FOR EACH ROW BEGIN DECLARE dept_count INT ; SELECT COUNT ( * ) INTO dept_count FROM dept_emp WHERE emp_no = OLD . emp_no ; IF dept_count > 0 THEN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Cannot delete employee who is currently assigned to a department.' ; END IF ; END ; In this example, a trigger named \"prevent_emp_delete\" is created using the CREATE TRIGGER statement. The trigger is defined to execute \"BEFORE DELETE\" on the \"employees\" table, and is set to execute \"FOR EACH ROW\" that is deleted. The body of the trigger consists of a block of code that checks whether the employee being deleted is currently assigned to a department, and if so, raises an error using the SIGNAL statement to prevent the delete operation. Examples of using triggers on the employees database Example 1: Audit trail for changes to the employees table CREATE TABLE employees_audit ( action VARCHAR ( 50 ) NOT NULL , emp_no INT NOT NULL , last_name VARCHAR ( 50 ) NOT NULL , first_name VARCHAR ( 50 ) NOT NULL , timestamp DATETIME DEFAULT NOW () ); DELIMITER // CREATE TRIGGER employees_audit_insert AFTER INSERT ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'insert' , NEW . emp_no , NEW . last_name , NEW . first_name ); END // CREATE TRIGGER employees_audit_update AFTER UPDATE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'update' , NEW . emp_no , NEW . last_name , NEW . first_name ); END // CREATE TRIGGER employees_audit_delete AFTER DELETE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'delete' , OLD . emp_no , OLD . last_name , OLD . first_name ); END // DELIMITER ; In this example, a new table \"employees_audit\" is created to store an audit trail of changes to the \"employees\" table. Three triggers are then defined using the CREATE TRIGGER statement to execute after each insert, update, and delete operation on the \"employees\" table. These triggers use the INSERT statement to add a new row to the \"employees_audit\" table that records the action, employee number, last name, first name, and timestamp of the change. Example 2: Preventing changes to the departments table CREATE TRIGGER prevent_dept_change BEFORE UPDATE ON departments FOR EACH ROW BEGIN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Changes to departments table are not allowed.' ; END ; In this example, a trigger named \"prevent_dept_change\" is defined using the CREATE TRIGGER statement to execute before any update operation on the \"departments\" table. The body of the trigger consists of a single statement that raises an error using the SIGNAL statement to prevent the update operation. Example 3: Automatic promotion to manager for department heads CREATE TRIGGER promote_to_manager AFTER UPDATE ON dept_manager FOR EACH ROW BEGIN IF NEW . to_date = '9999-01-01' THEN UPDATE employees SET emp_title = 'Manager' WHERE emp_no = NEW . emp_no ; END IF ; END ; In this example, a trigger named \"promote_to_manager\" is defined using the CREATE TRIGGER statement to execute after any update operation on the \"dept_manager\" table. The body of the trigger consists of an IF statement that checks whether the \"to_date\" field for the updated row is set to the special value of '9999-01-01', which indicates that the employee is currently a department head. If so, the trigger uses the UPDATE statement to change the employee's title to \"Manager\" in the \"employees\" table. Why triggers are important for data analysis job Triggers are important in data analysis because they allow you to automate tasks and enforce data integrity rules within a database. By using triggers, you can ensure that data is consistent and accurate over time, and that business rules are enforced consistently across different operations and users. For example, if you are analyzing sales data in a database, you might use triggers to automatically update certain fields or tables whenever new sales data is added or existing data is updated. You could also use triggers to prevent certain types of changes to the database, or to notify you when certain events occur. By automating tasks and enforcing rules using triggers, you can save time and reduce the risk of errors or inconsistencies in your data. This can help you to make more informed decisions and gain deeper insights into the patterns and trends in your data. Overall, triggers are an important tool for data analysts and other database professionals who need to ensure the accuracy and integrity of their data. By mastering the use of triggers in SQL, you can become more effective at managing and analyzing data in a wide range of contexts. Conclusion Using triggers in SQL is a powerful way to automate tasks, maintain data integrity, and enforce business rules. Triggers can be used to respond to specific events or actions that occur within a database, and can be defined to execute before or after specific operations such as inserts, updates, and deletes. By understanding how to use triggers in SQL, you can ensure that your database remains consistent and accurate over time.","title":"Triggers"},{"location":"sql/20_triggers/#triggers","text":"","title":"Triggers"},{"location":"sql/20_triggers/#introduction-to-triggers-in-sql","text":"In SQL, a trigger is a special kind of stored program that is automatically executed in response to specific events or actions that occur within a database. Triggers can be used to automate tasks, maintain data integrity, and enforce business rules. They are often used in conjunction with other SQL features, such as constraints and indexes, to ensure that a database remains consistent and accurate over time.","title":"Introduction to Triggers in SQL"},{"location":"sql/20_triggers/#overview-of-how-to-use-triggers-to-automate-tasks","text":"Here's an example of creating a trigger in SQL that automatically sets the \"to_date\" field of a row in the \"dept_emp\" table to the current date whenever a new row is inserted: CREATE TRIGGER update_dept_emp_to_date BEFORE INSERT ON dept_emp FOR EACH ROW SET NEW . to_date = NOW (); In this example, a trigger named \"update_dept_emp_to_date\" is created using the CREATE TRIGGER statement. The trigger is defined to execute \"BEFORE INSERT\" on the \"dept_emp\" table, and is set to execute \"FOR EACH ROW\" that is inserted. The body of the trigger consists of a single statement that sets the value of the \"to_date\" field for the newly inserted row to the current date and time, using the NOW() function. Here's an example of creating a trigger in SQL that prevents the deletion of a row in the \"employees\" table if that row is currently assigned to a department: CREATE TRIGGER prevent_emp_delete BEFORE DELETE ON employees FOR EACH ROW BEGIN DECLARE dept_count INT ; SELECT COUNT ( * ) INTO dept_count FROM dept_emp WHERE emp_no = OLD . emp_no ; IF dept_count > 0 THEN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Cannot delete employee who is currently assigned to a department.' ; END IF ; END ; In this example, a trigger named \"prevent_emp_delete\" is created using the CREATE TRIGGER statement. The trigger is defined to execute \"BEFORE DELETE\" on the \"employees\" table, and is set to execute \"FOR EACH ROW\" that is deleted. The body of the trigger consists of a block of code that checks whether the employee being deleted is currently assigned to a department, and if so, raises an error using the SIGNAL statement to prevent the delete operation.","title":"Overview of How to Use Triggers to Automate Tasks"},{"location":"sql/20_triggers/#examples-of-using-triggers-on-the-employees-database","text":"","title":"Examples of using triggers on the employees database"},{"location":"sql/20_triggers/#example-1-audit-trail-for-changes-to-the-employees-table","text":"CREATE TABLE employees_audit ( action VARCHAR ( 50 ) NOT NULL , emp_no INT NOT NULL , last_name VARCHAR ( 50 ) NOT NULL , first_name VARCHAR ( 50 ) NOT NULL , timestamp DATETIME DEFAULT NOW () ); DELIMITER // CREATE TRIGGER employees_audit_insert AFTER INSERT ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'insert' , NEW . emp_no , NEW . last_name , NEW . first_name ); END // CREATE TRIGGER employees_audit_update AFTER UPDATE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'update' , NEW . emp_no , NEW . last_name , NEW . first_name ); END // CREATE TRIGGER employees_audit_delete AFTER DELETE ON employees FOR EACH ROW BEGIN INSERT INTO employees_audit ( action , emp_no , last_name , first_name ) VALUES ( 'delete' , OLD . emp_no , OLD . last_name , OLD . first_name ); END // DELIMITER ; In this example, a new table \"employees_audit\" is created to store an audit trail of changes to the \"employees\" table. Three triggers are then defined using the CREATE TRIGGER statement to execute after each insert, update, and delete operation on the \"employees\" table. These triggers use the INSERT statement to add a new row to the \"employees_audit\" table that records the action, employee number, last name, first name, and timestamp of the change.","title":"Example 1: Audit trail for changes to the employees table"},{"location":"sql/20_triggers/#example-2-preventing-changes-to-the-departments-table","text":"CREATE TRIGGER prevent_dept_change BEFORE UPDATE ON departments FOR EACH ROW BEGIN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Changes to departments table are not allowed.' ; END ; In this example, a trigger named \"prevent_dept_change\" is defined using the CREATE TRIGGER statement to execute before any update operation on the \"departments\" table. The body of the trigger consists of a single statement that raises an error using the SIGNAL statement to prevent the update operation.","title":"Example 2: Preventing changes to the departments table"},{"location":"sql/20_triggers/#example-3-automatic-promotion-to-manager-for-department-heads","text":"CREATE TRIGGER promote_to_manager AFTER UPDATE ON dept_manager FOR EACH ROW BEGIN IF NEW . to_date = '9999-01-01' THEN UPDATE employees SET emp_title = 'Manager' WHERE emp_no = NEW . emp_no ; END IF ; END ; In this example, a trigger named \"promote_to_manager\" is defined using the CREATE TRIGGER statement to execute after any update operation on the \"dept_manager\" table. The body of the trigger consists of an IF statement that checks whether the \"to_date\" field for the updated row is set to the special value of '9999-01-01', which indicates that the employee is currently a department head. If so, the trigger uses the UPDATE statement to change the employee's title to \"Manager\" in the \"employees\" table.","title":"Example 3: Automatic promotion to manager for department heads"},{"location":"sql/20_triggers/#why-triggers-are-important-for-data-analysis-job","text":"Triggers are important in data analysis because they allow you to automate tasks and enforce data integrity rules within a database. By using triggers, you can ensure that data is consistent and accurate over time, and that business rules are enforced consistently across different operations and users. For example, if you are analyzing sales data in a database, you might use triggers to automatically update certain fields or tables whenever new sales data is added or existing data is updated. You could also use triggers to prevent certain types of changes to the database, or to notify you when certain events occur. By automating tasks and enforcing rules using triggers, you can save time and reduce the risk of errors or inconsistencies in your data. This can help you to make more informed decisions and gain deeper insights into the patterns and trends in your data. Overall, triggers are an important tool for data analysts and other database professionals who need to ensure the accuracy and integrity of their data. By mastering the use of triggers in SQL, you can become more effective at managing and analyzing data in a wide range of contexts.","title":"Why triggers are important for data analysis job"},{"location":"sql/20_triggers/#conclusion","text":"Using triggers in SQL is a powerful way to automate tasks, maintain data integrity, and enforce business rules. Triggers can be used to respond to specific events or actions that occur within a database, and can be defined to execute before or after specific operations such as inserts, updates, and deletes. By understanding how to use triggers in SQL, you can ensure that your database remains consistent and accurate over time.","title":"Conclusion"},{"location":"sql/21_er_diagrams/","text":"ER Diagrams Explanation of ER Diagrams and their Importance ER (Entity-Relationship) diagrams are a visual representation of the relationships between entities (tables) in a database. They are an important tool for designing and managing databases, as they provide a clear and concise way to understand how different tables are related to each other. ER diagrams are used to model and plan database structures, to optimize data retrieval and to make queries more efficient. They are also used to help non-technical stakeholders understand the relationships between data tables, as they provide a simple and visual way to communicate complex data structures. Overview of How to Use ER Diagrams to Visualize Data ER diagrams consist of entities (tables) and relationships (connections between tables), and are created using a set of symbols and rules. Entities are represented as rectangles, and relationships are represented as lines connecting the entities. Here's an example of an ER diagram for our employees database: You can see the full documentation of the database on the official page here In this diagram, the rectangles represent entities (tables), and the lines connecting the entities represent relationships between them. The relationships are labeled with the type of relationship (e.g. \"belongs to\", \"manages\", etc.), and with cardinality symbols that indicate the minimum and maximum number of instances of one entity that can be related to instances of the other entity. ER diagrams can be used to model complex relationships between tables, including one-to-one, one-to-many, and many-to-many relationships. They can also be used to represent subtypes and supertypes, and to define constraints and business rules that govern the relationships between entities. By using ER diagrams to visualize data, you can better understand the structure of your database and optimize your queries for maximum efficiency. ER diagrams can also be used to communicate complex data structures to non-technical stakeholders, helping to ensure that everyone involved in your data analysis projects has a common understanding of the data structures and relationships at play. In-depth Look at How to Design an ER Diagram The process of designing an ER (Entity-Relationship) diagram involves identifying the entities (tables) that will be included in the database, and determining the relationships between them. This process is crucial for building a database that is efficient, accurate, and easy to work with. Here are the steps involved in designing an ER diagram: Identify the entities : Start by identifying the different entities (tables) that will be included in the database. These might include things like customers, products, orders, and so on. Determine the attributes : For each entity, determine the attributes (columns) that will be included in the table. These might include things like names, addresses, dates, and so on. Determine the relationships : Once you have identified the entities and their attributes, determine the relationships between them. For example, a customer might place many orders, or an order might contain many products. Choose the cardinality : For each relationship, determine the cardinality, or how many instances of one entity can be related to instances of another entity. This might be one-to-one, one-to-many, or many-to-many. Choose the modality : For each relationship, determine the modality, or whether the relationship is mandatory or optional. This might be represented using symbols such as \"1\" for mandatory or \"0\" for optional. Create the ER diagram : Finally, create the ER diagram using symbols and conventions that represent the entities, attributes, and relationships. These might include rectangles for entities, diamonds for relationships, and lines connecting them. Explanation of Different Entities, Attributes, and Relationships In ER diagrams, entities are represented as rectangles, attributes are represented as ovals, and relationships are represented as diamonds. Here's a brief explanation of each: Entities : Entities represent tables in the database. Each entity has a name and a set of attributes that define the data it contains. Attributes : Attributes represent the columns within an entity. They define the specific data that the entity contains, such as names, addresses, and dates. Relationships : Relationships represent the connections between entities. They define how different entities are related to each other, and the cardinality and modality of the relationship. Some common types of relationships that you might encounter in ER diagrams include: One-to-one : One instance of an entity is related to only one instance of another entity. One-to-many : One instance of an entity is related to multiple instances of another entity. Many-to-many : Multiple instances of an entity are related to multiple instances of another entity. By understanding the different entities, attributes, and relationships involved in an ER diagram, you can design a database that is efficient, accurate, and easy to work with. This can help you to analyze and manage your data more effectively, and to gain deeper insights into the patterns and trends in your data. Explanation of How to Convert ER Diagrams to Database Schemas Once you have designed an ER (Entity-Relationship) diagram for your database, the next step is to convert it into a database schema. This involves creating tables and relationships that reflect the structure and relationships defined in the ER diagram. Here are the steps involved in converting an ER diagram to a database schema: Identify the tables: Start by identifying the tables that will be created based on the entities in the ER diagram. For example, if the ER diagram has an entity called \"Employees\", create a table called \"Employees\" in the database. Create the columns: For each table, create the columns that reflect the attributes of the entity in the ER diagram. For example, if the \"Employees\" entity has attributes such as \"Employee Number\", \"First Name\", and \"Last Name\", create columns in the \"Employees\" table with these same names and data types. Define the relationships: Once the tables and columns are created, define the relationships between the tables based on the relationships in the ER diagram. For example, if the \"Employees\" entity is related to the \"Departments\" entity in the ER diagram, create a foreign key column in the \"Employees\" table that references the primary key column in the \"Departments\" table. Specify the constraints: Finally, specify any constraints or rules that are defined in the ER diagram, such as unique constraints or not-null constraints. Overview of How to Create Tables and Relationships Based on the ER Diagram Example of how to convert the ER diagram for the Employees database into a database schema: Identify the tables : Based on the ER diagram, we need to create tables for employees, departments, titles, salaries, and more. Create the columns : For the \"Employees\" table, we need to create columns such as \"emp_no\", \"birth_date\", \"first_name\", \"last_name\", \"gender\", and \"hire_date\". For the \"Departments\" table, we need to create columns such as \"dept_no\" and \"dept_name\". For the other tables, we would create columns based on the attributes defined in the ER diagram. Define the relationships : Based on the ER diagram, we know that employees are related to departments, titles, and salaries. To define these relationships, we would create foreign key columns in the \"Employees\" table that reference the primary key columns in the other tables. For example, the \"Employees\" table would have a foreign key column called \"dept_no\" that references the \"dept_no\" column in the \"Departments\" table. Specify the constraints : We would specify any constraints or rules that are defined in the ER diagram, such as unique constraints or not-null constraints. For example, we might specify that the \"emp_no\" column in the \"Employees\" table must be unique and not-null. By converting the ER diagram to a database schema in this way, we can create a database that accurately reflects the structure and relationships defined in the ER diagram. This can help us to manage and analyze data more effectively, and to gain deeper insights into the patterns and trends in our data. Wrap-up Let's summarize what we've learn about ER diagrams : ER diagrams, or Entity-Relationship diagrams, are visual representations of data models that show the relationships between different entities, such as tables in a database. ER diagrams help to simplify complex data structures and make them easier to understand by providing a high-level view of the data and its relationships. ER diagrams can help companies to design more effective databases by providing a clear picture of how data is structured and related, and can help to identify areas for optimization or improvement. ER diagrams are important in the development process of software and databases, as they provide a blueprint for developers to follow when building and maintaining data systems. ER diagrams can be used to facilitate communication between stakeholders, such as developers, designers, project managers, and business users, by providing a common language and visual reference point for discussing the data model. By using ER diagrams to design and maintain their data models, companies can improve the accuracy, efficiency, and reliability of their data systems, which in turn can lead to better decision-making, cost savings, and increased competitiveness in the marketplace.","title":"ER DIagrams"},{"location":"sql/21_er_diagrams/#er-diagrams","text":"","title":"ER Diagrams"},{"location":"sql/21_er_diagrams/#explanation-of-er-diagrams-and-their-importance","text":"ER (Entity-Relationship) diagrams are a visual representation of the relationships between entities (tables) in a database. They are an important tool for designing and managing databases, as they provide a clear and concise way to understand how different tables are related to each other. ER diagrams are used to model and plan database structures, to optimize data retrieval and to make queries more efficient. They are also used to help non-technical stakeholders understand the relationships between data tables, as they provide a simple and visual way to communicate complex data structures.","title":"Explanation of ER Diagrams and their Importance"},{"location":"sql/21_er_diagrams/#overview-of-how-to-use-er-diagrams-to-visualize-data","text":"ER diagrams consist of entities (tables) and relationships (connections between tables), and are created using a set of symbols and rules. Entities are represented as rectangles, and relationships are represented as lines connecting the entities. Here's an example of an ER diagram for our employees database: You can see the full documentation of the database on the official page here In this diagram, the rectangles represent entities (tables), and the lines connecting the entities represent relationships between them. The relationships are labeled with the type of relationship (e.g. \"belongs to\", \"manages\", etc.), and with cardinality symbols that indicate the minimum and maximum number of instances of one entity that can be related to instances of the other entity. ER diagrams can be used to model complex relationships between tables, including one-to-one, one-to-many, and many-to-many relationships. They can also be used to represent subtypes and supertypes, and to define constraints and business rules that govern the relationships between entities. By using ER diagrams to visualize data, you can better understand the structure of your database and optimize your queries for maximum efficiency. ER diagrams can also be used to communicate complex data structures to non-technical stakeholders, helping to ensure that everyone involved in your data analysis projects has a common understanding of the data structures and relationships at play.","title":"Overview of How to Use ER Diagrams to Visualize Data"},{"location":"sql/21_er_diagrams/#in-depth-look-at-how-to-design-an-er-diagram","text":"The process of designing an ER (Entity-Relationship) diagram involves identifying the entities (tables) that will be included in the database, and determining the relationships between them. This process is crucial for building a database that is efficient, accurate, and easy to work with. Here are the steps involved in designing an ER diagram: Identify the entities : Start by identifying the different entities (tables) that will be included in the database. These might include things like customers, products, orders, and so on. Determine the attributes : For each entity, determine the attributes (columns) that will be included in the table. These might include things like names, addresses, dates, and so on. Determine the relationships : Once you have identified the entities and their attributes, determine the relationships between them. For example, a customer might place many orders, or an order might contain many products. Choose the cardinality : For each relationship, determine the cardinality, or how many instances of one entity can be related to instances of another entity. This might be one-to-one, one-to-many, or many-to-many. Choose the modality : For each relationship, determine the modality, or whether the relationship is mandatory or optional. This might be represented using symbols such as \"1\" for mandatory or \"0\" for optional. Create the ER diagram : Finally, create the ER diagram using symbols and conventions that represent the entities, attributes, and relationships. These might include rectangles for entities, diamonds for relationships, and lines connecting them.","title":"In-depth Look at How to Design an ER Diagram"},{"location":"sql/21_er_diagrams/#explanation-of-different-entities-attributes-and-relationships","text":"In ER diagrams, entities are represented as rectangles, attributes are represented as ovals, and relationships are represented as diamonds. Here's a brief explanation of each: Entities : Entities represent tables in the database. Each entity has a name and a set of attributes that define the data it contains. Attributes : Attributes represent the columns within an entity. They define the specific data that the entity contains, such as names, addresses, and dates. Relationships : Relationships represent the connections between entities. They define how different entities are related to each other, and the cardinality and modality of the relationship.","title":"Explanation of Different Entities, Attributes, and Relationships"},{"location":"sql/21_er_diagrams/#some-common-types-of-relationships-that-you-might-encounter-in-er-diagrams-include","text":"One-to-one : One instance of an entity is related to only one instance of another entity. One-to-many : One instance of an entity is related to multiple instances of another entity. Many-to-many : Multiple instances of an entity are related to multiple instances of another entity. By understanding the different entities, attributes, and relationships involved in an ER diagram, you can design a database that is efficient, accurate, and easy to work with. This can help you to analyze and manage your data more effectively, and to gain deeper insights into the patterns and trends in your data.","title":"Some common types of relationships that you might encounter in ER diagrams include:"},{"location":"sql/21_er_diagrams/#explanation-of-how-to-convert-er-diagrams-to-database-schemas","text":"Once you have designed an ER (Entity-Relationship) diagram for your database, the next step is to convert it into a database schema. This involves creating tables and relationships that reflect the structure and relationships defined in the ER diagram. Here are the steps involved in converting an ER diagram to a database schema: Identify the tables: Start by identifying the tables that will be created based on the entities in the ER diagram. For example, if the ER diagram has an entity called \"Employees\", create a table called \"Employees\" in the database. Create the columns: For each table, create the columns that reflect the attributes of the entity in the ER diagram. For example, if the \"Employees\" entity has attributes such as \"Employee Number\", \"First Name\", and \"Last Name\", create columns in the \"Employees\" table with these same names and data types. Define the relationships: Once the tables and columns are created, define the relationships between the tables based on the relationships in the ER diagram. For example, if the \"Employees\" entity is related to the \"Departments\" entity in the ER diagram, create a foreign key column in the \"Employees\" table that references the primary key column in the \"Departments\" table. Specify the constraints: Finally, specify any constraints or rules that are defined in the ER diagram, such as unique constraints or not-null constraints. Overview of How to Create Tables and Relationships Based on the ER Diagram","title":"Explanation of How to Convert ER Diagrams to Database Schemas"},{"location":"sql/21_er_diagrams/#example-of-how-to-convert-the-er-diagram-for-the-employees-database-into-a-database-schema","text":"Identify the tables : Based on the ER diagram, we need to create tables for employees, departments, titles, salaries, and more. Create the columns : For the \"Employees\" table, we need to create columns such as \"emp_no\", \"birth_date\", \"first_name\", \"last_name\", \"gender\", and \"hire_date\". For the \"Departments\" table, we need to create columns such as \"dept_no\" and \"dept_name\". For the other tables, we would create columns based on the attributes defined in the ER diagram. Define the relationships : Based on the ER diagram, we know that employees are related to departments, titles, and salaries. To define these relationships, we would create foreign key columns in the \"Employees\" table that reference the primary key columns in the other tables. For example, the \"Employees\" table would have a foreign key column called \"dept_no\" that references the \"dept_no\" column in the \"Departments\" table. Specify the constraints : We would specify any constraints or rules that are defined in the ER diagram, such as unique constraints or not-null constraints. For example, we might specify that the \"emp_no\" column in the \"Employees\" table must be unique and not-null. By converting the ER diagram to a database schema in this way, we can create a database that accurately reflects the structure and relationships defined in the ER diagram. This can help us to manage and analyze data more effectively, and to gain deeper insights into the patterns and trends in our data.","title":"Example of how to convert the ER diagram for the Employees database into a database schema:"},{"location":"sql/21_er_diagrams/#wrap-up","text":"Let's summarize what we've learn about ER diagrams : ER diagrams, or Entity-Relationship diagrams, are visual representations of data models that show the relationships between different entities, such as tables in a database. ER diagrams help to simplify complex data structures and make them easier to understand by providing a high-level view of the data and its relationships. ER diagrams can help companies to design more effective databases by providing a clear picture of how data is structured and related, and can help to identify areas for optimization or improvement. ER diagrams are important in the development process of software and databases, as they provide a blueprint for developers to follow when building and maintaining data systems. ER diagrams can be used to facilitate communication between stakeholders, such as developers, designers, project managers, and business users, by providing a common language and visual reference point for discussing the data model. By using ER diagrams to design and maintain their data models, companies can improve the accuracy, efficiency, and reliability of their data systems, which in turn can lead to better decision-making, cost savings, and increased competitiveness in the marketplace.","title":"Wrap-up"},{"location":"sql/22_Python_SQLAlchemy/","text":"Python and SQLAlchemy SQLAlchemy is an open-source library for Python that provides a way to work with relational databases. It allows you to interact with databases using a high-level object-oriented interface, rather than writing SQL code directly. Explanation of What SQLAlchemy is and Its Benefits for Working with Databases SQLAlchemy provides several benefits for working with databases, including: Abstraction : SQLAlchemy provides a layer of abstraction between the application and the database, which makes it easier to switch between different databases without changing the application code. Flexibility : SQLAlchemy allows you to work with databases using an object-oriented paradigm, which can be more flexible than using SQL directly. Portability : SQLAlchemy is a Python library, which means it can be used on any platform that supports Python, including Windows, Mac, and Linux. Security : SQLAlchemy includes built-in security features, such as support for parameterized queries, which can help protect against SQL injection attacks. Overview of SQLAlchemy Concepts and How They Relate to SQL SQLAlchemy provides several concepts that are used to work with databases, including: Engine : The Engine is the entry point for working with a database. It provides a way to connect to the database and execute SQL commands. Connection : A Connection is an instance of a connection to the database. It allows you to execute SQL commands and manage transactions. Transaction : A Transaction is a sequence of SQL commands that are executed as a single unit of work. Transactions provide a way to ensure that multiple SQL commands are executed atomically. Session : A Session is a high-level object that provides an interface for working with a database. It provides a way to manage database operations, such as creating, updating, and deleting records. Model : A Model is a Python class that represents a table in the database. It provides a way to interact with the data in the table using Python code. SQLAlchemy installation The first step is to install SQLAlchemy. You can install it using pip, the Python package manager. Open a terminal or command prompt and type the following command: pip install sqlalchemy This will download and install the latest version of SQLAlchemy. You can verify the installation with this command on your terminal : pip list | grep sqlalchemy Install a MySQL database connector In order to connect to a MySQL database, you'll need to install a database connector (we will see later that we can connect multiple databases not only MySQL). There are several available, but the most popular one is mysql-connector-python . You can install it using pip: pip install mysql-connector-python Verify the connection Now that you have both SQLAlchemy and the MySQL connector installed, you can connect to your MySQL database. First, import the necessary modules: from sqlalchemy import create_engine import mysql.connector Next, create an engine that connects to your MySQL database: engine = create_engine ( 'mysql+mysqlconnector://user:password@host:port/database' ) Replace user and password with your MySQL username and password , host with the hostname or IP address of your MySQL server, port with the port number (usually 3306), and database with the name of the database you want to connect to. Databases supports SQLAlchemy supports a variety of different database connections, each with its own advantages and use cases. Here's an overview of some of the most common connection types with the engine object: SQLite SQLite is a serverless, file-based database that is popular for small to medium-sized projects. SQLAlchemy provides a SQLite dialect that allows you to work with SQLite databases in Python. Since SQLite is a file-based database, it is relatively easy to set up and use, and is a good choice for applications that don't require a lot of concurrent users or high write volumes. from sqlalchemy import create_engine engine = create_engine ( 'sqlite:///mydatabase.db' ) In this example, we use the create_engine() function to create an engine object that connects to a SQLite database file named mydatabase.db. MySQL MySQL is a popular open-source relational database system that is commonly used for web applications. SQLAlchemy provides a MySQL dialect that allows you to work with MySQL databases in Python. MySQL is a good choice for applications that require scalability and performance, since it can handle high volumes of reads and writes. from sqlalchemy import create_engine engine = create_engine ( 'mysql+pymysql://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a MySQL database named database running on a host with the specified username , password , host , and port . In our case we will be connection our localhost , so we can re write this command as follow : engine = create_engine ( 'mysql+mysqlconnector://user:password@localhost/database' ) Use MAMP SQL Server on Mac OS In case you have this error : ModuleNotFoundError : No module named 'MySQLdb' We have seen in the chapter 5 how to install a SQL server with MAMP so we will leverage this installation for connecting our python code to MAMP SQL Server ! Follow these steps : Install the latest version of XCode & XCode developer tools. Install brew package manager, here you can find the official documentation Install MySQL using this command: brew install mysql Run this command into your terminal to make a async link between MySQL socket: sudo ln -s /Applications/MAMP/tmp/mysql/mysql.sock /tmp/mysql.sock (your socket url could be different if you using are xampp or any other local server.) Start MAMP/XAMMP MySQL server Then install (or re-install) sqlalchemy etc. We will not see in detail the other types of connectors for others databases, it's very similar as you can see bellow. PostgreSQL PostgreSQL is a powerful open-source relational database system that is known for its robustness and scalability. SQLAlchemy provides a PostgreSQL dialect that allows you to work with PostgreSQL databases in Python. PostgreSQL is a good choice for applications that require high performance, scalability, and data integrity. from sqlalchemy import create_engine engine = create_engine ( 'postgresql+psycopg2://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a PostgreSQL database named database running on a host with the specified username, password, host, and port. Oracle Oracle is a commercial relational database system that is popular in enterprise settings. SQLAlchemy provides an Oracle dialect that allows you to work with Oracle databases in Python. Oracle is a good choice for applications that require high performance, scalability, and data integrity, and can handle large amounts of data and concurrent users. from sqlalchemy import create_engine engine = create_engine ( 'oracle+cx_oracle://username:password@host:port/SID' ) In this example, we use the create_engine() function to create an engine object that connects to an Oracle database identified by its SID, running on a host with the specified username, password, and port. Microsoft SQL Server Microsoft SQL Server is a popular relational database system that is commonly used in Windows environments. SQLAlchemy provides a SQL Server dialect that allows you to work with SQL Server databases in Python. SQL Server is a good choice for applications that require high performance and scalability, and can handle large amounts of data and concurrent users. from sqlalchemy import create_engine engine = create_engine ( 'mssql+pymssql://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a Microsoft SQL Server database named database running on a host with the specified username, password, host, and port. As you can see, it is pretty much the same process. Choosing the right Database Choosing the right database connection is an important consideration when working with SQLAlchemy. By understanding the strengths and weaknesses of different database systems and dialects, you can choose the best option for your project. SQLAlchemy provides a consistent, high-level interface to all of these systems, allowing you to work with them in a uniform way. With SQLAlchemy, you can write Python code that works with a wide range of databases, and easily switch between them as your needs change. Connect to our Employees database Let's write a python script connect_db.py who connects to our MySQL employees database using SQLAlchemy library and retrieves some data (or metadata) about the database tables. from sqlalchemy import create_engine , inspect host = '127.0.0.1' db = 'employees' # create an engine to connect to the database print ( f '--- Connecting database : mysql://user:password@ { host } :3306/ { db } --- \\n ' ) engine = create_engine ( f 'mysql://user:password@localhost:3306/ { db } ' ) # create an inspector to get metadata about the database inspector = inspect ( engine ) print ( f '--- \u2705 Connection database OK \u2705---' ) # get a list of all the table names in the database table_names = inspector . get_table_names () print ( 'Tables : \\n\\n ' ) # iterate over the table names and print the table schema for table_name in table_names : columns = inspector . get_columns ( table_name ) print ( f \"Table: { table_name } \" ) for column in columns : print ( f \" { column [ 'name' ] } : { column [ 'type' ] } \" ) print ( \" \\n \" ) # explicitly close the connection to the database engine . dispose () Here is a step-by-step explanation of what the script does: The script starts by importing the required modules from the SQLAlchemy library, including the create_engine and inspect functions. The host and db variables are defined to store the connection details for the MySQL database. In this case, the host is set to 127.0.0.1 and the database name is employees. The create_engine function is used to create a connection to the MySQL database using the specified host, port, username, password, and database name. The connection details are passed as a string to the create_engine function, which returns an engine object. An inspector object is created using the inspect function and the engine object. The inspector is used to retrieve metadata about the database tables. A success message is printed to the console to indicate that the database connection was successful. The get_table_names function is called on the inspector object to retrieve a list of all table names in the database. A loop is then used to iterate over the table names and retrieve the column information for each table. Inside the loop, the get_columns function is called on the inspector object to retrieve the column details for the current table. The column details are then printed to the console, including the column name and data type. Once all tables have been processed, the loop ends and the script exits and the connection to the database is closed. In summary, this script connects to a MySQL database, retrieves metadata about the tables, and displays the column details for each table. This is a useful way to quickly check the schema of a database and verify that it is set up correctly. Do not forget to use you username and password in order to insure a good connection. Closing the connection When a SQLAlchemy Engine is created, it creates a pool of connections to the database which are automatically checked out and returned when needed by the application. This means that once an Engine has been created, it can be used for the lifetime of the application without the need to explicitly close the connection or perform any other cleanup. However, it's good practice to close the connection when you're done using it, especially in long-running applications, to ensure that database resources are properly released. To close the connection in SQLAlchemy, you can use the Engine.dispose() method, which will release all resources associated with the Engine.","title":"Introduction"},{"location":"sql/22_Python_SQLAlchemy/#python-and-sqlalchemy","text":"SQLAlchemy is an open-source library for Python that provides a way to work with relational databases. It allows you to interact with databases using a high-level object-oriented interface, rather than writing SQL code directly.","title":"Python and SQLAlchemy"},{"location":"sql/22_Python_SQLAlchemy/#explanation-of-what-sqlalchemy-is-and-its-benefits-for-working-with-databases","text":"SQLAlchemy provides several benefits for working with databases, including: Abstraction : SQLAlchemy provides a layer of abstraction between the application and the database, which makes it easier to switch between different databases without changing the application code. Flexibility : SQLAlchemy allows you to work with databases using an object-oriented paradigm, which can be more flexible than using SQL directly. Portability : SQLAlchemy is a Python library, which means it can be used on any platform that supports Python, including Windows, Mac, and Linux. Security : SQLAlchemy includes built-in security features, such as support for parameterized queries, which can help protect against SQL injection attacks.","title":"Explanation of What SQLAlchemy is and Its Benefits for Working with Databases"},{"location":"sql/22_Python_SQLAlchemy/#overview-of-sqlalchemy-concepts-and-how-they-relate-to-sql","text":"SQLAlchemy provides several concepts that are used to work with databases, including: Engine : The Engine is the entry point for working with a database. It provides a way to connect to the database and execute SQL commands. Connection : A Connection is an instance of a connection to the database. It allows you to execute SQL commands and manage transactions. Transaction : A Transaction is a sequence of SQL commands that are executed as a single unit of work. Transactions provide a way to ensure that multiple SQL commands are executed atomically. Session : A Session is a high-level object that provides an interface for working with a database. It provides a way to manage database operations, such as creating, updating, and deleting records. Model : A Model is a Python class that represents a table in the database. It provides a way to interact with the data in the table using Python code.","title":"Overview of SQLAlchemy Concepts and How They Relate to SQL"},{"location":"sql/22_Python_SQLAlchemy/#sqlalchemy-installation","text":"The first step is to install SQLAlchemy. You can install it using pip, the Python package manager. Open a terminal or command prompt and type the following command: pip install sqlalchemy This will download and install the latest version of SQLAlchemy. You can verify the installation with this command on your terminal : pip list | grep sqlalchemy","title":"SQLAlchemy installation"},{"location":"sql/22_Python_SQLAlchemy/#install-a-mysql-database-connector","text":"In order to connect to a MySQL database, you'll need to install a database connector (we will see later that we can connect multiple databases not only MySQL). There are several available, but the most popular one is mysql-connector-python . You can install it using pip: pip install mysql-connector-python","title":"Install a MySQL database connector"},{"location":"sql/22_Python_SQLAlchemy/#verify-the-connection","text":"Now that you have both SQLAlchemy and the MySQL connector installed, you can connect to your MySQL database. First, import the necessary modules: from sqlalchemy import create_engine import mysql.connector Next, create an engine that connects to your MySQL database: engine = create_engine ( 'mysql+mysqlconnector://user:password@host:port/database' ) Replace user and password with your MySQL username and password , host with the hostname or IP address of your MySQL server, port with the port number (usually 3306), and database with the name of the database you want to connect to.","title":"Verify the connection"},{"location":"sql/22_Python_SQLAlchemy/#databases-supports","text":"SQLAlchemy supports a variety of different database connections, each with its own advantages and use cases. Here's an overview of some of the most common connection types with the engine object:","title":"Databases supports"},{"location":"sql/22_Python_SQLAlchemy/#sqlite","text":"SQLite is a serverless, file-based database that is popular for small to medium-sized projects. SQLAlchemy provides a SQLite dialect that allows you to work with SQLite databases in Python. Since SQLite is a file-based database, it is relatively easy to set up and use, and is a good choice for applications that don't require a lot of concurrent users or high write volumes. from sqlalchemy import create_engine engine = create_engine ( 'sqlite:///mydatabase.db' ) In this example, we use the create_engine() function to create an engine object that connects to a SQLite database file named mydatabase.db.","title":"SQLite"},{"location":"sql/22_Python_SQLAlchemy/#mysql","text":"MySQL is a popular open-source relational database system that is commonly used for web applications. SQLAlchemy provides a MySQL dialect that allows you to work with MySQL databases in Python. MySQL is a good choice for applications that require scalability and performance, since it can handle high volumes of reads and writes. from sqlalchemy import create_engine engine = create_engine ( 'mysql+pymysql://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a MySQL database named database running on a host with the specified username , password , host , and port . In our case we will be connection our localhost , so we can re write this command as follow : engine = create_engine ( 'mysql+mysqlconnector://user:password@localhost/database' )","title":"MySQL"},{"location":"sql/22_Python_SQLAlchemy/#use-mamp-sql-server-on-mac-os","text":"In case you have this error : ModuleNotFoundError : No module named 'MySQLdb' We have seen in the chapter 5 how to install a SQL server with MAMP so we will leverage this installation for connecting our python code to MAMP SQL Server ! Follow these steps : Install the latest version of XCode & XCode developer tools. Install brew package manager, here you can find the official documentation Install MySQL using this command: brew install mysql Run this command into your terminal to make a async link between MySQL socket: sudo ln -s /Applications/MAMP/tmp/mysql/mysql.sock /tmp/mysql.sock (your socket url could be different if you using are xampp or any other local server.) Start MAMP/XAMMP MySQL server Then install (or re-install) sqlalchemy etc. We will not see in detail the other types of connectors for others databases, it's very similar as you can see bellow.","title":"Use MAMP SQL Server on Mac OS"},{"location":"sql/22_Python_SQLAlchemy/#postgresql","text":"PostgreSQL is a powerful open-source relational database system that is known for its robustness and scalability. SQLAlchemy provides a PostgreSQL dialect that allows you to work with PostgreSQL databases in Python. PostgreSQL is a good choice for applications that require high performance, scalability, and data integrity. from sqlalchemy import create_engine engine = create_engine ( 'postgresql+psycopg2://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a PostgreSQL database named database running on a host with the specified username, password, host, and port.","title":"PostgreSQL"},{"location":"sql/22_Python_SQLAlchemy/#oracle","text":"Oracle is a commercial relational database system that is popular in enterprise settings. SQLAlchemy provides an Oracle dialect that allows you to work with Oracle databases in Python. Oracle is a good choice for applications that require high performance, scalability, and data integrity, and can handle large amounts of data and concurrent users. from sqlalchemy import create_engine engine = create_engine ( 'oracle+cx_oracle://username:password@host:port/SID' ) In this example, we use the create_engine() function to create an engine object that connects to an Oracle database identified by its SID, running on a host with the specified username, password, and port.","title":"Oracle"},{"location":"sql/22_Python_SQLAlchemy/#microsoft-sql-server","text":"Microsoft SQL Server is a popular relational database system that is commonly used in Windows environments. SQLAlchemy provides a SQL Server dialect that allows you to work with SQL Server databases in Python. SQL Server is a good choice for applications that require high performance and scalability, and can handle large amounts of data and concurrent users. from sqlalchemy import create_engine engine = create_engine ( 'mssql+pymssql://username:password@host:port/database' ) In this example, we use the create_engine() function to create an engine object that connects to a Microsoft SQL Server database named database running on a host with the specified username, password, host, and port. As you can see, it is pretty much the same process.","title":"Microsoft SQL Server"},{"location":"sql/22_Python_SQLAlchemy/#choosing-the-right-database","text":"Choosing the right database connection is an important consideration when working with SQLAlchemy. By understanding the strengths and weaknesses of different database systems and dialects, you can choose the best option for your project. SQLAlchemy provides a consistent, high-level interface to all of these systems, allowing you to work with them in a uniform way. With SQLAlchemy, you can write Python code that works with a wide range of databases, and easily switch between them as your needs change.","title":"Choosing the right Database"},{"location":"sql/22_Python_SQLAlchemy/#connect-to-our-employees-database","text":"Let's write a python script connect_db.py who connects to our MySQL employees database using SQLAlchemy library and retrieves some data (or metadata) about the database tables. from sqlalchemy import create_engine , inspect host = '127.0.0.1' db = 'employees' # create an engine to connect to the database print ( f '--- Connecting database : mysql://user:password@ { host } :3306/ { db } --- \\n ' ) engine = create_engine ( f 'mysql://user:password@localhost:3306/ { db } ' ) # create an inspector to get metadata about the database inspector = inspect ( engine ) print ( f '--- \u2705 Connection database OK \u2705---' ) # get a list of all the table names in the database table_names = inspector . get_table_names () print ( 'Tables : \\n\\n ' ) # iterate over the table names and print the table schema for table_name in table_names : columns = inspector . get_columns ( table_name ) print ( f \"Table: { table_name } \" ) for column in columns : print ( f \" { column [ 'name' ] } : { column [ 'type' ] } \" ) print ( \" \\n \" ) # explicitly close the connection to the database engine . dispose () Here is a step-by-step explanation of what the script does: The script starts by importing the required modules from the SQLAlchemy library, including the create_engine and inspect functions. The host and db variables are defined to store the connection details for the MySQL database. In this case, the host is set to 127.0.0.1 and the database name is employees. The create_engine function is used to create a connection to the MySQL database using the specified host, port, username, password, and database name. The connection details are passed as a string to the create_engine function, which returns an engine object. An inspector object is created using the inspect function and the engine object. The inspector is used to retrieve metadata about the database tables. A success message is printed to the console to indicate that the database connection was successful. The get_table_names function is called on the inspector object to retrieve a list of all table names in the database. A loop is then used to iterate over the table names and retrieve the column information for each table. Inside the loop, the get_columns function is called on the inspector object to retrieve the column details for the current table. The column details are then printed to the console, including the column name and data type. Once all tables have been processed, the loop ends and the script exits and the connection to the database is closed. In summary, this script connects to a MySQL database, retrieves metadata about the tables, and displays the column details for each table. This is a useful way to quickly check the schema of a database and verify that it is set up correctly. Do not forget to use you username and password in order to insure a good connection.","title":"Connect to our Employees database"},{"location":"sql/22_Python_SQLAlchemy/#closing-the-connection","text":"When a SQLAlchemy Engine is created, it creates a pool of connections to the database which are automatically checked out and returned when needed by the application. This means that once an Engine has been created, it can be used for the lifetime of the application without the need to explicitly close the connection or perform any other cleanup. However, it's good practice to close the connection when you're done using it, especially in long-running applications, to ensure that database resources are properly released. To close the connection in SQLAlchemy, you can use the Engine.dispose() method, which will release all resources associated with the Engine.","title":"Closing the connection"},{"location":"sql/23_python_query/","text":"Querying Data with Python In this section we will see how to use SQLAlchemy for connecting MySQL database and play with the data. Querying an existing database Let's write a python script query_db.py who connects to a MySQL database using SQLAlchemy library, select a table and perform a query. from sqlalchemy import create_engine , inspect # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create an inspector to get metadata about the database inspector = inspect ( engine ) # get a list of all the table names in the database table_names = inspector . get_table_names () # iterate over the table, select employees table and perform a query for table_name in table_names : if table_name == 'employees' : columns = inspector . get_columns ( table_name ) print ( f \"Table: { table_name } \" ) for column in columns : print ( f \" { column [ 'name' ] } : { column [ 'type' ] } \" ) print ( \" \\n \" ) employees = engine . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in employees : print ( row ) The only difference with the previous code connect_db.py is the highlight part when we iterate over each table name in the list and checks if the table name is employees . If it is, it gets the columns for the employees table and prints out each column name and data type. Next, the code executes a SQL query to select the first 10 male employees in the employees table and prints out each row returned by the query. Overall, this code demonstrates how to use SQLAlchemy to connect to a MySQL database, inspect its metadata, and execute SQL queries to retrieve data from a specific table. Using session() object We don't necessarily need to use a session object if we only need to execute simple queries, as we can execute queries directly using the engine object. However, if you need to work with transactions or make more complex database operations, using a session object would be a good practice. A session object provides a transactional scope and also allows you to perform more complex operations such as creating, updating or deleting records. Let's rewrite the query above with a session() object : from sqlalchemy import create_engine , inspect from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # perform a query to select customers from London result = session . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in result : print ( row ) # close the session session . close () Note that in this example, we first create a session object using the sessionmaker function and passing the engine object as an argument. Then we can use the session object to execute the query and work with the results. Finally, we close the session to release resources. Pro and Con using session() Goods: Sessions help manage transactions by keeping track of the state of the database and the changes made to it. This helps ensure that changes are made in a consistent and safe way. Sessions can help improve performance by batching together changes made to the database and reducing the number of round-trips to the database. Sessions can provide an additional layer of abstraction that makes it easier to work with the database in a more object-oriented way. Sessions can be used to automatically generate SQL statements to perform CRUD operations on the database, reducing the amount of SQL code that needs to be written. Bads: Sessions can add complexity to the code and make it more difficult to understand and maintain. Sessions can be prone to race conditions and deadlocks, especially in high-concurrency environments. Sessions can sometimes make it more difficult to diagnose and debug issues with the database. Sessions can sometimes lead to inconsistent or unexpected behavior if not used correctly or if there are bugs in the code. Using with session() Using the with statement to automatically close the session after the execution of the query : query_db_session_with.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) # perform a query to select customers from London with Session () as session : result = session . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in result : print ( row ) Using query() Using the query method of the session: query_db_session_query.py from sqlalchemy import create_engine from sqlalchemy.orm import Session # create an engine to connect to the database engine = create_engine ( 'mysql://root:root@localhost:3306/employees' ) # create a session to work with the database with Session ( engine ) as session : # perform a query to select employees with a salary greater than 100000 result = session . query ( Employees ) . filter ( Employees . salary > 100000 ) . limit ( 10 ) for row in result : print ( row . first_name , row . last_name , row . salary ) In this example, we use the with statement to automatically close the session when we're done with it. We also use the query method to execute the SQL statement, and the filter method to filter the results based on the salary column. Finally, we use the limit method to limit the number of rows returned to 10. Note that in order to use this method, we first need to define the Employees class using the declarative_base function from SQLAlchemy. This allows us to represent the employees table as a Python class, making it easier to query the database using SQLAlchemy we will see that in the next chapter.","title":"Querying Data with Python"},{"location":"sql/23_python_query/#querying-data-with-python","text":"In this section we will see how to use SQLAlchemy for connecting MySQL database and play with the data.","title":"Querying Data with Python"},{"location":"sql/23_python_query/#querying-an-existing-database","text":"Let's write a python script query_db.py who connects to a MySQL database using SQLAlchemy library, select a table and perform a query. from sqlalchemy import create_engine , inspect # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create an inspector to get metadata about the database inspector = inspect ( engine ) # get a list of all the table names in the database table_names = inspector . get_table_names () # iterate over the table, select employees table and perform a query for table_name in table_names : if table_name == 'employees' : columns = inspector . get_columns ( table_name ) print ( f \"Table: { table_name } \" ) for column in columns : print ( f \" { column [ 'name' ] } : { column [ 'type' ] } \" ) print ( \" \\n \" ) employees = engine . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in employees : print ( row ) The only difference with the previous code connect_db.py is the highlight part when we iterate over each table name in the list and checks if the table name is employees . If it is, it gets the columns for the employees table and prints out each column name and data type. Next, the code executes a SQL query to select the first 10 male employees in the employees table and prints out each row returned by the query. Overall, this code demonstrates how to use SQLAlchemy to connect to a MySQL database, inspect its metadata, and execute SQL queries to retrieve data from a specific table.","title":"Querying an existing database"},{"location":"sql/23_python_query/#using-session-object","text":"We don't necessarily need to use a session object if we only need to execute simple queries, as we can execute queries directly using the engine object. However, if you need to work with transactions or make more complex database operations, using a session object would be a good practice. A session object provides a transactional scope and also allows you to perform more complex operations such as creating, updating or deleting records. Let's rewrite the query above with a session() object : from sqlalchemy import create_engine , inspect from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # perform a query to select customers from London result = session . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in result : print ( row ) # close the session session . close () Note that in this example, we first create a session object using the sessionmaker function and passing the engine object as an argument. Then we can use the session object to execute the query and work with the results. Finally, we close the session to release resources.","title":"Using session() object"},{"location":"sql/23_python_query/#pro-and-con-using-session","text":"Goods: Sessions help manage transactions by keeping track of the state of the database and the changes made to it. This helps ensure that changes are made in a consistent and safe way. Sessions can help improve performance by batching together changes made to the database and reducing the number of round-trips to the database. Sessions can provide an additional layer of abstraction that makes it easier to work with the database in a more object-oriented way. Sessions can be used to automatically generate SQL statements to perform CRUD operations on the database, reducing the amount of SQL code that needs to be written. Bads: Sessions can add complexity to the code and make it more difficult to understand and maintain. Sessions can be prone to race conditions and deadlocks, especially in high-concurrency environments. Sessions can sometimes make it more difficult to diagnose and debug issues with the database. Sessions can sometimes lead to inconsistent or unexpected behavior if not used correctly or if there are bugs in the code.","title":"Pro and Con using session()"},{"location":"sql/23_python_query/#using-with-session","text":"Using the with statement to automatically close the session after the execution of the query : query_db_session_with.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) # perform a query to select customers from London with Session () as session : result = session . execute ( \"SELECT * FROM employees WHERE gender = 'M' LIMIT 10\" ) for row in result : print ( row )","title":"Using with session()"},{"location":"sql/23_python_query/#using-query","text":"Using the query method of the session: query_db_session_query.py from sqlalchemy import create_engine from sqlalchemy.orm import Session # create an engine to connect to the database engine = create_engine ( 'mysql://root:root@localhost:3306/employees' ) # create a session to work with the database with Session ( engine ) as session : # perform a query to select employees with a salary greater than 100000 result = session . query ( Employees ) . filter ( Employees . salary > 100000 ) . limit ( 10 ) for row in result : print ( row . first_name , row . last_name , row . salary ) In this example, we use the with statement to automatically close the session when we're done with it. We also use the query method to execute the SQL statement, and the filter method to filter the results based on the salary column. Finally, we use the limit method to limit the number of rows returned to 10. Note that in order to use this method, we first need to define the Employees class using the declarative_base function from SQLAlchemy. This allows us to represent the employees table as a Python class, making it easier to query the database using SQLAlchemy we will see that in the next chapter.","title":"Using query()"},{"location":"sql/24_python_orm/","text":"Object Relational Mapping Python What is ORM? ORM stands for Object-Relational Mapping, which is a programming technique that allows developers to work with a relational database using an object-oriented paradigm. It maps the data stored in a database to objects in a programming language, which makes it easier for developers to work with data in their application. Traditionally, when working with a relational database, developers use SQL to write queries and retrieve data. SQL is a powerful language, but it can be difficult to work with and maintain, especially for large and complex systems. ORM provides a layer of abstraction between the database and the application, allowing developers to work with objects instead of raw SQL. ORM benefits for developers Simplification of code: With ORM, developers can work with objects and classes instead of complex SQL queries. This makes the code more readable and maintainable. Portability: ORM allows the same code to work with different databases, as it provides a common interface to access the data. Database abstraction: ORM shields developers from the complexities of different databases and database-specific SQL syntax, allowing them to focus on the application logic. Security: ORM provides built-in protection against SQL injection attacks, as it automatically sanitizes input and uses parameterized queries. Increased productivity: With ORM, developers can write code faster, as they don't have to write complex SQL queries manually. Why SQLAlchemy SQLalchemy ORM (Object-Relational Mapping) is another way of working with databases using SQLalchemy. It supports a wide range of relational databases, including MySQL, PostgreSQL, SQLite, and Oracle. In ORM, database tables are represented as classes and rows in tables are represented as objects of those classes. SQLAlchemy provides several key features: Declarative base classes: SQLAlchemy allows developers to define their database schema using Python classes. This makes it easy to map tables to classes, and provides a clear separation between the database and application logic. Session management: SQLAlchemy provides a session management system that tracks changes made to objects and commits them to the database. Query API: SQLAlchemy provides a powerful and flexible query API that allows developers to construct complex queries using Python syntax. Like the filter() function we have seen in the previous course. Support for transactions: SQLAlchemy provides support for transactions, allowing developers to roll back changes if necessary. Built-in caching: SQLAlchemy provides built-in caching to improve performance. Support for migrations: SQLAlchemy provides support for database migrations, allowing developers to change the schema of their database over time. How to use SQLAlchemy for ORM Using SQLAlchemy for ORM involves several steps: Define the database schema using Python classes. Create an engine that connects to the database. Define a session factory that will be used to create sessions. Use the session factory to create a session. Use the session to perform CRUD (Create, Read, Update, Delete) operations on the database. Here is an example of how to use SQLAlchemy for ORM and perform a query with the SQLAlchemy Query API : from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # import the Employee model from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column , Integer , String , Date , Enum Base = declarative_base () class Employee ( Base ): __tablename__ = 'employees' emp_no = Column ( Integer , primary_key = True ) birth_date = Column ( Date ) first_name = Column ( String ( 14 )) last_name = Column ( String ( 16 )) gender = Column ( Enum ( 'M' , 'F' )) hire_date = Column ( Date ) # query the database using the ORM employees = session . query ( Employee ) . filter ( Employee . gender == 'M' ) . limit ( 10 ) . all () # print the results for employee in employees : print ( employee . emp_no , employee . first_name , employee . last_name ) # close the session session . close () In this example, we define an ORM class Employee that represents the employees table in the database. We define the columns of the table as attributes of the class. To query the database, we create a session using sessionmaker and use the query method to perform a query. We filter the query to only include employees with gender 'M' and limit the results to 10 rows. We then iterate over the results and print them. The main advantage of ORM is that it allows developers to work with databases in a more object-oriented way. They can use the familiar syntax of Python classes and objects to interact with databases. It also allows for better organization and abstraction of database code. SQLalchemy provides a powerful ORM that allows developers to work with databases in a high-level, Pythonic way. The ORM provides an abstraction layer between the Python code and the database, making it easier to work with databases without needing to know SQL. Mapping the employees database from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the departments table departments = Table ( 'departments' , metadata , Column ( 'dept_no' , String ( 4 ), primary_key = True ), Column ( 'dept_name' , String ( 40 )) ) # define the dept_emp table dept_emp = Table ( 'dept_emp' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'dept_no' , String ( 4 ), ForeignKey ( 'departments.dept_no' )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the dept_manager table dept_manager = Table ( 'dept_manager' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'dept_no' , String ( 4 ), ForeignKey ( 'departments.dept_no' )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the employees table employees = Table ( 'employees' , metadata , Column ( 'emp_no' , Integer , primary_key = True ), Column ( 'birth_date' , String ( 10 )), Column ( 'first_name' , String ( 14 )), Column ( 'last_name' , String ( 16 )), Column ( 'gender' , String ( 1 )), Column ( 'hire_date' , String ( 10 )) ) # define the salaries table salaries = Table ( 'salaries' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'salary' , Integer ), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the titles table titles = Table ( 'titles' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'title' , String ( 50 )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # example query to select all employees result = session . query ( employees ) . all () for row in result : print ( row . emp_no , row . first_name , row . last_name ) # close the session session . close () In this example, we first create an engine to connect to the database using the create_engine function. Then we define the tables in the database using the Table and Column objects from the sqlalchemy module, and create a metadata object to reflect the database schema. We then create a session to interact with the database using the sessionmaker function, and use the query method of the session object to execute a SELECT statement on the employees table, selecting all rows and printing : the employee number, first name, and last name. Finally, we close the session using the close method. We can refactor our code like this : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # # define tables like in orm_1.py script # # example query try : result = session . query ( employees , salaries ) . filter ( employees . c . emp_no == salaries . c . emp_no ) . limit ( 10 ) for row in result : print ( row ) finally : session . close () Let's dig into this code First, we import the necessary modules, including create_engine, Table, Column, Integer, String, MetaData, ForeignKey, and sessionmaker. Next, we create an engine to connect to our MySQL database, using the create_engine function. We then create a session to work with the database using sessionmaker. We define metadata using MetaData. We define the employees and salaries tables using Table and Column. We join the two tables using the filter function, specifying the emp_no columns to join on. We limit the results to 10 using the limit function. Finally, we print the results of the query. This code joins the employees and salaries tables on their respective emp_no columns and returns the first 10 results. The result is a list of tuples, with each tuple containing the corresponding rows from the two tables. Perform queries Let's take a look at the top 5 highest paid employees : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # # define tables like in orm_1.py script # # example query try : result = ( session . query ( employees , salaries ) \\ . filter ( employees . c . emp_no == salaries . c . emp_no ) \\ . order_by ( desc ( salaries . c . salary )) \\ . limit ( 5 ) ) for row in result : print ( f 'Employee { row . first_name } - { row . last_name } earn { row . salary } $/year' ) #print(f\"{employee.first_name} earns {salary.salary} dollars\") finally : session . close () Here we've added an order_by() method to sort the results in descending order by the salary column. The desc() function is used to specify a descending sort. Finally, we use the limit() method to limit the results to the top 10 highest paid employees. Let's take a look to an other query, this time without using session() and ORM mapping. Using SQLAlchemy Table object to join data Let's take a look at SQLAlchemy join and why this method has several benefits over manual SQL joins: Abstraction : SQLAlchemy provides a high-level object-oriented abstraction layer over SQL, making it easier to write queries and perform joins without needing to write low-level SQL code. Portability : Because SQLAlchemy provides a layer of abstraction, it makes it easier to switch databases without having to re-write queries. This is especially useful for larger projects where databases may need to be switched due to scaling or other requirements. Security : SQLAlchemy's query system provides a safe and secure way to build complex queries, helping to prevent SQL injection attacks. Easier to read and maintain : SQLAlchemy queries are often easier to read and maintain than raw SQL queries. This is because they are written in Python, which is a more expressive and easier to read language than SQL. Object-Relational Mapping (ORM) : SQLAlchemy provides an ORM that maps database tables to Python classes, making it easy to work with data in an object-oriented manner. This can simplify code and make it easier to reason about the data model. Overall, using SQLAlchemy to join data provides a more efficient and effective way to interact with databases, reducing the likelihood of errors and making it easier to maintain code in the long-term. Using Table() The primary difference between using Table and the ORM approach in SQLAlchemy is how the data is represented and accessed. When using Table , the data is represented as tables and columns in the database. Queries are constructed using SQL-like syntax, and the results are returned as tuples or dictionaries. This approach requires a good understanding of SQL and database structure, as well as the ability to write complex queries. from sqlalchemy import create_engine , MetaData , Table , select # Define the engine engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # Define the metadata metadata = MetaData () # Define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) departments = Table ( 'departments' , metadata , autoload = True , autoload_with = engine ) # Define the query query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , departments . c . dept_name ]) . \\ select_from ( employees . join ( dept_emp ) . join ( departments , dept_emp . c . dept_no == departments . c . dept_no )) # Execute the query result = engine . execute ( query ) # Print the results for row in result : print ( row ) This query selects the emp_no , first_name , last_name , and dept_name columns from the employees , dept_emp , and departments tables, and joins them together based on the dept_no column in dept_emp and departments . It then executes the query and prints the results. ORM or Table On the other hand, the ORM approach in SQLAlchemy allows developers to work with Python classes that represent database tables, and instances of those classes represent rows in the database. Queries are constructed using Python methods and attributes, which are then translated to SQL statements by SQLAlchemy. The results are returned as instances of the corresponding Python classes, making it easier to work with the data in a more object-oriented way. Overall, the ORM approach is more intuitive for developers who are more familiar with object-oriented programming and less familiar with SQL. It also provides better abstraction from the underlying database structure, making it easier to make changes to the schema without affecting the application code. However, the Table approach may be more appropriate in cases where more fine-grained control over the SQL queries is required. Wrap-up Whether you prefer to work with SQL directly or use an ORM, SQLalchemy has you covered. With its intuitive API, SQLalchemy makes it easy to connect to databases, perform queries, and manipulate data. Understanding how to use SQLalchemy is an essential skill for any Python developer working with databases.","title":"Object Relational Mapping in Python"},{"location":"sql/24_python_orm/#object-relational-mapping-python","text":"","title":"Object Relational Mapping Python"},{"location":"sql/24_python_orm/#what-is-orm","text":"ORM stands for Object-Relational Mapping, which is a programming technique that allows developers to work with a relational database using an object-oriented paradigm. It maps the data stored in a database to objects in a programming language, which makes it easier for developers to work with data in their application. Traditionally, when working with a relational database, developers use SQL to write queries and retrieve data. SQL is a powerful language, but it can be difficult to work with and maintain, especially for large and complex systems. ORM provides a layer of abstraction between the database and the application, allowing developers to work with objects instead of raw SQL.","title":"What is ORM?"},{"location":"sql/24_python_orm/#orm-benefits-for-developers","text":"Simplification of code: With ORM, developers can work with objects and classes instead of complex SQL queries. This makes the code more readable and maintainable. Portability: ORM allows the same code to work with different databases, as it provides a common interface to access the data. Database abstraction: ORM shields developers from the complexities of different databases and database-specific SQL syntax, allowing them to focus on the application logic. Security: ORM provides built-in protection against SQL injection attacks, as it automatically sanitizes input and uses parameterized queries. Increased productivity: With ORM, developers can write code faster, as they don't have to write complex SQL queries manually.","title":"ORM benefits for developers"},{"location":"sql/24_python_orm/#why-sqlalchemy","text":"SQLalchemy ORM (Object-Relational Mapping) is another way of working with databases using SQLalchemy. It supports a wide range of relational databases, including MySQL, PostgreSQL, SQLite, and Oracle. In ORM, database tables are represented as classes and rows in tables are represented as objects of those classes. SQLAlchemy provides several key features: Declarative base classes: SQLAlchemy allows developers to define their database schema using Python classes. This makes it easy to map tables to classes, and provides a clear separation between the database and application logic. Session management: SQLAlchemy provides a session management system that tracks changes made to objects and commits them to the database. Query API: SQLAlchemy provides a powerful and flexible query API that allows developers to construct complex queries using Python syntax. Like the filter() function we have seen in the previous course. Support for transactions: SQLAlchemy provides support for transactions, allowing developers to roll back changes if necessary. Built-in caching: SQLAlchemy provides built-in caching to improve performance. Support for migrations: SQLAlchemy provides support for database migrations, allowing developers to change the schema of their database over time.","title":"Why SQLAlchemy"},{"location":"sql/24_python_orm/#how-to-use-sqlalchemy-for-orm","text":"Using SQLAlchemy for ORM involves several steps: Define the database schema using Python classes. Create an engine that connects to the database. Define a session factory that will be used to create sessions. Use the session factory to create a session. Use the session to perform CRUD (Create, Read, Update, Delete) operations on the database. Here is an example of how to use SQLAlchemy for ORM and perform a query with the SQLAlchemy Query API : from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # import the Employee model from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column , Integer , String , Date , Enum Base = declarative_base () class Employee ( Base ): __tablename__ = 'employees' emp_no = Column ( Integer , primary_key = True ) birth_date = Column ( Date ) first_name = Column ( String ( 14 )) last_name = Column ( String ( 16 )) gender = Column ( Enum ( 'M' , 'F' )) hire_date = Column ( Date ) # query the database using the ORM employees = session . query ( Employee ) . filter ( Employee . gender == 'M' ) . limit ( 10 ) . all () # print the results for employee in employees : print ( employee . emp_no , employee . first_name , employee . last_name ) # close the session session . close () In this example, we define an ORM class Employee that represents the employees table in the database. We define the columns of the table as attributes of the class. To query the database, we create a session using sessionmaker and use the query method to perform a query. We filter the query to only include employees with gender 'M' and limit the results to 10 rows. We then iterate over the results and print them. The main advantage of ORM is that it allows developers to work with databases in a more object-oriented way. They can use the familiar syntax of Python classes and objects to interact with databases. It also allows for better organization and abstraction of database code. SQLalchemy provides a powerful ORM that allows developers to work with databases in a high-level, Pythonic way. The ORM provides an abstraction layer between the Python code and the database, making it easier to work with databases without needing to know SQL.","title":"How to use SQLAlchemy for ORM"},{"location":"sql/24_python_orm/#mapping-the-employees-database","text":"from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the departments table departments = Table ( 'departments' , metadata , Column ( 'dept_no' , String ( 4 ), primary_key = True ), Column ( 'dept_name' , String ( 40 )) ) # define the dept_emp table dept_emp = Table ( 'dept_emp' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'dept_no' , String ( 4 ), ForeignKey ( 'departments.dept_no' )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the dept_manager table dept_manager = Table ( 'dept_manager' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'dept_no' , String ( 4 ), ForeignKey ( 'departments.dept_no' )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the employees table employees = Table ( 'employees' , metadata , Column ( 'emp_no' , Integer , primary_key = True ), Column ( 'birth_date' , String ( 10 )), Column ( 'first_name' , String ( 14 )), Column ( 'last_name' , String ( 16 )), Column ( 'gender' , String ( 1 )), Column ( 'hire_date' , String ( 10 )) ) # define the salaries table salaries = Table ( 'salaries' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'salary' , Integer ), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # define the titles table titles = Table ( 'titles' , metadata , Column ( 'emp_no' , Integer , ForeignKey ( 'employees.emp_no' )), Column ( 'title' , String ( 50 )), Column ( 'from_date' , String ( 10 )), Column ( 'to_date' , String ( 10 )) ) # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # example query to select all employees result = session . query ( employees ) . all () for row in result : print ( row . emp_no , row . first_name , row . last_name ) # close the session session . close () In this example, we first create an engine to connect to the database using the create_engine function. Then we define the tables in the database using the Table and Column objects from the sqlalchemy module, and create a metadata object to reflect the database schema. We then create a session to interact with the database using the sessionmaker function, and use the query method of the session object to execute a SELECT statement on the employees table, selecting all rows and printing : the employee number, first name, and last name. Finally, we close the session using the close method. We can refactor our code like this : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # # define tables like in orm_1.py script # # example query try : result = session . query ( employees , salaries ) . filter ( employees . c . emp_no == salaries . c . emp_no ) . limit ( 10 ) for row in result : print ( row ) finally : session . close () Let's dig into this code First, we import the necessary modules, including create_engine, Table, Column, Integer, String, MetaData, ForeignKey, and sessionmaker. Next, we create an engine to connect to our MySQL database, using the create_engine function. We then create a session to work with the database using sessionmaker. We define metadata using MetaData. We define the employees and salaries tables using Table and Column. We join the two tables using the filter function, specifying the emp_no columns to join on. We limit the results to 10 using the limit function. Finally, we print the results of the query. This code joins the employees and salaries tables on their respective emp_no columns and returns the first 10 results. The result is a list of tuples, with each tuple containing the corresponding rows from the two tables.","title":"Mapping the employees database"},{"location":"sql/24_python_orm/#perform-queries","text":"Let's take a look at the top 5 highest paid employees : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # create a session to interact with the database from sqlalchemy.orm import sessionmaker Session = sessionmaker ( bind = engine ) session = Session () # # define tables like in orm_1.py script # # example query try : result = ( session . query ( employees , salaries ) \\ . filter ( employees . c . emp_no == salaries . c . emp_no ) \\ . order_by ( desc ( salaries . c . salary )) \\ . limit ( 5 ) ) for row in result : print ( f 'Employee { row . first_name } - { row . last_name } earn { row . salary } $/year' ) #print(f\"{employee.first_name} earns {salary.salary} dollars\") finally : session . close () Here we've added an order_by() method to sort the results in descending order by the salary column. The desc() function is used to specify a descending sort. Finally, we use the limit() method to limit the results to the top 10 highest paid employees. Let's take a look to an other query, this time without using session() and ORM mapping.","title":"Perform queries"},{"location":"sql/24_python_orm/#using-sqlalchemy-table-object-to-join-data","text":"Let's take a look at SQLAlchemy join and why this method has several benefits over manual SQL joins: Abstraction : SQLAlchemy provides a high-level object-oriented abstraction layer over SQL, making it easier to write queries and perform joins without needing to write low-level SQL code. Portability : Because SQLAlchemy provides a layer of abstraction, it makes it easier to switch databases without having to re-write queries. This is especially useful for larger projects where databases may need to be switched due to scaling or other requirements. Security : SQLAlchemy's query system provides a safe and secure way to build complex queries, helping to prevent SQL injection attacks. Easier to read and maintain : SQLAlchemy queries are often easier to read and maintain than raw SQL queries. This is because they are written in Python, which is a more expressive and easier to read language than SQL. Object-Relational Mapping (ORM) : SQLAlchemy provides an ORM that maps database tables to Python classes, making it easy to work with data in an object-oriented manner. This can simplify code and make it easier to reason about the data model. Overall, using SQLAlchemy to join data provides a more efficient and effective way to interact with databases, reducing the likelihood of errors and making it easier to maintain code in the long-term.","title":"Using SQLAlchemy Table object to join data"},{"location":"sql/24_python_orm/#using-table","text":"The primary difference between using Table and the ORM approach in SQLAlchemy is how the data is represented and accessed. When using Table , the data is represented as tables and columns in the database. Queries are constructed using SQL-like syntax, and the results are returned as tuples or dictionaries. This approach requires a good understanding of SQL and database structure, as well as the ability to write complex queries. from sqlalchemy import create_engine , MetaData , Table , select # Define the engine engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # Define the metadata metadata = MetaData () # Define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) departments = Table ( 'departments' , metadata , autoload = True , autoload_with = engine ) # Define the query query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , departments . c . dept_name ]) . \\ select_from ( employees . join ( dept_emp ) . join ( departments , dept_emp . c . dept_no == departments . c . dept_no )) # Execute the query result = engine . execute ( query ) # Print the results for row in result : print ( row ) This query selects the emp_no , first_name , last_name , and dept_name columns from the employees , dept_emp , and departments tables, and joins them together based on the dept_no column in dept_emp and departments . It then executes the query and prints the results.","title":"Using Table()"},{"location":"sql/24_python_orm/#orm-or-table","text":"On the other hand, the ORM approach in SQLAlchemy allows developers to work with Python classes that represent database tables, and instances of those classes represent rows in the database. Queries are constructed using Python methods and attributes, which are then translated to SQL statements by SQLAlchemy. The results are returned as instances of the corresponding Python classes, making it easier to work with the data in a more object-oriented way. Overall, the ORM approach is more intuitive for developers who are more familiar with object-oriented programming and less familiar with SQL. It also provides better abstraction from the underlying database structure, making it easier to make changes to the schema without affecting the application code. However, the Table approach may be more appropriate in cases where more fine-grained control over the SQL queries is required.","title":"ORM or Table"},{"location":"sql/24_python_orm/#wrap-up","text":"Whether you prefer to work with SQL directly or use an ORM, SQLalchemy has you covered. With its intuitive API, SQLalchemy makes it easy to connect to databases, perform queries, and manipulate data. Understanding how to use SQLalchemy is an essential skill for any Python developer working with databases.","title":"Wrap-up"},{"location":"sql/25_python_update_delete/","text":"Updating and Deleting Data with Python Updating Data with Python and SQLAlchemy Using update() method The update() method in SQLAlchemy can be used to update data in a table. The syntax of update() method is as follows: table . update () . where ( condition ) . values ( column_name = new_value ) Where : table : The table that needs to be updated. condition : The condition that specifies which rows should be updated. column_name : The name of the column that needs to be updated. new_value : The new value for the column. Here's an example of how to use the update() method in SQLAlchemy to update the salary of an employee: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey , desc # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Update the salary of employee with emp_no = 10001 update_query = employees . update () . where ( employees . c . emp_no == 10001 ) . values ( first_name = 'jonh' ) # Execute the query with engine . connect () as conn : conn . execute ( update_query ) Here, the line employees = Table('employees', metadata, autoload=True, autoload_with=engine) loads the employees table from the database into a Table object named employees using the metadata object and the engine object. Then we define the update operation to be performed on the employees table. This query will update the first_name column of the row where emp_no is equal to 10001 to john . Let's verify if the database is updated with this query after the update_db.py script : # Verify if the field has been modified select_query = select ([ employees ]) . where ( employees . c . emp_no == 10001 ) # Execute the query with engine . connect () as conn : result = conn . execute ( select_query ) for row in result : print ( row ) Using execute() method Another way to update data in a table is by using the execute() method in SQLAlchemy. Here's an example: # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Define the query update_query = \"UPDATE employees SET first_name = 'jonh' WHERE emp_no = 10001\" # Execute the query with engine . connect () as conn : conn . execute ( update_query ) \ud83d\udca1 update() VS execute() The execute() method is more flexible and can be used to execute any SQL statement, including SELECT , INSERT , UPDATE , DELETE , and more. However, it requires the statement to be provided as a string, making it more error-prone and harder to maintain in large applications. On the other hand, the update() method provides a more structured way to update data in a table. It takes a filter condition that specifies which rows to update, and a set of values to apply to the specified rows. The update() method also returns a ResultProxy object that can be used to get information about the update operation, such as the number of affected rows. In general, the update() method is preferred for modifying data in a table using SQLAlchemy, as it provides a more structured and safer approach. However, in some cases, the execute() method may be more appropriate for executing complex SQL statements that cannot be expressed using the update() method. Deleting Data with Python and SQLAlchemy Using delete() method The delete() method in SQLAlchemy can be used to delete data from a table. The syntax of delete() method is as follows: table . delete () . where ( condition ) where : - table : The table that needs to be updated. - condition : The condition that specifies which rows should be deleted. Here's an example of how to use the delete() method in SQLAlchemy to delete an employee from the employees table: delete_dd.py # #import and other stuff like all the previous scripts # # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Delete the employee with emp_no = 10001 delete_query = employees . delete () . where ( employees . c . emp_no == 10001 ) # Execute the query with engine . connect () as conn : conn . execute ( delete_query ) In this example, we deleted the employee with emp_no 10001 from the employees table. Using execute() method Another way to delete data from a table is by using the execute() method in SQLAlchemy. Here's an example: # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Define the query delete_query = \"DELETE FROM employees WHERE emp_no = 10001\" # Execute the query with engine . connect () as conn : conn . execute ( delete_query ) Same principle as before. Wrap-up That's it! Now we know how to update and delete data in tables using Python and SQLAlchemy \ud83e\udd73","title":"Updating and Deleting Data with Python"},{"location":"sql/25_python_update_delete/#updating-and-deleting-data-with-python","text":"","title":"Updating and Deleting Data with Python"},{"location":"sql/25_python_update_delete/#updating-data-with-python-and-sqlalchemy","text":"","title":"Updating Data with Python and SQLAlchemy"},{"location":"sql/25_python_update_delete/#using-update-method","text":"The update() method in SQLAlchemy can be used to update data in a table. The syntax of update() method is as follows: table . update () . where ( condition ) . values ( column_name = new_value ) Where : table : The table that needs to be updated. condition : The condition that specifies which rows should be updated. column_name : The name of the column that needs to be updated. new_value : The new value for the column. Here's an example of how to use the update() method in SQLAlchemy to update the salary of an employee: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey , desc # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Update the salary of employee with emp_no = 10001 update_query = employees . update () . where ( employees . c . emp_no == 10001 ) . values ( first_name = 'jonh' ) # Execute the query with engine . connect () as conn : conn . execute ( update_query ) Here, the line employees = Table('employees', metadata, autoload=True, autoload_with=engine) loads the employees table from the database into a Table object named employees using the metadata object and the engine object. Then we define the update operation to be performed on the employees table. This query will update the first_name column of the row where emp_no is equal to 10001 to john . Let's verify if the database is updated with this query after the update_db.py script : # Verify if the field has been modified select_query = select ([ employees ]) . where ( employees . c . emp_no == 10001 ) # Execute the query with engine . connect () as conn : result = conn . execute ( select_query ) for row in result : print ( row )","title":"Using update() method"},{"location":"sql/25_python_update_delete/#using-execute-method","text":"Another way to update data in a table is by using the execute() method in SQLAlchemy. Here's an example: # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Define the query update_query = \"UPDATE employees SET first_name = 'jonh' WHERE emp_no = 10001\" # Execute the query with engine . connect () as conn : conn . execute ( update_query )","title":"Using execute() method"},{"location":"sql/25_python_update_delete/#update-vs-execute","text":"The execute() method is more flexible and can be used to execute any SQL statement, including SELECT , INSERT , UPDATE , DELETE , and more. However, it requires the statement to be provided as a string, making it more error-prone and harder to maintain in large applications. On the other hand, the update() method provides a more structured way to update data in a table. It takes a filter condition that specifies which rows to update, and a set of values to apply to the specified rows. The update() method also returns a ResultProxy object that can be used to get information about the update operation, such as the number of affected rows. In general, the update() method is preferred for modifying data in a table using SQLAlchemy, as it provides a more structured and safer approach. However, in some cases, the execute() method may be more appropriate for executing complex SQL statements that cannot be expressed using the update() method.","title":"\ud83d\udca1 update() VS execute()"},{"location":"sql/25_python_update_delete/#deleting-data-with-python-and-sqlalchemy","text":"","title":"Deleting Data with Python and SQLAlchemy"},{"location":"sql/25_python_update_delete/#using-delete-method","text":"The delete() method in SQLAlchemy can be used to delete data from a table. The syntax of delete() method is as follows: table . delete () . where ( condition ) where : - table : The table that needs to be updated. - condition : The condition that specifies which rows should be deleted. Here's an example of how to use the delete() method in SQLAlchemy to delete an employee from the employees table: delete_dd.py # #import and other stuff like all the previous scripts # # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Delete the employee with emp_no = 10001 delete_query = employees . delete () . where ( employees . c . emp_no == 10001 ) # Execute the query with engine . connect () as conn : conn . execute ( delete_query ) In this example, we deleted the employee with emp_no 10001 from the employees table.","title":"Using delete() method"},{"location":"sql/25_python_update_delete/#using-execute-method_1","text":"Another way to delete data from a table is by using the execute() method in SQLAlchemy. Here's an example: # Define the table employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # Define the query delete_query = \"DELETE FROM employees WHERE emp_no = 10001\" # Execute the query with engine . connect () as conn : conn . execute ( delete_query ) Same principle as before.","title":"Using execute() method"},{"location":"sql/25_python_update_delete/#wrap-up","text":"That's it! Now we know how to update and delete data in tables using Python and SQLAlchemy \ud83e\udd73","title":"Wrap-up"},{"location":"sql/26_python_queries/","text":"Advanced Queries with Python SQLAlchemy provides a wide range of advanced querying techniques that allow you to perform complex database queries using Python code. In this tutorial, we will cover how to use joins, subqueries, and other advanced SQL features with SQLAlchemy. Joins Joins are used to combine data from two or more tables in a single query like we have seen in the SQL section. SQLAlchemy provides several ways to perform joins, including the join() , outerjoin() , and select_from() methods. Inner Join An inner join returns only the rows that have matching values in both tables being joined. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData #connect database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () #Map employees tables employees and titles employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) titles = Table ( 'titles' , metadata , autoload = True , autoload_with = engine ) #perform query query = employees . join ( titles , employees . c . emp_no == titles . c . emp_no ) result = engine . execute ( query . select ()) #print result for row in result : print ( row ) In the above example, we used the join() method to join the employees and titles tables on the emp_no column. The resulting query returns only the rows where there is a matching emp_no in both tables. Left Join A left join returns all the rows from the left table and the matched rows from the right table. If there is no match in the right table, the result will contain NULL values. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) query = employees . join ( salaries , employees . c . emp_no == salaries . c . emp_no , isouter = True ) result = engine . execute ( query . select ()) for row in result : print ( row ) In the above example, we used the join() method to perform a left join between the employees and salaries tables on the emp_no column. The isouter=True argument specifies that we want to perform a left join . If there is no matching emp_no in the salaries table, the result will contain NULL values. Right Join A right join returns all the rows from the right table and the matched rows from the left table. If there is no match in the left table, the result will contain NULL values. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) query = salaries . join ( employees , salaries . c . emp_no == employees . c . emp_no , isouter = True ) result = engine . execute ( query . select ()) for row in result : print ( row ) In the above example, we used the join() method to perform a right join between the salaries and employees tables on the emp_no column. The iso uter=True argument specifies that we want to perform a right join. Using multiple join conditions with select_from() Let's try to print the 10 employees with the highest salary in the Development department : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey , desc , select , func # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # Define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) departments = Table ( 'departments' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) # Define the query to select the 10 employees with the highest salary in the Development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( employees . c . emp_no == salaries . c . emp_no ) . \\ where ( dept_emp . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == departments . c . dept_no ) . \\ where ( departments . c . dept_name == 'Development' ) . \\ order_by ( desc ( salaries . c . salary )) . \\ limit ( 10 ) . \\ select_from ( employees . join ( salaries ) . join ( dept_emp ) . join ( departments )) # Execute the query result = engine . execute ( query ) # Print the results for row in result : print ( row ) The query selects the columns emp_no , first_name , last_name , and salary from the employees and salaries tables, respectively. It then joins the dept_emp and departments tables to get only the employees in the Development department, and sorts the result in descending order by salary, and limits the result to the top 10 highest paid employees . Finally, it uses the select_from() method to specify the join conditions between the tables. The execute method is called on the engine object with the query object as an argument, and the results are printed in a loop as usual. Sub Queries and correlate() function In SQLAlchemy, the correlate() function is used to control correlated subqueries, which are subqueries that reference the outer query. A correlated subquery allows you to filter or aggregate data in a subquery based on the results of the outer query. The correlate() function is used to specify which tables in the subquery should be correlated to the outer query. It takes one or more tables as arguments and returns a new Select object that is a correlated subquery. Here's the same example as before modify for using correlate() function : from sqlalchemy import select , func from sqlalchemy.orm import correlate # define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) # Define the subquery to calculate average salary of development department employees subquery = select ([ func . avg ( salaries . c . salary )]) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ correlate ( employees ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # Define the query to select the 10 most paid employees in development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( salaries . c . salary > subquery ) . \\ order_by ( desc ( salaries . c . salary )) . \\ limit ( 10 ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # execute the query and print the results result = engine . execute ( query ) for row in result : print ( row ) This query is similar to the previous one we discussed, which also selects the 10 most highly paid employees in the development department. However, in this query, we use the correlate() function to manage auto-correlation between tables. The subquery is used to calculate the average salary of employees in the development department. We use the func.avg() function to calculate the average salary and apply two where clauses to filter the salaries of employees in the development department. To handle auto-correlation between tables, we use the correlate() function and pass the employees table as the argument. Finally, we join the dept_emp , employees , and salaries tables together using the join() method. The main query selects the emp_no , first_name , last_name , and salary fields from the employees and salaries tables. We again use the where clause to filter the employees in the development department and use the correlate() function to handle auto-correlation. We also add a third where clause to compare the salaries of employees in the development department with the average salary calculated by the subquery. Finally, we join the dept_emp , employees , and salaries tables together using the join() method. We sort the results in descending order of salaries and limit the output to the top 10 results. Joins and correlate() The correlate() function is different from joins in that it allows you to filter or aggregate data in a subquery based on the results of the outer query, without actually joining the tables together. \ud83d\udd0e This can be more efficient than using joins in certain situations, especially when dealing with large datasets. However, correlated subqueries can also be slower than joins , especially if the subquery is complex or returns a large amount of data. It's important to test and optimize your queries to ensure that they perform well for your specific use case. Difference between the select_from() and the joins methods The select_from() method and the join() method are both used in SQLAlchemy to specify the tables used in a SQL query, but they differ in how they are used and the types of queries they can generate. The join() method is used to specify a join between two or more tables. It generates an INNER JOIN by default, but can be used to generate other types of joins as well. This method allows you to specify the join condition, i.e. the columns used to match rows between the tables. The join() method is useful when you need to combine data from multiple tables into a single query result. The select_from() method, on the other hand, is used to specify the main table(s) used in a SQL query. It is used to specify the primary table or tables that the query will be based on. This method is useful when you need to specify a complex subquery or a nested query, or when you need to select from a view or other non-standard source of data. In general, the join() method is used to combine data from multiple tables, while the select_from() method is used to specify the primary table or tables used in the query. However, both methods can be used together to generate more complex queries that combine data from multiple tables and use subqueries or nested queries to filter or manipulate the data. Working with VIEW Let's dig a little aroud SQL view concept. What is a View in SQL? A view in SQL is a virtual table that is created based on a query. It is a named query that is saved in the database and can be used like a table in other queries. A view can be used to simplify complex queries, filter data, or provide users with access to a subset of data in the database without having to grant access to the underlying tables. Creating a View A view is created using the CREATE VIEW statement, which defines the name of the view, the columns to include, and the SELECT statement that defines the query used to create the view. Here's an example: CREATE VIEW sales_report AS SELECT year , month , sum ( revenue ) as total_revenue FROM sales GROUP BY year , month ; This creates a view called \"sales_report\" that contains three columns: year, month, and total_revenue. The view is based on the sales table, and calculates the total revenue for each month and year. Using a View Once a view is created, it can be used like any other table in SQL. Here's an example: SELECT * FROM sales_report WHERE year = 2022 ; This query uses the \"sales_report\" view to select all records where the year is equal to 2022. The view simplifies the query by hiding the complexity of the underlying data. Updating a View A view can be updated using the ALTER VIEW statement. Here's an example: ALTER VIEW sales_report AS SELECT year , month , sum ( revenue ) as total_revenue FROM sales WHERE year > 2020 GROUP BY year , month ; This updates the \"sales_report\" view to only include records where the year is greater than 2020. Dropping a View A view can be dropped using the DROP VIEW statement. Here's an example: DROP VIEW sales_report ; This deletes the \"sales_report\" view from the database. Importance of Views in Data Analysis Job Views are important in day-to-day data analysis because they simplify complex queries and provide a way to control access to sensitive data. Views allow users to focus on the data that is important to them, without having to understand the underlying database schema or query language. Views can also be used to filter data, hide sensitive information, and provide access to a subset of data in the database. In summary, views in SQL are a powerful tool for simplifying complex queries and providing users with access to a subset of data in the database. They are an important part of day-to-day data analysis and can help improve productivity, accuracy, and security. VIEW with SQLAlchemy Let's create a python script call view_0.py with a view that shows the number of employees hired each year and just print the result. from sqlalchemy import create_engine , MetaData , Table , Column , Integer , String , Date , select , text # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the tables to reflect the database schema employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # create a view that shows the number of employees hired each year query = text ( \"\"\" CREATE VIEW employee_hires AS SELECT YEAR(hire_date) AS year, COUNT(*) AS hires FROM employees GROUP BY year; \"\"\" ) # execute the view creation query with engine . connect () as conn : conn . execute ( query ) # define a select statement to query the view employee_hires_view = select ([ text ( \"year, hires\" )]) . select_from ( text ( \"employee_hires\" )) # execute the select statement and print the results with engine . connect () as conn : result = conn . execute ( employee_hires_view ) for row in result : print ( row ) In this example, we create a view called employee_hires that shows the number of employees hired each year. We use the text() function from SQLAlchemy to define the SQL query for creating the view. Then, we execute the query using the engine.connect() method. Next, we define a select statement that queries the employee_hires view using the select_from() method and the text() function to specify the table name. Finally, we execute the select statement and print the results using the engine.connect() method. Note that the select_from() method is used to specify the table or view to select data from. In this example, we use the text() function to specify the name of the employee_hires view . This is similar to specifying a table name using the Table() function, but with the added benefit of being able to specify more complex SQL queries using the text() function. Let's take a look at an other example with VIEW for select the 10 most paid employees in development department : from sqlalchemy import create_engine , MetaData , Table , Column , Integer , String , Date , select , text , func # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the tables to reflect the database schema employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) # drop the existing view if it exists with engine . connect () as conn : conn . execute ( text ( \"DROP VIEW IF EXISTS employee_hires\" )) # (1) # create a view that shows the number of employees hired each year query = text ( \"\"\" CREATE VIEW employee_hires AS SELECT YEAR(hire_date) AS year, COUNT(*) AS hires FROM employees GROUP BY year; \"\"\" ) # execute the view creation query with engine . connect () as conn : conn . execute ( query ) # define the subquery to calculate average salary of development department employees subquery = select ([ func . avg ( salaries . c . salary )]) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ correlate ( employees ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # define the query to select the 10 most paid employees in development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( salaries . c . salary > subquery ) . \\ order_by ( salaries . c . salary . desc ()) . \\ limit ( 10 ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # execute the query and print the results with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row ) \ud83d\udd0e You may have an error message indicates that the view employee_hires already exists in the database so we deleted it before re write it. You can either drop the existing view before creating it again, or modify the create statement to use CREATE OR REPLACE VIEW which will create the view if it does not exist, or replace the existing view if it does.","title":"Advanced Queries with Python"},{"location":"sql/26_python_queries/#advanced-queries-with-python","text":"SQLAlchemy provides a wide range of advanced querying techniques that allow you to perform complex database queries using Python code. In this tutorial, we will cover how to use joins, subqueries, and other advanced SQL features with SQLAlchemy.","title":"Advanced Queries with Python"},{"location":"sql/26_python_queries/#joins","text":"Joins are used to combine data from two or more tables in a single query like we have seen in the SQL section. SQLAlchemy provides several ways to perform joins, including the join() , outerjoin() , and select_from() methods.","title":"Joins"},{"location":"sql/26_python_queries/#inner-join","text":"An inner join returns only the rows that have matching values in both tables being joined. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData #connect database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () #Map employees tables employees and titles employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) titles = Table ( 'titles' , metadata , autoload = True , autoload_with = engine ) #perform query query = employees . join ( titles , employees . c . emp_no == titles . c . emp_no ) result = engine . execute ( query . select ()) #print result for row in result : print ( row ) In the above example, we used the join() method to join the employees and titles tables on the emp_no column. The resulting query returns only the rows where there is a matching emp_no in both tables.","title":"Inner Join"},{"location":"sql/26_python_queries/#left-join","text":"A left join returns all the rows from the left table and the matched rows from the right table. If there is no match in the right table, the result will contain NULL values. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) query = employees . join ( salaries , employees . c . emp_no == salaries . c . emp_no , isouter = True ) result = engine . execute ( query . select ()) for row in result : print ( row ) In the above example, we used the join() method to perform a left join between the employees and salaries tables on the emp_no column. The isouter=True argument specifies that we want to perform a left join . If there is no matching emp_no in the salaries table, the result will contain NULL values.","title":"Left Join"},{"location":"sql/26_python_queries/#right-join","text":"A right join returns all the rows from the right table and the matched rows from the left table. If there is no match in the left table, the result will contain NULL values. Here's an example: from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) query = salaries . join ( employees , salaries . c . emp_no == employees . c . emp_no , isouter = True ) result = engine . execute ( query . select ()) for row in result : print ( row ) In the above example, we used the join() method to perform a right join between the salaries and employees tables on the emp_no column. The iso uter=True argument specifies that we want to perform a right join.","title":"Right Join"},{"location":"sql/26_python_queries/#using-multiple-join-conditions-with-select_from","text":"Let's try to print the 10 employees with the highest salary in the Development department : from sqlalchemy import create_engine , Table , Column , Integer , String , MetaData , ForeignKey , desc , select , func # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # Define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) departments = Table ( 'departments' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) # Define the query to select the 10 employees with the highest salary in the Development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( employees . c . emp_no == salaries . c . emp_no ) . \\ where ( dept_emp . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == departments . c . dept_no ) . \\ where ( departments . c . dept_name == 'Development' ) . \\ order_by ( desc ( salaries . c . salary )) . \\ limit ( 10 ) . \\ select_from ( employees . join ( salaries ) . join ( dept_emp ) . join ( departments )) # Execute the query result = engine . execute ( query ) # Print the results for row in result : print ( row ) The query selects the columns emp_no , first_name , last_name , and salary from the employees and salaries tables, respectively. It then joins the dept_emp and departments tables to get only the employees in the Development department, and sorts the result in descending order by salary, and limits the result to the top 10 highest paid employees . Finally, it uses the select_from() method to specify the join conditions between the tables. The execute method is called on the engine object with the query object as an argument, and the results are printed in a loop as usual.","title":"Using multiple join conditions with select_from()"},{"location":"sql/26_python_queries/#sub-queries-and-correlate-function","text":"In SQLAlchemy, the correlate() function is used to control correlated subqueries, which are subqueries that reference the outer query. A correlated subquery allows you to filter or aggregate data in a subquery based on the results of the outer query. The correlate() function is used to specify which tables in the subquery should be correlated to the outer query. It takes one or more tables as arguments and returns a new Select object that is a correlated subquery. Here's the same example as before modify for using correlate() function : from sqlalchemy import select , func from sqlalchemy.orm import correlate # define the tables employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) # Define the subquery to calculate average salary of development department employees subquery = select ([ func . avg ( salaries . c . salary )]) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ correlate ( employees ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # Define the query to select the 10 most paid employees in development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( salaries . c . salary > subquery ) . \\ order_by ( desc ( salaries . c . salary )) . \\ limit ( 10 ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # execute the query and print the results result = engine . execute ( query ) for row in result : print ( row ) This query is similar to the previous one we discussed, which also selects the 10 most highly paid employees in the development department. However, in this query, we use the correlate() function to manage auto-correlation between tables. The subquery is used to calculate the average salary of employees in the development department. We use the func.avg() function to calculate the average salary and apply two where clauses to filter the salaries of employees in the development department. To handle auto-correlation between tables, we use the correlate() function and pass the employees table as the argument. Finally, we join the dept_emp , employees , and salaries tables together using the join() method. The main query selects the emp_no , first_name , last_name , and salary fields from the employees and salaries tables. We again use the where clause to filter the employees in the development department and use the correlate() function to handle auto-correlation. We also add a third where clause to compare the salaries of employees in the development department with the average salary calculated by the subquery. Finally, we join the dept_emp , employees , and salaries tables together using the join() method. We sort the results in descending order of salaries and limit the output to the top 10 results.","title":"Sub Queries and correlate() function"},{"location":"sql/26_python_queries/#joins-and-correlate","text":"The correlate() function is different from joins in that it allows you to filter or aggregate data in a subquery based on the results of the outer query, without actually joining the tables together. \ud83d\udd0e This can be more efficient than using joins in certain situations, especially when dealing with large datasets. However, correlated subqueries can also be slower than joins , especially if the subquery is complex or returns a large amount of data. It's important to test and optimize your queries to ensure that they perform well for your specific use case.","title":"Joins and correlate()"},{"location":"sql/26_python_queries/#difference-between-the-select_from-and-the-joins-methods","text":"The select_from() method and the join() method are both used in SQLAlchemy to specify the tables used in a SQL query, but they differ in how they are used and the types of queries they can generate. The join() method is used to specify a join between two or more tables. It generates an INNER JOIN by default, but can be used to generate other types of joins as well. This method allows you to specify the join condition, i.e. the columns used to match rows between the tables. The join() method is useful when you need to combine data from multiple tables into a single query result. The select_from() method, on the other hand, is used to specify the main table(s) used in a SQL query. It is used to specify the primary table or tables that the query will be based on. This method is useful when you need to specify a complex subquery or a nested query, or when you need to select from a view or other non-standard source of data. In general, the join() method is used to combine data from multiple tables, while the select_from() method is used to specify the primary table or tables used in the query. However, both methods can be used together to generate more complex queries that combine data from multiple tables and use subqueries or nested queries to filter or manipulate the data.","title":"Difference between the select_from() and the joins methods"},{"location":"sql/26_python_queries/#working-with-view","text":"Let's dig a little aroud SQL view concept.","title":"Working with VIEW"},{"location":"sql/26_python_queries/#what-is-a-view-in-sql","text":"A view in SQL is a virtual table that is created based on a query. It is a named query that is saved in the database and can be used like a table in other queries. A view can be used to simplify complex queries, filter data, or provide users with access to a subset of data in the database without having to grant access to the underlying tables.","title":"What is a View in SQL?"},{"location":"sql/26_python_queries/#creating-a-view","text":"A view is created using the CREATE VIEW statement, which defines the name of the view, the columns to include, and the SELECT statement that defines the query used to create the view. Here's an example: CREATE VIEW sales_report AS SELECT year , month , sum ( revenue ) as total_revenue FROM sales GROUP BY year , month ; This creates a view called \"sales_report\" that contains three columns: year, month, and total_revenue. The view is based on the sales table, and calculates the total revenue for each month and year.","title":"Creating a View"},{"location":"sql/26_python_queries/#using-a-view","text":"Once a view is created, it can be used like any other table in SQL. Here's an example: SELECT * FROM sales_report WHERE year = 2022 ; This query uses the \"sales_report\" view to select all records where the year is equal to 2022. The view simplifies the query by hiding the complexity of the underlying data.","title":"Using a View"},{"location":"sql/26_python_queries/#updating-a-view","text":"A view can be updated using the ALTER VIEW statement. Here's an example: ALTER VIEW sales_report AS SELECT year , month , sum ( revenue ) as total_revenue FROM sales WHERE year > 2020 GROUP BY year , month ; This updates the \"sales_report\" view to only include records where the year is greater than 2020.","title":"Updating a View"},{"location":"sql/26_python_queries/#dropping-a-view","text":"A view can be dropped using the DROP VIEW statement. Here's an example: DROP VIEW sales_report ; This deletes the \"sales_report\" view from the database.","title":"Dropping a View"},{"location":"sql/26_python_queries/#importance-of-views-in-data-analysis-job","text":"Views are important in day-to-day data analysis because they simplify complex queries and provide a way to control access to sensitive data. Views allow users to focus on the data that is important to them, without having to understand the underlying database schema or query language. Views can also be used to filter data, hide sensitive information, and provide access to a subset of data in the database. In summary, views in SQL are a powerful tool for simplifying complex queries and providing users with access to a subset of data in the database. They are an important part of day-to-day data analysis and can help improve productivity, accuracy, and security.","title":"Importance of Views in Data Analysis Job"},{"location":"sql/26_python_queries/#view-with-sqlalchemy","text":"Let's create a python script call view_0.py with a view that shows the number of employees hired each year and just print the result. from sqlalchemy import create_engine , MetaData , Table , Column , Integer , String , Date , select , text # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the tables to reflect the database schema employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) # create a view that shows the number of employees hired each year query = text ( \"\"\" CREATE VIEW employee_hires AS SELECT YEAR(hire_date) AS year, COUNT(*) AS hires FROM employees GROUP BY year; \"\"\" ) # execute the view creation query with engine . connect () as conn : conn . execute ( query ) # define a select statement to query the view employee_hires_view = select ([ text ( \"year, hires\" )]) . select_from ( text ( \"employee_hires\" )) # execute the select statement and print the results with engine . connect () as conn : result = conn . execute ( employee_hires_view ) for row in result : print ( row ) In this example, we create a view called employee_hires that shows the number of employees hired each year. We use the text() function from SQLAlchemy to define the SQL query for creating the view. Then, we execute the query using the engine.connect() method. Next, we define a select statement that queries the employee_hires view using the select_from() method and the text() function to specify the table name. Finally, we execute the select statement and print the results using the engine.connect() method. Note that the select_from() method is used to specify the table or view to select data from. In this example, we use the text() function to specify the name of the employee_hires view . This is similar to specifying a table name using the Table() function, but with the added benefit of being able to specify more complex SQL queries using the text() function. Let's take a look at an other example with VIEW for select the 10 most paid employees in development department : from sqlalchemy import create_engine , MetaData , Table , Column , Integer , String , Date , select , text , func # create an engine to connect to the database engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) # create a metadata object to reflect the database schema metadata = MetaData () # define the tables to reflect the database schema employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) salaries = Table ( 'salaries' , metadata , autoload = True , autoload_with = engine ) dept_emp = Table ( 'dept_emp' , metadata , autoload = True , autoload_with = engine ) # drop the existing view if it exists with engine . connect () as conn : conn . execute ( text ( \"DROP VIEW IF EXISTS employee_hires\" )) # (1) # create a view that shows the number of employees hired each year query = text ( \"\"\" CREATE VIEW employee_hires AS SELECT YEAR(hire_date) AS year, COUNT(*) AS hires FROM employees GROUP BY year; \"\"\" ) # execute the view creation query with engine . connect () as conn : conn . execute ( query ) # define the subquery to calculate average salary of development department employees subquery = select ([ func . avg ( salaries . c . salary )]) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ correlate ( employees ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # define the query to select the 10 most paid employees in development department query = select ([ employees . c . emp_no , employees . c . first_name , employees . c . last_name , salaries . c . salary ]) . \\ where ( dept_emp . c . dept_no == 'd001' ) . \\ where ( salaries . c . emp_no == employees . c . emp_no ) . \\ where ( salaries . c . salary > subquery ) . \\ order_by ( salaries . c . salary . desc ()) . \\ limit ( 10 ) . \\ select_from ( dept_emp . join ( employees , employees . c . emp_no == dept_emp . c . emp_no ) . \\ join ( salaries , salaries . c . emp_no == employees . c . emp_no )) # execute the query and print the results with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row ) \ud83d\udd0e You may have an error message indicates that the view employee_hires already exists in the database so we deleted it before re write it. You can either drop the existing view before creating it again, or modify the create statement to use CREATE OR REPLACE VIEW which will create the view if it does not exist, or replace the existing view if it does.","title":"VIEW with SQLAlchemy"},{"location":"sql/27_python_large_data/","text":"Working with Large Datasets in Python Working with large datasets can be challenging, especially if you're working with a database that contains millions of records. This is usually the case with companies. In this tutorial, we'll explore some techniques for managing and processing large amounts of data using Python and SQLAlchemy. 1. Limiting Results Like we have seen and used before, one of the simplest ways to work with large datasets is to limit the number of results returned by a query. This is especially useful when you're working with a table that contains millions of records and you only need a small subset of those records for your analysis. In SQLAlchemy, you can limit the number of results returned by a query using the limit method. For example, to select the first 100 records from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) . limit ( 100 ) with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row ) 2. Using Pagination Another technique for working with large datasets is pagination. Pagination involves breaking up the results of a query into smaller chunks, or pages, and fetching each page separately. This can help to reduce memory usage and improve performance when working with large datasets. In SQLAlchemy, you can use the limit and offset methods to implement pagination. The limit method limits the number of records returned by the query, while the offset method specifies the starting point for the query. For example, to fetch records 101-200 from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) . limit ( 100 ) . offset ( 100 ) with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row ) 3. Chunking Data Sometimes, even pagination isn't enough to handle really large datasets. In these cases, you can chunk your data into smaller pieces and process each chunk separately. This can help to reduce memory usage and improve performance. In SQLAlchemy, you can use the fetchmany method to fetch a specified number of rows at a time. For example, to fetch 1000 rows at a time from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) with engine . connect () as conn : chunk_size = 1000 offset = 0 while True : chunk = conn . execute ( query . limit ( chunk_size ) . offset ( offset )) results = chunk . fetchall () if not results : break for row in results : print ( row ) offset += chunk_size This code fetches 1000 rows at a time from the employees table and processes each chunk separately. 4. Streaming Data When working with large datasets, it may be useful to stream the data from the database rather than loading the entire dataset into memory. This can be done using the yield_per() method in SQLAlchemy. The yield_per() method will fetch a certain number of rows at a time, allowing you to process the data in smaller chunks. Here is an example of how to use yield_per() to stream data from the employees table in the MySQL employees database: from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # stream the data from the employees table in chunks of 1000 rows query = session . query ( employees ) . yield_per ( 1000 ) # process the data for employee in query : # do something with the employee data print ( employee . emp_no , employee . first_name , employee . last_name ) 5. Batch Processing When updating or inserting large amounts of data, it can be more efficient to do it in batches rather than one row at a time. This can be done using the add_all() method in SQLAlchemy. Here is an example of how to use add_all() to insert a batch of employees into the employees table: from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # create a list of employees to insert employees_list = [ { 'emp_no' : 10001 , 'first_name' : 'John' , 'last_name' : 'Doe' }, { 'emp_no' : 10002 , 'first_name' : 'Jane' , 'last_name' : 'Smith' }, # more employees... ] # insert the employees in batches of 1000 batch_size = 1000 for i in range ( 0 , len ( employees_list ), batch_size ): batch = employees_list [ i : i + batch_size ] session . add_all ([ employees ( ** e ) for e in batch ]) session . commit () 6. Parallel Processing Another technique for processing large datasets is to use parallel processing. This can be done using the concurrent.futures module in Python. Here is an example of how to use the ThreadPoolExecutor class to execute a function in parallel on a large dataset: from concurrent.futures import ThreadPoolExecutor from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # define a function to process an employee record def process_employee ( employee ): # do something with the employee data print ( employee . emp_no , employee . first_name , employee . last_name ) # query the employees table query = session . query ( employees ) # create a ThreadPoolExecutor with 4 threads with ThreadPoolExecutor ( max_workers = 4 ) as executor : # submit the process_employee function for each row in the query futures = [ executor . submit ( process_employee , row ) for row in query ] # wait for all the functions to complete for future in futures : future . result () These are just a few techniques for managing and processing large amounts of data in Python and SQLAlchemy. Depending on your specific use case, other techniques such as indexing, partitioning, and caching may also be useful.","title":"Working with large dataset"},{"location":"sql/27_python_large_data/#working-with-large-datasets-in-python","text":"Working with large datasets can be challenging, especially if you're working with a database that contains millions of records. This is usually the case with companies. In this tutorial, we'll explore some techniques for managing and processing large amounts of data using Python and SQLAlchemy.","title":"Working with Large Datasets in Python"},{"location":"sql/27_python_large_data/#1-limiting-results","text":"Like we have seen and used before, one of the simplest ways to work with large datasets is to limit the number of results returned by a query. This is especially useful when you're working with a table that contains millions of records and you only need a small subset of those records for your analysis. In SQLAlchemy, you can limit the number of results returned by a query using the limit method. For example, to select the first 100 records from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) . limit ( 100 ) with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row )","title":"1. Limiting Results"},{"location":"sql/27_python_large_data/#2-using-pagination","text":"Another technique for working with large datasets is pagination. Pagination involves breaking up the results of a query into smaller chunks, or pages, and fetching each page separately. This can help to reduce memory usage and improve performance when working with large datasets. In SQLAlchemy, you can use the limit and offset methods to implement pagination. The limit method limits the number of records returned by the query, while the offset method specifies the starting point for the query. For example, to fetch records 101-200 from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) . limit ( 100 ) . offset ( 100 ) with engine . connect () as conn : result = conn . execute ( query ) for row in result : print ( row )","title":"2. Using Pagination"},{"location":"sql/27_python_large_data/#3-chunking-data","text":"Sometimes, even pagination isn't enough to handle really large datasets. In these cases, you can chunk your data into smaller pieces and process each chunk separately. This can help to reduce memory usage and improve performance. In SQLAlchemy, you can use the fetchmany method to fetch a specified number of rows at a time. For example, to fetch 1000 rows at a time from the employees table, you can use the following code: from sqlalchemy import create_engine , MetaData , Table , select engine = create_engine ( 'mysql://user:password@localhost:3306/employees' ) metadata = MetaData () employees = Table ( 'employees' , metadata , autoload = True , autoload_with = engine ) query = select ([ employees ]) with engine . connect () as conn : chunk_size = 1000 offset = 0 while True : chunk = conn . execute ( query . limit ( chunk_size ) . offset ( offset )) results = chunk . fetchall () if not results : break for row in results : print ( row ) offset += chunk_size This code fetches 1000 rows at a time from the employees table and processes each chunk separately.","title":"3. Chunking Data"},{"location":"sql/27_python_large_data/#4-streaming-data","text":"When working with large datasets, it may be useful to stream the data from the database rather than loading the entire dataset into memory. This can be done using the yield_per() method in SQLAlchemy. The yield_per() method will fetch a certain number of rows at a time, allowing you to process the data in smaller chunks. Here is an example of how to use yield_per() to stream data from the employees table in the MySQL employees database: from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # stream the data from the employees table in chunks of 1000 rows query = session . query ( employees ) . yield_per ( 1000 ) # process the data for employee in query : # do something with the employee data print ( employee . emp_no , employee . first_name , employee . last_name )","title":"4. Streaming Data"},{"location":"sql/27_python_large_data/#5-batch-processing","text":"When updating or inserting large amounts of data, it can be more efficient to do it in batches rather than one row at a time. This can be done using the add_all() method in SQLAlchemy. Here is an example of how to use add_all() to insert a batch of employees into the employees table: from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # create a list of employees to insert employees_list = [ { 'emp_no' : 10001 , 'first_name' : 'John' , 'last_name' : 'Doe' }, { 'emp_no' : 10002 , 'first_name' : 'Jane' , 'last_name' : 'Smith' }, # more employees... ] # insert the employees in batches of 1000 batch_size = 1000 for i in range ( 0 , len ( employees_list ), batch_size ): batch = employees_list [ i : i + batch_size ] session . add_all ([ employees ( ** e ) for e in batch ]) session . commit ()","title":"5. Batch Processing"},{"location":"sql/27_python_large_data/#6-parallel-processing","text":"Another technique for processing large datasets is to use parallel processing. This can be done using the concurrent.futures module in Python. Here is an example of how to use the ThreadPoolExecutor class to execute a function in parallel on a large dataset: from concurrent.futures import ThreadPoolExecutor from sqlalchemy.orm import sessionmaker # create a session to work with the database Session = sessionmaker ( bind = engine ) session = Session () # define a function to process an employee record def process_employee ( employee ): # do something with the employee data print ( employee . emp_no , employee . first_name , employee . last_name ) # query the employees table query = session . query ( employees ) # create a ThreadPoolExecutor with 4 threads with ThreadPoolExecutor ( max_workers = 4 ) as executor : # submit the process_employee function for each row in the query futures = [ executor . submit ( process_employee , row ) for row in query ] # wait for all the functions to complete for future in futures : future . result () These are just a few techniques for managing and processing large amounts of data in Python and SQLAlchemy. Depending on your specific use case, other techniques such as indexing, partitioning, and caching may also be useful.","title":"6. Parallel Processing"},{"location":"sql/28_python_fastapi/","text":"SQLAlchemy and FastAPI Introduction FastAPI is a popular Python web framework that allows for fast and efficient development of APIs. It includes a built-in support for SQLAlchemy, a powerful SQL toolkit and ORM. SQLAlchemy provides a way to interact with databases in a more pythonic way than using raw SQL, and FastAPI's integration with SQLAlchemy makes it easy to build high-performance web applications with a database backend. In this tutorial, we will use FastAPI and SQLAlchemy to build a RESTful API that interacts with a MySQL database containing employee data. Why building an API An API provides a structured and standardized way for applications to communicate with each other. By creating an API for your database, you allow other applications to access the data in a controlled manner. This can provide a number of benefits: Security : By creating an API, you can implement security measures to protect your database from unauthorized access. You can limit access to certain endpoints and require authentication to access the data. Scalability : An API allows you to separate the front-end and back-end of your application, which makes it easier to scale each component independently. You can also use caching and other techniques to improve performance. Flexibility : By exposing your database through an API, you can allow other applications to access and use the data in a variety of ways. This can lead to new and innovative use cases that you may not have considered. Standardization : By using a standardized API format, such as REST, you make it easier for other developers to understand and use your API. This can help to encourage collaboration and adoption. In summary, creating an API provides a number of benefits for accessing and managing data. It can help to improve security, scalability, flexibility, and standardization. Prerequisites Before we begin, make sure you have the following installed: Python 3.6 or later Pip MySQL database MySQL Python connector To install FastAPI and SQLAlchemy, run the following command: pip install fastapi sqlalchemy Setting up the Database in FastAPI For this tutorial, we will be using the MySQL employees database as usual. Once you have the database set up, create a new file main.py and add the following code: from fastapi import FastAPI , HTTPException from sqlalchemy.orm import Session from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base # create the FastAPI instance app = FastAPI () # create the database engine engine = create_engine ( \"mysql://root:root@localhost:3306/employees\" ) # reflect the database schema to an ORM base Base = automap_base () Base . prepare ( engine , reflect = True ) # map the tables to ORM classes Employees = Base . classes . employees Departments = Base . classes . departments Dept_Emp = Base . classes . dept_emp Salaries = Base . classes . salaries # create a function to get a database session def get_db (): try : db = Session ( bind = engine ) yield db finally : db . close () The new thing in the code is a function called get_db() which is used to create a database session to interact with the database. The function first tries to create a session using Session(bind=engine) and binds the session to the database engine object engine. The yield keyword makes the function a generator function which can be iterated over to yield a database session. When the session is no longer needed, the finally block of the function is executed which closes the database session using db.close() . This ensures that the session is closed properly and any resources associated with the session are released back to the system. Using this function, you can get a database session by calling get_db() , which returns a generator object. You can then iterate over the generator to get a database session, like so: with get_db () as db : # interact with the database session here Addind Endpoints to our app In FastAPI, an endpoint or route is a combination of a specific HTTP method (such as GET, POST, PUT, DELETE, etc.) and a URL path that a client can use to communicate with the server and perform some specific action or operation. Endpoints are defined in your FastAPI application using decorated functions that are called view functions. These view functions handle the request/response cycle for a specific endpoint, which includes handling any incoming requests with specific parameters and returning the appropriate response data. In other words, endpoints are the primary building blocks of a FastAPI application, and they determine how clients can interact with your API. They enable you to define a clear and well-structured API that can be easily understood and used by other developers, while also providing a standardized way for clients to communicate with your application. Endpoint /employees This route is used to retrieve a list of employees from the database. It returns a JSON object containing the employee's first name, last name, gender, and hire date. It also accepts query parameters for filtering the results by gender and hire date. Endpoint /employees/{emp_no} This route is used to retrieve information about a specific employee by their employee number. It returns a JSON object containing the employee's first name , last name , gender , and hire date . Let's code this in python : # define a route to get the employee data by ID @app . get ( \"/employees/ {emp_no} \" ) async def get_employee ( emp_no : int , db : Session = Depends ( get_db )): employee = db . query ( Employees ) . filter ( Employees . emp_no == emp_no ) . first () if not employee : raise HTTPException ( status_code = 404 , detail = \"Employee not found\" ) return employee Endpoint /departments This route is used to retrieve a list of departments from the database. It returns a JSON object containing the department's name and number. Endpoint /departments/{dept_no} This route is used to retrieve information about a specific department by its department number. It returns a JSON object containing the department's name and number. Let's implement this : # define a route to get the department data by ID @app . get ( \"/departments/ {dept_no} \" ) async def get_department ( dept_no : str , db : Session = Depends ( get_db )): department = db . query ( Departments ) . filter ( Departments . dept_no == dept_no ) . first () if not department : raise HTTPException ( status_code = 404 , detail = \"Department not found\" ) return department Endpoint /employees/{emp_no}/salaries This route is used to retrieve a list of salaries for a specific employee by their employee number. It returns a JSON object containing the salary amount, start and end dates of the salary period. # define a route to get the salary history of an employee by ID @app . get ( \"/employees/ {emp_no} /salaries\" ) async def get_employee_salaries ( emp_no : int , db : Session = Depends ( get_db )): salaries = db . query ( Salaries ) . filter ( Salaries . emp_no == emp_no ) . all () if not salaries : raise HTTPException ( status_code = 404 , detail = \"No salary history found for employee\" ) return salaries Writing our API Let's wrap up and add a final endpoint /departments/{dept_no}/employees : main.py from fastapi import FastAPI , HTTPException from sqlalchemy.orm import Session from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base # create the FastAPI instance app = FastAPI () # create the database engine engine = create_engine ( \"mysql://root:root@localhost:3306/employees\" ) # reflect the database schema to an ORM base Base = automap_base () Base . prepare ( engine , reflect = True ) # map the tables to ORM classes Employees = Base . classes . employees Departments = Base . classes . departments Dept_Emp = Base . classes . dept_emp Salaries = Base . classes . salaries # create a function to get a database session def get_db (): try : db = Session ( bind = engine ) yield db finally : db . close () # define a route to get the employee data by ID @app . get ( \"/employees/ {emp_no} \" ) async def get_employee ( emp_no : int , db : Session = Depends ( get_db )): employee = db . query ( Employees ) . filter ( Employees . emp_no == emp_no ) . first () if not employee : raise HTTPException ( status_code = 404 , detail = \"Employee not found\" ) return employee # define a route to get the department data by ID @app . get ( \"/departments/ {dept_no} \" ) async def get_department ( dept_no : str , db : Session = Depends ( get_db )): department = db . query ( Departments ) . filter ( Departments . dept_no == dept_no ) . first () if not department : raise HTTPException ( status_code = 404 , detail = \"Department not found\" ) return department # define a route to get the list of employees in a department by ID @app . get ( \"/departments/ {dept_no} /employees\" ) async def get_department_employees ( dept_no : str , db : Session = Depends ( get_db )): employees = db . query ( Employees ) . \\ join ( Dept_Emp , Employees . emp_no == Dept_Emp . emp_no ) . \\ filter ( Dept_Emp . dept_no == dept_no ) . all () if not employees : raise HTTPException ( status_code = 404 , detail = \"No employees found in department\" ) return employees # define a route to get the salary history of an employee by ID @app . get ( \"/employees/ {emp_no} /salaries\" ) async def get_employee_salaries ( emp_no : int , db : Session = Depends ( get_db )): salaries = db . query ( Salaries ) . filter ( Salaries . emp_no == emp_no ) . all () if not salaries : raise HTTPException ( status_code = 404 , detail = \"No salary history found for employee\" ) return salaries To run this script, you can save it as a Python file (e.g. main.py) and then run the following command in your terminal: uvicorn main:app --reload This will start a local web server at http://localhost:8000/docs where you can access the different routes defined in the script \ud83e\udd13 Go on the first endpoint employees/{emp_no} and click on try it out and take an example employee ID (let's say 10001) you should see this in your browser :","title":"SQLAlchemy and FastAPI"},{"location":"sql/28_python_fastapi/#sqlalchemy-and-fastapi","text":"","title":"SQLAlchemy and FastAPI"},{"location":"sql/28_python_fastapi/#introduction","text":"FastAPI is a popular Python web framework that allows for fast and efficient development of APIs. It includes a built-in support for SQLAlchemy, a powerful SQL toolkit and ORM. SQLAlchemy provides a way to interact with databases in a more pythonic way than using raw SQL, and FastAPI's integration with SQLAlchemy makes it easy to build high-performance web applications with a database backend. In this tutorial, we will use FastAPI and SQLAlchemy to build a RESTful API that interacts with a MySQL database containing employee data.","title":"Introduction"},{"location":"sql/28_python_fastapi/#why-building-an-api","text":"An API provides a structured and standardized way for applications to communicate with each other. By creating an API for your database, you allow other applications to access the data in a controlled manner. This can provide a number of benefits: Security : By creating an API, you can implement security measures to protect your database from unauthorized access. You can limit access to certain endpoints and require authentication to access the data. Scalability : An API allows you to separate the front-end and back-end of your application, which makes it easier to scale each component independently. You can also use caching and other techniques to improve performance. Flexibility : By exposing your database through an API, you can allow other applications to access and use the data in a variety of ways. This can lead to new and innovative use cases that you may not have considered. Standardization : By using a standardized API format, such as REST, you make it easier for other developers to understand and use your API. This can help to encourage collaboration and adoption. In summary, creating an API provides a number of benefits for accessing and managing data. It can help to improve security, scalability, flexibility, and standardization.","title":"Why building an API"},{"location":"sql/28_python_fastapi/#prerequisites","text":"Before we begin, make sure you have the following installed: Python 3.6 or later Pip MySQL database MySQL Python connector To install FastAPI and SQLAlchemy, run the following command: pip install fastapi sqlalchemy","title":"Prerequisites"},{"location":"sql/28_python_fastapi/#setting-up-the-database-in-fastapi","text":"For this tutorial, we will be using the MySQL employees database as usual. Once you have the database set up, create a new file main.py and add the following code: from fastapi import FastAPI , HTTPException from sqlalchemy.orm import Session from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base # create the FastAPI instance app = FastAPI () # create the database engine engine = create_engine ( \"mysql://root:root@localhost:3306/employees\" ) # reflect the database schema to an ORM base Base = automap_base () Base . prepare ( engine , reflect = True ) # map the tables to ORM classes Employees = Base . classes . employees Departments = Base . classes . departments Dept_Emp = Base . classes . dept_emp Salaries = Base . classes . salaries # create a function to get a database session def get_db (): try : db = Session ( bind = engine ) yield db finally : db . close () The new thing in the code is a function called get_db() which is used to create a database session to interact with the database. The function first tries to create a session using Session(bind=engine) and binds the session to the database engine object engine. The yield keyword makes the function a generator function which can be iterated over to yield a database session. When the session is no longer needed, the finally block of the function is executed which closes the database session using db.close() . This ensures that the session is closed properly and any resources associated with the session are released back to the system. Using this function, you can get a database session by calling get_db() , which returns a generator object. You can then iterate over the generator to get a database session, like so: with get_db () as db : # interact with the database session here","title":"Setting up the Database in FastAPI"},{"location":"sql/28_python_fastapi/#addind-endpoints-to-our-app","text":"In FastAPI, an endpoint or route is a combination of a specific HTTP method (such as GET, POST, PUT, DELETE, etc.) and a URL path that a client can use to communicate with the server and perform some specific action or operation. Endpoints are defined in your FastAPI application using decorated functions that are called view functions. These view functions handle the request/response cycle for a specific endpoint, which includes handling any incoming requests with specific parameters and returning the appropriate response data. In other words, endpoints are the primary building blocks of a FastAPI application, and they determine how clients can interact with your API. They enable you to define a clear and well-structured API that can be easily understood and used by other developers, while also providing a standardized way for clients to communicate with your application.","title":"Addind Endpoints to our app"},{"location":"sql/28_python_fastapi/#endpoint-employees","text":"This route is used to retrieve a list of employees from the database. It returns a JSON object containing the employee's first name, last name, gender, and hire date. It also accepts query parameters for filtering the results by gender and hire date.","title":"Endpoint /employees"},{"location":"sql/28_python_fastapi/#endpoint-employeesemp_no","text":"This route is used to retrieve information about a specific employee by their employee number. It returns a JSON object containing the employee's first name , last name , gender , and hire date . Let's code this in python : # define a route to get the employee data by ID @app . get ( \"/employees/ {emp_no} \" ) async def get_employee ( emp_no : int , db : Session = Depends ( get_db )): employee = db . query ( Employees ) . filter ( Employees . emp_no == emp_no ) . first () if not employee : raise HTTPException ( status_code = 404 , detail = \"Employee not found\" ) return employee","title":"Endpoint /employees/{emp_no}"},{"location":"sql/28_python_fastapi/#endpoint-departments","text":"This route is used to retrieve a list of departments from the database. It returns a JSON object containing the department's name and number.","title":"Endpoint /departments"},{"location":"sql/28_python_fastapi/#endpoint-departmentsdept_no","text":"This route is used to retrieve information about a specific department by its department number. It returns a JSON object containing the department's name and number. Let's implement this : # define a route to get the department data by ID @app . get ( \"/departments/ {dept_no} \" ) async def get_department ( dept_no : str , db : Session = Depends ( get_db )): department = db . query ( Departments ) . filter ( Departments . dept_no == dept_no ) . first () if not department : raise HTTPException ( status_code = 404 , detail = \"Department not found\" ) return department","title":"Endpoint  /departments/{dept_no}"},{"location":"sql/28_python_fastapi/#endpoint-employeesemp_nosalaries","text":"This route is used to retrieve a list of salaries for a specific employee by their employee number. It returns a JSON object containing the salary amount, start and end dates of the salary period. # define a route to get the salary history of an employee by ID @app . get ( \"/employees/ {emp_no} /salaries\" ) async def get_employee_salaries ( emp_no : int , db : Session = Depends ( get_db )): salaries = db . query ( Salaries ) . filter ( Salaries . emp_no == emp_no ) . all () if not salaries : raise HTTPException ( status_code = 404 , detail = \"No salary history found for employee\" ) return salaries","title":"Endpoint /employees/{emp_no}/salaries"},{"location":"sql/28_python_fastapi/#writing-our-api","text":"Let's wrap up and add a final endpoint /departments/{dept_no}/employees : main.py from fastapi import FastAPI , HTTPException from sqlalchemy.orm import Session from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base # create the FastAPI instance app = FastAPI () # create the database engine engine = create_engine ( \"mysql://root:root@localhost:3306/employees\" ) # reflect the database schema to an ORM base Base = automap_base () Base . prepare ( engine , reflect = True ) # map the tables to ORM classes Employees = Base . classes . employees Departments = Base . classes . departments Dept_Emp = Base . classes . dept_emp Salaries = Base . classes . salaries # create a function to get a database session def get_db (): try : db = Session ( bind = engine ) yield db finally : db . close () # define a route to get the employee data by ID @app . get ( \"/employees/ {emp_no} \" ) async def get_employee ( emp_no : int , db : Session = Depends ( get_db )): employee = db . query ( Employees ) . filter ( Employees . emp_no == emp_no ) . first () if not employee : raise HTTPException ( status_code = 404 , detail = \"Employee not found\" ) return employee # define a route to get the department data by ID @app . get ( \"/departments/ {dept_no} \" ) async def get_department ( dept_no : str , db : Session = Depends ( get_db )): department = db . query ( Departments ) . filter ( Departments . dept_no == dept_no ) . first () if not department : raise HTTPException ( status_code = 404 , detail = \"Department not found\" ) return department # define a route to get the list of employees in a department by ID @app . get ( \"/departments/ {dept_no} /employees\" ) async def get_department_employees ( dept_no : str , db : Session = Depends ( get_db )): employees = db . query ( Employees ) . \\ join ( Dept_Emp , Employees . emp_no == Dept_Emp . emp_no ) . \\ filter ( Dept_Emp . dept_no == dept_no ) . all () if not employees : raise HTTPException ( status_code = 404 , detail = \"No employees found in department\" ) return employees # define a route to get the salary history of an employee by ID @app . get ( \"/employees/ {emp_no} /salaries\" ) async def get_employee_salaries ( emp_no : int , db : Session = Depends ( get_db )): salaries = db . query ( Salaries ) . filter ( Salaries . emp_no == emp_no ) . all () if not salaries : raise HTTPException ( status_code = 404 , detail = \"No salary history found for employee\" ) return salaries To run this script, you can save it as a Python file (e.g. main.py) and then run the following command in your terminal: uvicorn main:app --reload This will start a local web server at http://localhost:8000/docs where you can access the different routes defined in the script \ud83e\udd13 Go on the first endpoint employees/{emp_no} and click on try it out and take an example employee ID (let's say 10001) you should see this in your browser :","title":"Writing our API"},{"location":"sql/29_python_docker_sql/","text":"SQLAlchemy and docker Add an existing SQL database into a container In this section we will be looking at an existing database and put it into a docker container in order to query this database with a python script. Here's an example database let's call this script init.sql : CREATE DATABASE testdb ; USE testdb ; CREATE TABLE users ( id INT NOT NULL AUTO_INCREMENT , name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 50 ) NOT NULL , PRIMARY KEY ( id ) ); INSERT INTO users ( name , email ) VALUES ( 'John Doe' , 'johndoe@example.com' ), ( 'Jane Doe' , 'janedoe@example.com' ), ( 'Bob Smith' , 'bobsmith@example.com' ); GRANT ALL PRIVILEGES ON testdb . * TO 'user' @ '%' IDENTIFIED BY 'user-password' WITH GRANT OPTION ; FLUSH PRIVILEGES ; This SQL script is used to create a new database called testdb , create a table called users with three columns id , name , and email , insert three records into the users table, and grant privileges to a user to access the testdb database. What is grant privileges The GRANT ALL PRIVILEGES command is used in MySQL to grant a user all possible privileges on a database or a specific table within a database. This command allows the user to perform any action on the specified database or table, including creating, modifying, and deleting data. Here's a breakdown of the syntax of the GRANT ALL PRIVILEGES command: GRANT ALL PRIVILEGES ON database_name . * TO 'username' @ 'localhost' IDENTIFIED BY 'password' ; FLUSH PRIVILEGES ; GRANT ALL PRIVILEGES : This command grants all possible privileges to the user. ON database_name.* : This specifies the database and any tables within it that the user will have privileges on. The * wildcard character specifies all tables within the database. TO 'username'@'localhost' : This specifies the username and the host from which the user can connect to the database. IDENTIFIED BY 'password' : This specifies the password for the user. FLUSH PRIVILEGES : This line reloads the grant tables in the mysql database to apply the changes made by the GRANT command. The GRANT ALL PRIVILEGES command is a powerful command that should be used with caution. It is recommended to only grant the necessary privileges to users to minimize the risk of data loss or security breaches. In our case this database is created in order to be connected by a python script and for that we must have a user, is it not recommended to connect the database as root user. Write a Dockerfile with init.sql file Let's write a Dockerfile for our sql container : FROM mysql:5.6 # set root password ENV MYSQL_ROOT_PASSWORD = my-secret-pw # create database and table COPY init.sql /docker-entrypoint-initdb.d/ # add a new user ENV MYSQL_USER = user ENV MYSQL_PASSWORD = user-password EXPOSE 3306 As you know in a Dockerfile , the COPY command is used to copy files or directories from the host machine into the Docker container. In the context of our Dockerfile , the line COPY init.sql /docker-entrypoint-initdb.d/ copies the init.sql file from the host machine into the /docker-entrypoint-initdb.d/ directory within the Docker container. The /docker-entrypoint-initdb.d/ directory is a default directory that is used by the mysql Docker image to automatically execute any SQL scripts that are located in this directory when the container is started up. By copying the init.sql file into the /docker-entrypoint-initdb.d/ directory within the Docker container, you are instructing the mysql image to automatically execute this script when the container starts up. This allows you to automatically create our database, tables, and insert data into the database without having to manually execute SQL commands every time the container is started up. Run our MySQL container To build our custom MySQL image, you can run the following command in the directory containing the Dockerfile and init.sql script: Build the image docker build -t my-mysql-image . This command will build the Docker image using the Dockerfile in the current directory and tag it as my-mysql-image:latest . Start the container Once the image is built, you can start a new container using the following command: docker run --name mysql-container -d -p 3306 :3306 my-mysql-image This command will start a new container (named mysql-container ) using the custom MySQL image, now we can connect to our container in order to verify if all the options are passed. Connect to our container Go to your terminal and run this command in order to enter into the container : docker exec -it mysql-container /bin/bash or the equivalent comnmand : docker exec -it <your-container-id> /bin/bash Now that you are in the container we can access the MySQL CLI with the following command : mysql -uuser -puser-password You should see the following prompt : mysql > It means that you're currently in the MySQL Shell of the container. You can now view our testdb and users table with the command : mysql > show databases ; you should see this output : +--------------------+ | Database | +--------------------+ | information_schema | | testdb | +--------------------+ 2 rows in set ( 0 .00 sec ) It means our user is able to see our database, you can also do a test query in order to verify the integrity of our database. In this section we have seen how to build a custom MySQL container with a database in it and how to access this database with the docker CLI \ud83d\ude80 Query a docker MySQL container with a python script Now, let's write a python script connect_db.py to connect our database \ud83e\udd73 from sqlalchemy import create_engine , MetaData , Table # create engine to connect to MySQL engine = create_engine ( 'mysql://user:user-password@0.0.0.0:3306/testdb' ) # create metadata object metadata = MetaData () # reflect the users table users_table = Table ( 'users' , metadata , autoload = True , autoload_with = engine ) # select all rows from the users table select_query = users_table . select () # execute the query with engine . connect () as conn : result = conn . execute ( select_query ) for row in result : print ( row ) Let's break down the different parts of the script: create_engine('mysql://root:my-secret-pw@0.0.0.0:3306/testdb') : This creates a SQLAlchemy engine that connects to the MySQL database running in the Docker container. The username is \"root\", the password is \"my-secret-pw\", the host is \"localhost\", the port is \"3306\", and the database name is \"testdb\". metadata = MetaData() : This creates a metadata object that will be used to reflect the database schema. users_table = Table('users', metadata, autoload=True, autoload_with=engine) : This reflects the \"users\" table from the database using the metadata object. select_query = users_table.select() : This creates a query that selects all rows from the \"users\" table. with engine.connect() as conn : result = conn.execute(select_query): This creates a connection to the database using the engine, executes the select query, and stores the result in a variable. for row in result: print(row) : This loops through the result set and prints each row. This script assumes that you have the necessary dependencies installed, including SQLAlchemy and the MySQL driver for Python. You can install these dependencies using pip : pip install sqlalchemy pymysql Note that the driver used to connect to the MySQL database is \"pymysql\", which is a Python MySQL client library that works with SQLAlchemy. If you execute this script, you should see this output : ( 1 , 'John Doe' , 'johndoe@example.com' ) ( 2 , 'Jane Doe' , 'janedoe@example.com' ) ( 3 , 'Bob Smith' , 'bobsmith@example.com' ) Next steps In this section we covered a pretty simple database and python script so for the next steps if you want to upgrade this project you can consider : Add FastAPI CRUD endpoints to interact with the database Add a better database and init.sql script Wrap-up the project with a docker-compose.yml file Code a dashboard route in FastAPI in order to visualize some data of your database Be creative \ud83d\ude03 Wrap-up Let's summarize whant we have seen in this section : Docker provides a consistent and portable environment for running applications, including databases like MySQL. We can use Docker to run a MySQL database in a container by pulling the mysql image and specifying the appropriate environment variables and port mappings. We can use the mysql-connector-python library in Python to connect to a MySQL database and perform SQL queries. We can create a SQL script to create tables and insert data into the MySQL database, and include this script in the Docker image to automatically execute it when the container starts up. We have learned how to write a Dockerfile to define the configuration of a Docker container, and how to use it to build a Docker image. Overall, connecting Docker MySQL and Python provides a more efficient and reliable way to manage and run databases and applications, and reduces the complexity and time involved in setting up and maintaining software installations.","title":"SQLAlchemy and Docker"},{"location":"sql/29_python_docker_sql/#sqlalchemy-and-docker","text":"","title":"SQLAlchemy and docker"},{"location":"sql/29_python_docker_sql/#add-an-existing-sql-database-into-a-container","text":"In this section we will be looking at an existing database and put it into a docker container in order to query this database with a python script. Here's an example database let's call this script init.sql : CREATE DATABASE testdb ; USE testdb ; CREATE TABLE users ( id INT NOT NULL AUTO_INCREMENT , name VARCHAR ( 50 ) NOT NULL , email VARCHAR ( 50 ) NOT NULL , PRIMARY KEY ( id ) ); INSERT INTO users ( name , email ) VALUES ( 'John Doe' , 'johndoe@example.com' ), ( 'Jane Doe' , 'janedoe@example.com' ), ( 'Bob Smith' , 'bobsmith@example.com' ); GRANT ALL PRIVILEGES ON testdb . * TO 'user' @ '%' IDENTIFIED BY 'user-password' WITH GRANT OPTION ; FLUSH PRIVILEGES ; This SQL script is used to create a new database called testdb , create a table called users with three columns id , name , and email , insert three records into the users table, and grant privileges to a user to access the testdb database.","title":"Add an existing SQL database into a container"},{"location":"sql/29_python_docker_sql/#what-is-grant-privileges","text":"The GRANT ALL PRIVILEGES command is used in MySQL to grant a user all possible privileges on a database or a specific table within a database. This command allows the user to perform any action on the specified database or table, including creating, modifying, and deleting data. Here's a breakdown of the syntax of the GRANT ALL PRIVILEGES command: GRANT ALL PRIVILEGES ON database_name . * TO 'username' @ 'localhost' IDENTIFIED BY 'password' ; FLUSH PRIVILEGES ; GRANT ALL PRIVILEGES : This command grants all possible privileges to the user. ON database_name.* : This specifies the database and any tables within it that the user will have privileges on. The * wildcard character specifies all tables within the database. TO 'username'@'localhost' : This specifies the username and the host from which the user can connect to the database. IDENTIFIED BY 'password' : This specifies the password for the user. FLUSH PRIVILEGES : This line reloads the grant tables in the mysql database to apply the changes made by the GRANT command. The GRANT ALL PRIVILEGES command is a powerful command that should be used with caution. It is recommended to only grant the necessary privileges to users to minimize the risk of data loss or security breaches. In our case this database is created in order to be connected by a python script and for that we must have a user, is it not recommended to connect the database as root user.","title":"What is grant privileges"},{"location":"sql/29_python_docker_sql/#write-a-dockerfile-with-initsql-file","text":"Let's write a Dockerfile for our sql container : FROM mysql:5.6 # set root password ENV MYSQL_ROOT_PASSWORD = my-secret-pw # create database and table COPY init.sql /docker-entrypoint-initdb.d/ # add a new user ENV MYSQL_USER = user ENV MYSQL_PASSWORD = user-password EXPOSE 3306 As you know in a Dockerfile , the COPY command is used to copy files or directories from the host machine into the Docker container. In the context of our Dockerfile , the line COPY init.sql /docker-entrypoint-initdb.d/ copies the init.sql file from the host machine into the /docker-entrypoint-initdb.d/ directory within the Docker container. The /docker-entrypoint-initdb.d/ directory is a default directory that is used by the mysql Docker image to automatically execute any SQL scripts that are located in this directory when the container is started up. By copying the init.sql file into the /docker-entrypoint-initdb.d/ directory within the Docker container, you are instructing the mysql image to automatically execute this script when the container starts up. This allows you to automatically create our database, tables, and insert data into the database without having to manually execute SQL commands every time the container is started up.","title":"Write a Dockerfile with init.sql file"},{"location":"sql/29_python_docker_sql/#run-our-mysql-container","text":"To build our custom MySQL image, you can run the following command in the directory containing the Dockerfile and init.sql script:","title":"Run our MySQL container"},{"location":"sql/29_python_docker_sql/#build-the-image","text":"docker build -t my-mysql-image . This command will build the Docker image using the Dockerfile in the current directory and tag it as my-mysql-image:latest .","title":"Build the image"},{"location":"sql/29_python_docker_sql/#start-the-container","text":"Once the image is built, you can start a new container using the following command: docker run --name mysql-container -d -p 3306 :3306 my-mysql-image This command will start a new container (named mysql-container ) using the custom MySQL image, now we can connect to our container in order to verify if all the options are passed.","title":"Start the container"},{"location":"sql/29_python_docker_sql/#connect-to-our-container","text":"Go to your terminal and run this command in order to enter into the container : docker exec -it mysql-container /bin/bash or the equivalent comnmand : docker exec -it <your-container-id> /bin/bash Now that you are in the container we can access the MySQL CLI with the following command : mysql -uuser -puser-password You should see the following prompt : mysql > It means that you're currently in the MySQL Shell of the container. You can now view our testdb and users table with the command : mysql > show databases ; you should see this output : +--------------------+ | Database | +--------------------+ | information_schema | | testdb | +--------------------+ 2 rows in set ( 0 .00 sec ) It means our user is able to see our database, you can also do a test query in order to verify the integrity of our database. In this section we have seen how to build a custom MySQL container with a database in it and how to access this database with the docker CLI \ud83d\ude80","title":"Connect to our container"},{"location":"sql/29_python_docker_sql/#query-a-docker-mysql-container-with-a-python-script","text":"Now, let's write a python script connect_db.py to connect our database \ud83e\udd73 from sqlalchemy import create_engine , MetaData , Table # create engine to connect to MySQL engine = create_engine ( 'mysql://user:user-password@0.0.0.0:3306/testdb' ) # create metadata object metadata = MetaData () # reflect the users table users_table = Table ( 'users' , metadata , autoload = True , autoload_with = engine ) # select all rows from the users table select_query = users_table . select () # execute the query with engine . connect () as conn : result = conn . execute ( select_query ) for row in result : print ( row ) Let's break down the different parts of the script: create_engine('mysql://root:my-secret-pw@0.0.0.0:3306/testdb') : This creates a SQLAlchemy engine that connects to the MySQL database running in the Docker container. The username is \"root\", the password is \"my-secret-pw\", the host is \"localhost\", the port is \"3306\", and the database name is \"testdb\". metadata = MetaData() : This creates a metadata object that will be used to reflect the database schema. users_table = Table('users', metadata, autoload=True, autoload_with=engine) : This reflects the \"users\" table from the database using the metadata object. select_query = users_table.select() : This creates a query that selects all rows from the \"users\" table. with engine.connect() as conn : result = conn.execute(select_query): This creates a connection to the database using the engine, executes the select query, and stores the result in a variable. for row in result: print(row) : This loops through the result set and prints each row. This script assumes that you have the necessary dependencies installed, including SQLAlchemy and the MySQL driver for Python. You can install these dependencies using pip : pip install sqlalchemy pymysql Note that the driver used to connect to the MySQL database is \"pymysql\", which is a Python MySQL client library that works with SQLAlchemy. If you execute this script, you should see this output : ( 1 , 'John Doe' , 'johndoe@example.com' ) ( 2 , 'Jane Doe' , 'janedoe@example.com' ) ( 3 , 'Bob Smith' , 'bobsmith@example.com' )","title":"Query a docker MySQL container with a python script"},{"location":"sql/29_python_docker_sql/#next-steps","text":"In this section we covered a pretty simple database and python script so for the next steps if you want to upgrade this project you can consider : Add FastAPI CRUD endpoints to interact with the database Add a better database and init.sql script Wrap-up the project with a docker-compose.yml file Code a dashboard route in FastAPI in order to visualize some data of your database Be creative \ud83d\ude03","title":"Next steps"},{"location":"sql/29_python_docker_sql/#wrap-up","text":"Let's summarize whant we have seen in this section : Docker provides a consistent and portable environment for running applications, including databases like MySQL. We can use Docker to run a MySQL database in a container by pulling the mysql image and specifying the appropriate environment variables and port mappings. We can use the mysql-connector-python library in Python to connect to a MySQL database and perform SQL queries. We can create a SQL script to create tables and insert data into the MySQL database, and include this script in the Docker image to automatically execute it when the container starts up. We have learned how to write a Dockerfile to define the configuration of a Docker container, and how to use it to build a Docker image. Overall, connecting Docker MySQL and Python provides a more efficient and reliable way to manage and run databases and applications, and reduces the complexity and time involved in setting up and maintaining software installations.","title":"Wrap-up"},{"location":"sql/30_python_projects/","text":"Python wrap-up project Project: Employee Management System Description You are tasked with building an Employee Management System for a company using FastAPI and SQLAlchemy. The system should allow the HR department to manage employee records, including personal information, salary, and department information. Requirements Create three database tables: employees, departments, and salaries. The employees table should have columns for emp_no, first_name, last_name, gender, hire_date, and dept_no. The departments table should have columns for dept_no and dept_name. The salaries table should have columns for emp_no, salary, and from_date and to_date. Implement an endpoint to add new employees to the database. The endpoint should accept a JSON payload containing the employee's personal information and department. The endpoint should validate the data and insert it into the database. If the department does not exist in the departments table, it should be created. Implement an endpoint to update an employee's salary. The endpoint should accept a JSON payload containing the employee's emp_no and new salary. The endpoint should validate the data and update the salaries table accordingly. Implement an endpoint to retrieve a list of employees with their current salary and department. The endpoint should accept an optional query parameter to filter by department. If the parameter is not provided, the endpoint should return all employees. Implement an endpoint to retrieve a list of departments and their total salary budget. The endpoint should calculate the total salary budget for each department based on the current salaries of its employees. Implement an endpoint to delete an employee from the database. The endpoint should accept an emp_no parameter and delete the corresponding record from the employees, salaries, and dept_emp tables. Implement appropriate error handling for each endpoint. Write unit tests for each endpoint. Use FastAPI's dependency injection system to inject the database session into each endpoint. Use SQLAlchemy's built-in ORM features to define the database schema and manage database transactions. The system should incorporate triggers to notify HR personnel when an employee is due for a performance review, salary increase, or other HR-related tasks. The system should be able to join multiple tables to provide HR personnel with a holistic view of employee records and performance. Bonus points : Use Docker to containerize the application and its dependencies. Deploy the application to a cloud platform such as Heroku or AWS Elastic Beanstalk. Example Workflow: HR personnel log in to the system and are presented with a dashboard displaying employee records and performance metrics. HR personnel search for a specific employee record and update their personal information, such as address, phone number, or emergency contact. The system sends a notification to HR personnel when an employee is due for a performance review, prompting them to schedule a meeting with the employee. HR personnel use the system's data analysis tools to identify top-performing employees and provide them with bonuses or promotions. The system generates HR reports that provide insights into employee performance, attendance, and payroll data. Overall, this project would require a strong understanding of SQL, including views, triggers, and joins, as well as web development with FastAPI and SQLAlchemy. It would also require careful attention to detail and robust error handling to ensure the system is secure, reliable, and scalable (for the bonus point). Project: Online Bookstore Management System Description Build an online bookstore management system using FastAPI and SQLAlchemy. The system should allow users to browse, search, and purchase books online. It should also allow administrators to manage the books, customers, orders, and inventory. Requirements: Create a database schema with at least four tables: books, customers, orders, and inventory. Use SQLAlchemy to map the database tables to Python classes. Implement RESTful API endpoints using FastAPI to allow users to browse and search books, place orders, and manage their accounts. Implement a view to show the top 10 best-selling books based on sales data. Implement triggers to update the inventory when orders are placed or cancelled. Implement a view to show the current inventory levels and the books that are running low on stock. Implement joins to retrieve customer order history and book sales data. Bonus: Implement authentication and authorization using JWT tokens. Implement an admin panel with secure login for administrators to manage the books, customers, orders, and inventory. Implement email notifications to customers when they place orders or their orders are shipped. This project will require skills in database design, SQL, Python programming, web development, and system architecture. It will also provide you an opportunity to gain experience with common web development frameworks and tools such as authentication, authorization and email notifications. Project: E-commerce platform Description Create a web application for an e-commerce platform using SQLAlchemy and FastAPI. The application should have the following features: User registration and authentication: Users should be able to register for an account and authenticate themselves to access their account information, order history, and other features. Product management: Users with administrative access should be able to add, modify, and delete product listings. Shopping cart: Users should be able to add products to a shopping cart and proceed to checkout to complete their purchase. Order history: Users should be able to view their order history and check the status of their current orders. Search functionality: Users should be able to search for products by name, description, category, and other criteria. Requirements: Use SQLAlchemy to create and manage the database schema. Use FastAPI to create RESTful endpoints for user authentication, product management, shopping cart, order history, and search functionality. Implement CRUD operations for product management. Implement a shopping cart using SQLAlchemy ORM. Use SQLAlchemy to create triggers for order history and updating product inventory. Use SQLAlchemy to create views to provide reports such as total sales and most popular products. Use SQLAlchemy to create joins to combine information from multiple tables. Use Python and SQLAlchemy to seed the database with sample data from generatedata.com Deploy the application to a cloud-based server such as AWS or Heroku. This project will provide students with experience in building a complex web application using SQLAlchemy and FastAPI, as well as deploying and managing the application on a cloud-based server. They will also gain experience in implementing advanced database features such as triggers, views, and joins.","title":"Python Wrap-up Projects"},{"location":"sql/30_python_projects/#python-wrap-up-project","text":"","title":"Python wrap-up project"},{"location":"sql/30_python_projects/#project-employee-management-system","text":"","title":"Project: Employee Management System"},{"location":"sql/30_python_projects/#description","text":"You are tasked with building an Employee Management System for a company using FastAPI and SQLAlchemy. The system should allow the HR department to manage employee records, including personal information, salary, and department information.","title":"Description"},{"location":"sql/30_python_projects/#requirements","text":"Create three database tables: employees, departments, and salaries. The employees table should have columns for emp_no, first_name, last_name, gender, hire_date, and dept_no. The departments table should have columns for dept_no and dept_name. The salaries table should have columns for emp_no, salary, and from_date and to_date. Implement an endpoint to add new employees to the database. The endpoint should accept a JSON payload containing the employee's personal information and department. The endpoint should validate the data and insert it into the database. If the department does not exist in the departments table, it should be created. Implement an endpoint to update an employee's salary. The endpoint should accept a JSON payload containing the employee's emp_no and new salary. The endpoint should validate the data and update the salaries table accordingly. Implement an endpoint to retrieve a list of employees with their current salary and department. The endpoint should accept an optional query parameter to filter by department. If the parameter is not provided, the endpoint should return all employees. Implement an endpoint to retrieve a list of departments and their total salary budget. The endpoint should calculate the total salary budget for each department based on the current salaries of its employees. Implement an endpoint to delete an employee from the database. The endpoint should accept an emp_no parameter and delete the corresponding record from the employees, salaries, and dept_emp tables. Implement appropriate error handling for each endpoint. Write unit tests for each endpoint. Use FastAPI's dependency injection system to inject the database session into each endpoint. Use SQLAlchemy's built-in ORM features to define the database schema and manage database transactions. The system should incorporate triggers to notify HR personnel when an employee is due for a performance review, salary increase, or other HR-related tasks. The system should be able to join multiple tables to provide HR personnel with a holistic view of employee records and performance. Bonus points : Use Docker to containerize the application and its dependencies. Deploy the application to a cloud platform such as Heroku or AWS Elastic Beanstalk.","title":"Requirements"},{"location":"sql/30_python_projects/#example-workflow","text":"HR personnel log in to the system and are presented with a dashboard displaying employee records and performance metrics. HR personnel search for a specific employee record and update their personal information, such as address, phone number, or emergency contact. The system sends a notification to HR personnel when an employee is due for a performance review, prompting them to schedule a meeting with the employee. HR personnel use the system's data analysis tools to identify top-performing employees and provide them with bonuses or promotions. The system generates HR reports that provide insights into employee performance, attendance, and payroll data. Overall, this project would require a strong understanding of SQL, including views, triggers, and joins, as well as web development with FastAPI and SQLAlchemy. It would also require careful attention to detail and robust error handling to ensure the system is secure, reliable, and scalable (for the bonus point).","title":"Example Workflow:"},{"location":"sql/30_python_projects/#project-online-bookstore-management-system","text":"","title":"Project: Online Bookstore Management System"},{"location":"sql/30_python_projects/#description_1","text":"Build an online bookstore management system using FastAPI and SQLAlchemy. The system should allow users to browse, search, and purchase books online. It should also allow administrators to manage the books, customers, orders, and inventory.","title":"Description"},{"location":"sql/30_python_projects/#requirements_1","text":"Create a database schema with at least four tables: books, customers, orders, and inventory. Use SQLAlchemy to map the database tables to Python classes. Implement RESTful API endpoints using FastAPI to allow users to browse and search books, place orders, and manage their accounts. Implement a view to show the top 10 best-selling books based on sales data. Implement triggers to update the inventory when orders are placed or cancelled. Implement a view to show the current inventory levels and the books that are running low on stock. Implement joins to retrieve customer order history and book sales data. Bonus: Implement authentication and authorization using JWT tokens. Implement an admin panel with secure login for administrators to manage the books, customers, orders, and inventory. Implement email notifications to customers when they place orders or their orders are shipped. This project will require skills in database design, SQL, Python programming, web development, and system architecture. It will also provide you an opportunity to gain experience with common web development frameworks and tools such as authentication, authorization and email notifications.","title":"Requirements:"},{"location":"sql/30_python_projects/#project-e-commerce-platform","text":"","title":"Project: E-commerce platform"},{"location":"sql/30_python_projects/#description_2","text":"Create a web application for an e-commerce platform using SQLAlchemy and FastAPI. The application should have the following features: User registration and authentication: Users should be able to register for an account and authenticate themselves to access their account information, order history, and other features. Product management: Users with administrative access should be able to add, modify, and delete product listings. Shopping cart: Users should be able to add products to a shopping cart and proceed to checkout to complete their purchase. Order history: Users should be able to view their order history and check the status of their current orders. Search functionality: Users should be able to search for products by name, description, category, and other criteria.","title":"Description"},{"location":"sql/30_python_projects/#requirements_2","text":"Use SQLAlchemy to create and manage the database schema. Use FastAPI to create RESTful endpoints for user authentication, product management, shopping cart, order history, and search functionality. Implement CRUD operations for product management. Implement a shopping cart using SQLAlchemy ORM. Use SQLAlchemy to create triggers for order history and updating product inventory. Use SQLAlchemy to create views to provide reports such as total sales and most popular products. Use SQLAlchemy to create joins to combine information from multiple tables. Use Python and SQLAlchemy to seed the database with sample data from generatedata.com Deploy the application to a cloud-based server such as AWS or Heroku. This project will provide students with experience in building a complex web application using SQLAlchemy and FastAPI, as well as deploying and managing the application on a cloud-based server. They will also gain experience in implementing advanced database features such as triggers, views, and joins.","title":"Requirements:"}]}